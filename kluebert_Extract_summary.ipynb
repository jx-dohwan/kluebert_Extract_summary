{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1WQYN_92WMhuEKDj8DnhLUQwlsvYPyZTU",
      "authorship_tag": "ABX9TyOtCU3/cAsynh8pBhMFc1Cc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jx-dohwan/kluebert_Extract_summary/blob/main/kluebert_Extract_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 설치"
      ],
      "metadata": {
        "id": "5ZBy3xvviYUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mecab"
      ],
      "metadata": {
        "id": "lUMFREYyiu_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install konlpy"
      ],
      "metadata": {
        "id": "Qvf7JdfuiaGG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39983c2-e8c6-4c81-8f46-17aa61506d2e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.8/dist-packages (0.6.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.4.1)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update"
      ],
      "metadata": {
        "id": "vc1bORuWinRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b28e75-35e0-4493-8105-0041c3f83385"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpad.net\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connecting to ppa.launchpad.net\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "\r                                                                               \r0% [Waiting for headers] [Waiting for headers] [Waiting for headers]\r                                                                    \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,296 kB]\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [998 kB]\n",
            "Hit:16 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Fetched 2,630 kB in 1s (1,788 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install curl git"
      ],
      "metadata": {
        "id": "HEDJMAoriq3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937944b2-f2fe-4d20-9086-ededdfd8d1da"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "curl is already the newest version (7.68.0-1ubuntu2.15).\n",
            "git is already the newest version (1:2.25.1-1ubuntu3.10).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "metadata": {
        "id": "pvxxuf7PisWY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf995ab-1f64-46e5-e920-0138e47a3f48"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mecab-ko is already installed\n",
            "mecab-ko-dic is already installed\n",
            "mecab-python is already installed\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiprocess"
      ],
      "metadata": {
        "id": "Kp1lfZ5GizTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install multiprocess"
      ],
      "metadata": {
        "id": "6Cxum78FitjY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c00265-5d65-467e-a638-0d090dbf07eb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (0.70.14)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.8/dist-packages (from multiprocess) (0.3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers"
      ],
      "metadata": {
        "id": "4J2FY9u7i2tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "MXwPCEKRi1Fh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c5b639f-714e-4858-aa36-118bc98f9f65"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyRouge"
      ],
      "metadata": {
        "id": "LTMKtjw8i62a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyrouge --upgrade\n",
        "!pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
        "!pip install pyrouge\n",
        "!pip show pyrouge\n",
        "!git clone https://github.com/andersjo/pyrouge.git\n",
        "from pyrouge import Rouge155\n",
        "!pyrouge_set_rouge_path '/content/pyrouge/tools/ROUGE-1.5.5'"
      ],
      "metadata": {
        "id": "sey_OHqFi5JY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10b409d9-d990-4876-fe46-35229af146f8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyrouge in /usr/local/lib/python3.8/dist-packages (0.1.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
            "  Using cached https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyrouge in /usr/local/lib/python3.8/dist-packages (0.1.3)\n",
            "Name: pyrouge\n",
            "Version: 0.1.3\n",
            "Summary: A Python wrapper for the ROUGE summarization evaluation package.\n",
            "Home-page: https://github.com/noutenki/pyrouge\n",
            "Author: Benjamin Heinzerling, Anders Johannsen\n",
            "Author-email: benjamin.heinzerling@h-its.org\n",
            "License: LICENSE.txt\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: \n",
            "Required-by: \n",
            "fatal: destination path 'pyrouge' already exists and is not an empty directory.\n",
            "2023-02-16 12:20:29,951 [MainThread  ] [INFO ]  Set ROUGE home directory to /content/pyrouge/tools/ROUGE-1.5.5.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install libxml-parser-perl"
      ],
      "metadata": {
        "id": "pdXNcunLi9G7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61a5cf9c-ad10-4102-93d2-5bd0ac9ae6b6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libxml-parser-perl is already the newest version (2.46-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd pyrouge/tools/ROUGE-1.5.5/data\n",
        "rm WordNet-2.0.exc.db # only if exist\n",
        "cd WordNet-2.0-Exceptions\n",
        "rm WordNet-2.0.exc.db # only if exist\n",
        "\n",
        "./buildExeptionDB.pl . exc WordNet-2.0.exc.db\n",
        "cd ../\n",
        "ln -s WordNet-2.0-Exceptions/WordNet-2.0.exc.db WordNet-2.0.exc.db"
      ],
      "metadata": {
        "id": "9r7wRNCEi-QZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34c62e7-cdf9-4051-b0a1-21c03deef530"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "hyraces\n",
            "iambi\n",
            "ibices\n",
            "ibo\n",
            "ichthyosauri\n",
            "ichthyosauruses\n",
            "iconostases\n",
            "icosahedra\n",
            "ideata\n",
            "igorrorote\n",
            "ilia\n",
            "imagines\n",
            "imagoes\n",
            "imperia\n",
            "impies\n",
            "incubi\n",
            "incudes\n",
            "indices\n",
            "indigoes\n",
            "indumenta\n",
            "indusia\n",
            "infundibula\n",
            "ingushes\n",
            "innuendoes\n",
            "inocula\n",
            "inquisitors-general\n",
            "insectaria\n",
            "insulae\n",
            "intagli\n",
            "interleaves\n",
            "intermezzi\n",
            "interreges\n",
            "interregna\n",
            "intimae\n",
            "involucella\n",
            "involucra\n",
            "involucra\n",
            "irides\n",
            "irs\n",
            "is\n",
            "ischia\n",
            "isthmi\n",
            "jackeroos\n",
            "jackfishes\n",
            "jackknives\n",
            "jacks-in-the-box\n",
            "jambeaux\n",
            "jellyfishes\n",
            "jewelfishes\n",
            "jewfishes\n",
            "jingoes\n",
            "jinn\n",
            "joes\n",
            "judge_advocates_general\n",
            "jura\n",
            "kaddishim\n",
            "kalmuck\n",
            "kalmucks\n",
            "katabases\n",
            "keeshonden\n",
            "kibbutzim\n",
            "killifishes\n",
            "kingfishes\n",
            "kings-of-arms\n",
            "knights_bachelor\n",
            "knights_bachelors\n",
            "knights_templar\n",
            "knights_templars\n",
            "knives\n",
            "kohlrabies\n",
            "kronen\n",
            "kroner\n",
            "kronur\n",
            "krooni\n",
            "kylikes\n",
            "labara\n",
            "labella\n",
            "labia\n",
            "labra\n",
            "lactobacilli\n",
            "lacunae\n",
            "lacunaria\n",
            "ladies-in-waiting\n",
            "lamellae\n",
            "lamiae\n",
            "laminae\n",
            "lapilli\n",
            "lapithae\n",
            "larvae\n",
            "larynges\n",
            "lassoes\n",
            "lati\n",
            "latices\n",
            "latifundia\n",
            "latu\n",
            "lavaboes\n",
            "leaves\n",
            "lecythi\n",
            "leges\n",
            "lei\n",
            "lemmata\n",
            "lemnisci\n",
            "lenes\n",
            "lentigines\n",
            "leonides\n",
            "lepidoptera\n",
            "leprosaria\n",
            "lepta\n",
            "leptocephali\n",
            "leucocytozoa\n",
            "leva\n",
            "librae\n",
            "libretti\n",
            "lice\n",
            "lieder\n",
            "ligulae\n",
            "limbi\n",
            "limina\n",
            "limites\n",
            "limuli\n",
            "lingoes\n",
            "linguae\n",
            "linguae_francae\n",
            "lionfishes\n",
            "lipomata\n",
            "lire\n",
            "liriodendra\n",
            "listente\n",
            "litai\n",
            "litu\n",
            "lives\n",
            "lixivia\n",
            "loaves\n",
            "loci\n",
            "loculi\n",
            "loggie\n",
            "logia\n",
            "lomenta\n",
            "longobardi\n",
            "loricae\n",
            "luba\n",
            "lubritoria\n",
            "lumbus\n",
            "lumina\n",
            "lumpfishes\n",
            "lungfishes\n",
            "lunulae\n",
            "lures\n",
            "lustra\n",
            "lyings-in\n",
            "lymphangitides\n",
            "lymphomata\n",
            "lymphopoieses\n",
            "lyses\n",
            "lyttae\n",
            "maare\n",
            "macaronies\n",
            "maccaronies\n",
            "machzorim\n",
            "macronuclei\n",
            "macrosporangia\n",
            "maculae\n",
            "madornos\n",
            "maestri\n",
            "mafiosi\n",
            "magi\n",
            "magmata\n",
            "magnificoes\n",
            "mahzorim\n",
            "major-axes\n",
            "major_axes\n",
            "makuta\n",
            "mallei\n",
            "malleoli\n",
            "maloti\n",
            "mamillae\n",
            "mammae\n",
            "mammillae\n",
            "mandingoes\n",
            "mangoes\n",
            "manifestoes\n",
            "manteaux\n",
            "mantes\n",
            "manubria\n",
            "marchese\n",
            "marchesi\n",
            "maremme\n",
            "markkaa\n",
            "marsupia\n",
            "marvels-of-peru\n",
            "mass_media\n",
            "masses\n",
            "masters-at-arms\n",
            "matrices\n",
            "matzoth\n",
            "mausolea\n",
            "maxillae\n",
            "maxima\n",
            "media\n",
            "mediae\n",
            "mediastina\n",
            "medullae\n",
            "medullae_oblongatae\n",
            "medusae\n",
            "megara\n",
            "megasporangia\n",
            "megilloth\n",
            "meioses\n",
            "melanomata\n",
            "melismata\n",
            "mementoes\n",
            "memoranda\n",
            "men\n",
            "men-at-arms\n",
            "men-o'-war\n",
            "men-of-war\n",
            "men_of_letters\n",
            "menisci\n",
            "menservants\n",
            "menstrua\n",
            "mesdames\n",
            "mesdemoiselles\n",
            "mesentera\n",
            "mesothoraces\n",
            "messeigneurs\n",
            "messieurs\n",
            "mestizoes\n",
            "metacarpi\n",
            "metamorphoses\n",
            "metanephroi\n",
            "metastases\n",
            "metatarsi\n",
            "metatheses\n",
            "metathoraces\n",
            "metazoa\n",
            "metempsychoses\n",
            "metencephala\n",
            "mezuzoth\n",
            "miasmata\n",
            "mice\n",
            "microanalyses\n",
            "micrococci\n",
            "micronuclei\n",
            "microsporangia\n",
            "midrashim\n",
            "midwives\n",
            "milia\n",
            "milieux\n",
            "militated_against\n",
            "milkfishes\n",
            "millennia\n",
            "minae\n",
            "minima\n",
            "ministeria\n",
            "minutiae\n",
            "minyanim\n",
            "mioses\n",
            "miracidia\n",
            "miri\n",
            "mishnayoth\n",
            "mitochondria\n",
            "mitzvoth\n",
            "modioli\n",
            "moduli\n",
            "momenta\n",
            "moments_of_truth\n",
            "momi\n",
            "monades\n",
            "monkfishes\n",
            "monochasia\n",
            "monopodia\n",
            "monoptera\n",
            "monopteroi\n",
            "monsignori\n",
            "monts-de-piete\n",
            "mooncalves\n",
            "moonfishes\n",
            "morae\n",
            "moratoria\n",
            "morceaux\n",
            "morescoes\n",
            "moriscoes\n",
            "morphallaxes\n",
            "morphoses\n",
            "morulae\n",
            "mosasauri\n",
            "moshavim\n",
            "moslim\n",
            "moslims\n",
            "mosquitoes\n",
            "mothers-in-law\n",
            "mothers_superior\n",
            "mottoes\n",
            "movers_and_shakers\n",
            "mucosae\n",
            "mucrones\n",
            "mudejares\n",
            "mudfishes\n",
            "mulattoes\n",
            "multiparae\n",
            "murices\n",
            "muskallunge\n",
            "mycelia\n",
            "mycetomata\n",
            "mycobacteria\n",
            "mycorrhizae\n",
            "myelencephala\n",
            "myiases\n",
            "myocardia\n",
            "myofibrillae\n",
            "myomata\n",
            "myoses\n",
            "myrmidones\n",
            "mythoi\n",
            "myxomata\n",
            "naevi\n",
            "naiades\n",
            "naoi\n",
            "narcissi\n",
            "nares\n",
            "nasopharynges\n",
            "natatoria\n",
            "naumachiae\n",
            "nauplii\n",
            "nautili\n",
            "navahoes\n",
            "navajoes\n",
            "nebulae\n",
            "necropoleis\n",
            "needlefishes\n",
            "negrilloes\n",
            "negritoes\n",
            "negroes\n",
            "nemeses\n",
            "nephridia\n",
            "nereides\n",
            "neurohypophyses\n",
            "neuromata\n",
            "neuroptera\n",
            "neuroses\n",
            "nevi\n",
            "nibelungen\n",
            "nidi\n",
            "nielli\n",
            "nilgai\n",
            "nimbi\n",
            "nimbostrati\n",
            "noctilucae\n",
            "nodi\n",
            "noes\n",
            "nomina\n",
            "nota\n",
            "noumena\n",
            "novae\n",
            "novelle\n",
            "novenae\n",
            "nubeculae\n",
            "nucelli\n",
            "nuchae\n",
            "nuclei\n",
            "nucleoli\n",
            "nulliparae\n",
            "numbfishes\n",
            "numina\n",
            "nymphae\n",
            "oarfishes\n",
            "oases\n",
            "obeli\n",
            "objets_d'art\n",
            "obligati\n",
            "oboli\n",
            "occipita\n",
            "oceanaria\n",
            "oceanides\n",
            "ocelli\n",
            "ochreae\n",
            "ocreae\n",
            "octahedra\n",
            "octopi\n",
            "oculi\n",
            "odea\n",
            "oedemata\n",
            "oesophagi\n",
            "oldwives\n",
            "olea\n",
            "omasa\n",
            "omayyades\n",
            "omenta\n",
            "ommatidia\n",
            "ommiades\n",
            "onagri\n",
            "oogonia\n",
            "oothecae\n",
            "operas_seria\n",
            "opercula\n",
            "optima\n",
            "ora\n",
            "organa\n",
            "organums\n",
            "orthoptera\n",
            "osar\n",
            "oscula\n",
            "ossa\n",
            "osteomata\n",
            "ostia\n",
            "ottomans\n",
            "ova\n",
            "ovoli\n",
            "ovotestes\n",
            "oxen\n",
            "oxymora\n",
            "paddlefishes\n",
            "paise\n",
            "paleae\n",
            "palestrae\n",
            "palingeneses\n",
            "pallia\n",
            "palmettoes\n",
            "palpi\n",
            "pancratia\n",
            "panettoni\n",
            "paparazzi\n",
            "paperknives\n",
            "papillae\n",
            "papillomata\n",
            "pappi\n",
            "papulae\n",
            "papyri\n",
            "parabases\n",
            "paraleipses\n",
            "paralyses\n",
            "paramecia\n",
            "paramenta\n",
            "paraphyses\n",
            "parapodia\n",
            "parapraxes\n",
            "paraselenae\n",
            "parashoth\n",
            "parasyntheta\n",
            "parazoa\n",
            "parentheses\n",
            "parerga\n",
            "parhelia\n",
            "parietes\n",
            "paris-mutuels\n",
            "parrotfishes\n",
            "parulides\n",
            "pasos_dobles\n",
            "passers-by\n",
            "pastorali\n",
            "patagia\n",
            "patellae\n",
            "patinae\n",
            "patresfamilias\n",
            "pease\n",
            "peccadilloes\n",
            "pectines\n",
            "pedaloes\n",
            "pedes\n",
            "pekingese\n",
            "pelves\n",
            "pence\n",
            "penes\n",
            "penetralium\n",
            "penicillia\n",
            "penknives\n",
            "pennae\n",
            "pennia\n",
            "pentahedra\n",
            "pentimenti\n",
            "penumbrae\n",
            "pepla\n",
            "pericardia\n",
            "perichondria\n",
            "pericrania\n",
            "peridia\n",
            "perigonia\n",
            "perihelia\n",
            "perinea\n",
            "perinephria\n",
            "perionychia\n",
            "periostea\n",
            "periphrases\n",
            "peristalses\n",
            "perithecia\n",
            "peritonea\n",
            "personae\n",
            "petechiae\n",
            "pfennige\n",
            "phalanges\n",
            "phalli\n",
            "pharynges\n",
            "phenomena\n",
            "phi-phenomena\n",
            "philodendra\n",
            "phlyctenae\n",
            "phyla\n",
            "phylae\n",
            "phyllotaxes\n",
            "phylloxerae\n",
            "phylogeneses\n",
            "pieds-a-terre\n",
            "pigfishes\n",
            "pilea\n",
            "pilei\n",
            "pineta\n",
            "pinfishes\n",
            "pinkoes\n",
            "pinnae\n",
            "pinnulae\n",
            "pipefishes\n",
            "pirogi\n",
            "piscinae\n",
            "pithecanthropi\n",
            "pithoi\n",
            "placeboes\n",
            "placentae\n",
            "planetaria\n",
            "planulae\n",
            "plasmodesmata\n",
            "plasmodia\n",
            "plateaux\n",
            "plectra\n",
            "plena\n",
            "pleura\n",
            "pleurae\n",
            "plicae\n",
            "ploughmen\n",
            "pneumobacilli\n",
            "pneumococci\n",
            "pocketknives\n",
            "podetia\n",
            "podia\n",
            "poleis\n",
            "pollices\n",
            "pollinia\n",
            "polychasia\n",
            "polyhedra\n",
            "polyparia\n",
            "polypi\n",
            "polyzoa\n",
            "polyzoaria\n",
            "pontes\n",
            "pontifices\n",
            "portamenti\n",
            "porticoes\n",
            "portmanteaux\n",
            "postliminia\n",
            "potatoes\n",
            "praenomina\n",
            "praxes\n",
            "predelle\n",
            "premaxillae\n",
            "prenomina\n",
            "prese\n",
            "primi\n",
            "primigravidae\n",
            "primiparae\n",
            "primordia\n",
            "principia\n",
            "proboscides\n",
            "proces-verbaux\n",
            "proglottides\n",
            "prognoses\n",
            "prolegomena\n",
            "prolepses\n",
            "promycelia\n",
            "pronephra\n",
            "pronephroi\n",
            "pronuclei\n",
            "propositi\n",
            "proptoses\n",
            "propyla\n",
            "propylaea\n",
            "proscenia\n",
            "prosencephala\n",
            "prostheses\n",
            "prostomia\n",
            "protases\n",
            "prothalamia\n",
            "prothalli\n",
            "prothallia\n",
            "prothoraces\n",
            "protonemata\n",
            "protozoa\n",
            "proventriculi\n",
            "provisoes\n",
            "prytanea\n",
            "psalteria\n",
            "pseudopodia\n",
            "psychoneuroses\n",
            "psychoses\n",
            "pterygia\n",
            "pterylae\n",
            "ptoses\n",
            "pubes\n",
            "pudenda\n",
            "puli\n",
            "pulvilli\n",
            "pulvini\n",
            "punchinelloes\n",
            "pupae\n",
            "puparia\n",
            "putamina\n",
            "putti\n",
            "pycnidia\n",
            "pygidia\n",
            "pylori\n",
            "pyxides\n",
            "pyxidia\n",
            "qaddishim\n",
            "quadrennia\n",
            "quadrigae\n",
            "qualia\n",
            "quanta\n",
            "quarterstaves\n",
            "quezales\n",
            "quinquennia\n",
            "quizzes\n",
            "rabatos\n",
            "rabbitfishes\n",
            "rachides\n",
            "radices\n",
            "radii\n",
            "radulae\n",
            "ramenta\n",
            "rami\n",
            "ranulae\n",
            "ranunculi\n",
            "raphae\n",
            "raphides\n",
            "ratfishes\n",
            "reales\n",
            "rearmice\n",
            "recta\n",
            "recti\n",
            "rectrices\n",
            "redfishes\n",
            "rediae\n",
            "referenda\n",
            "refugia\n",
            "reguli\n",
            "reis\n",
            "relata\n",
            "remiges\n",
            "reremice\n",
            "reseaux\n",
            "residua\n",
            "responsa\n",
            "retia\n",
            "retiarii\n",
            "reticula\n",
            "retinacula\n",
            "retinae\n",
            "rhabdomyomata\n",
            "rhachides\n",
            "rhachises\n",
            "rhinencephala\n",
            "rhizobia\n",
            "rhombi\n",
            "rhonchi\n",
            "rhyta\n",
            "ribbonfishes\n",
            "ricercacari\n",
            "ricercari\n",
            "rickettsiae\n",
            "rilievi\n",
            "rimae\n",
            "robes-de-chambre\n",
            "rockfishes\n",
            "roma\n",
            "romans-fleuves\n",
            "rondeaux\n",
            "rosaria\n",
            "rosefishes\n",
            "rostella\n",
            "rostra\n",
            "rouleaux\n",
            "rugae\n",
            "rumina\n",
            "runners-up\n",
            "sacra\n",
            "sacraria\n",
            "saguaros\n",
            "sailfishes\n",
            "salespeople\n",
            "salmonellae\n",
            "salpae\n",
            "salpinges\n",
            "saltarelli\n",
            "salvoes\n",
            "sancta\n",
            "sanitaria\n",
            "santimi\n",
            "saphenae\n",
            "sarcophagi\n",
            "sartorii\n",
            "sassanidae\n",
            "sawfishes\n",
            "scaldfishes\n",
            "scaleni\n",
            "scapulae\n",
            "scarabaei\n",
            "scarves\n",
            "schatchonim\n",
            "schemata\n",
            "scherzandi\n",
            "scherzi\n",
            "schmoes\n",
            "scholia\n",
            "schuln\n",
            "schutzstaffeln\n",
            "scirrhi\n",
            "scleromata\n",
            "scleroses\n",
            "sclerotia\n",
            "scoleces\n",
            "scolices\n",
            "scopulae\n",
            "scoriae\n",
            "scotomata\n",
            "scriptoria\n",
            "scrota\n",
            "scudi\n",
            "scuta\n",
            "scutella\n",
            "scyphi\n",
            "scyphistomae\n",
            "scyphozoa\n",
            "secondi\n",
            "secretaries-general\n",
            "segni\n",
            "seleucidae\n",
            "selves\n",
            "senores\n",
            "sensilla\n",
            "senti\n",
            "senussis\n",
            "separatrices\n",
            "sephardim\n",
            "septa\n",
            "septaria\n",
            "septennia\n",
            "sequelae\n",
            "sequestra\n",
            "sera\n",
            "seraphim\n",
            "sestertia\n",
            "setae\n",
            "sgraffiti\n",
            "shabbasim\n",
            "shabbatim\n",
            "shackoes\n",
            "shadchanim\n",
            "shadchans\n",
            "shakoes\n",
            "shammosim\n",
            "sheatfishes\n",
            "sheaves\n",
            "shellfishes\n",
            "shelves\n",
            "shinleaves\n",
            "shittim\n",
            "shmoes\n",
            "shofroth\n",
            "shophroth\n",
            "shrewmice\n",
            "shuln\n",
            "siddurim\n",
            "sigloi\n",
            "signore\n",
            "signori\n",
            "signorine\n",
            "siliquae\n",
            "silvae\n",
            "silverfishes\n",
            "simulacra\n",
            "sincipita\n",
            "sinfonie\n",
            "sisters-in-law\n",
            "sistra\n",
            "situlae\n",
            "smalti\n",
            "snaggleteeth\n",
            "snailfishes\n",
            "snipefishes\n",
            "socmen\n",
            "sola\n",
            "solaria\n",
            "solatia\n",
            "soldi\n",
            "soles\n",
            "solfeggi\n",
            "soli\n",
            "solidi\n",
            "somata\n",
            "sons-in-law\n",
            "soprani\n",
            "sordini\n",
            "sori\n",
            "soroses\n",
            "sovkhozy\n",
            "spadefishes\n",
            "spadices\n",
            "spearfishes\n",
            "spectra\n",
            "specula\n",
            "spermatia\n",
            "spermatogonia\n",
            "spermatozoa\n",
            "spermogonia\n",
            "sphinges\n",
            "spicae\n",
            "spicula\n",
            "spirilla\n",
            "splayfeet\n",
            "splenii\n",
            "sporangia\n",
            "sporogonia\n",
            "sporozoa\n",
            "springhase\n",
            "spumoni\n",
            "sputa\n",
            "squamae\n",
            "squashes\n",
            "squillae\n",
            "squirrelfishes\n",
            "squizzes\n",
            "stadia\n",
            "stamina\n",
            "staminodia\n",
            "stapedes\n",
            "staphylococci\n",
            "starfishes\n",
            "startsy\n",
            "stelae\n",
            "stemmata\n",
            "stenoses\n",
            "stepchildren\n",
            "sterna\n",
            "stigmata\n",
            "stimuli\n",
            "stipites\n",
            "stirpes\n",
            "stoae\n",
            "stockfishes\n",
            "stomata\n",
            "stomodaea\n",
            "stomodea\n",
            "stonefishes\n",
            "stotinki\n",
            "stotkini\n",
            "strappadoes\n",
            "strata\n",
            "strati\n",
            "stratocumuli\n",
            "street_children\n",
            "streptococci\n",
            "stretti\n",
            "striae\n",
            "strobili\n",
            "stromata\n",
            "strumae\n",
            "stuccoes\n",
            "styli\n",
            "stylopes\n",
            "stylopodia\n",
            "subcortices\n",
            "subdeliria\n",
            "subgenera\n",
            "subindices\n",
            "submucosae\n",
            "subphyla\n",
            "substrasta\n",
            "succedanea\n",
            "succubi\n",
            "suckerfishes\n",
            "suckfishes\n",
            "sudaria\n",
            "sudatoria\n",
            "sulci\n",
            "summae\n",
            "sunfishes\n",
            "supercargoes\n",
            "superheroes\n",
            "supernovae\n",
            "superstrata\n",
            "surgeonfishes\n",
            "swamies\n",
            "sweetiewives\n",
            "swellfishes\n",
            "swordfishes\n",
            "syconia\n",
            "syllabi\n",
            "syllepses\n",
            "symphyses\n",
            "sympodia\n",
            "symposia\n",
            "synapses\n",
            "synarthroses\n",
            "synclinoria\n",
            "syncytia\n",
            "syndesmoses\n",
            "synopses\n",
            "syntagmata\n",
            "syntheses\n",
            "syphilomata\n",
            "syringes\n",
            "syssarcoses\n",
            "tableaux\n",
            "taeniae\n",
            "tali\n",
            "tallaisim\n",
            "tallithes\n",
            "tallitoth\n",
            "tapeta\n",
            "tarantulae\n",
            "tarsi\n",
            "tarsometatarsi\n",
            "taxa\n",
            "taxes\n",
            "taxies\n",
            "tectrices\n",
            "teeth\n",
            "tegmina\n",
            "telae\n",
            "telamones\n",
            "telangiectases\n",
            "telia\n",
            "tempi\n",
            "tenacula\n",
            "tenderfeet\n",
            "teniae\n",
            "tenues\n",
            "teraphim\n",
            "terata\n",
            "teredines\n",
            "terga\n",
            "termini\n",
            "terraria\n",
            "terzetti\n",
            "tesserae\n",
            "testae\n",
            "testes\n",
            "testudines\n",
            "tetrahedra\n",
            "tetraskelia\n",
            "thalamencephala\n",
            "thalami\n",
            "thalli\n",
            "theatres-in-the-round\n",
            "thecae\n",
            "therses\n",
            "thesauri\n",
            "theses\n",
            "thickleaves\n",
            "thieves\n",
            "tholoi\n",
            "thoraces\n",
            "thrombi\n",
            "thymi\n",
            "thyrsi\n",
            "tibiae\n",
            "tilefishes\n",
            "tintinnabula\n",
            "titmice\n",
            "toadfishes\n",
            "tobaccoes\n",
            "tomatoes\n",
            "tomenta\n",
            "tondi\n",
            "tonneaux\n",
            "tophi\n",
            "topoi\n",
            "tori\n",
            "tornadoes\n",
            "torpedoes\n",
            "torsi\n",
            "touracos\n",
            "trabeculae\n",
            "tracheae\n",
            "traditores\n",
            "tragi\n",
            "trapezia\n",
            "trapezohedra\n",
            "traumata\n",
            "treponemata\n",
            "trichinae\n",
            "triclinia\n",
            "triennia\n",
            "triforia\n",
            "triggerfishes\n",
            "trihedra\n",
            "triskelia\n",
            "trisoctahedra\n",
            "triumviri\n",
            "trivia\n",
            "trochleae\n",
            "tropaeola\n",
            "trous-de-loup\n",
            "trousseaux\n",
            "trunkfishes\n",
            "trymata\n",
            "tubae\n",
            "turves\n",
            "tympana\n",
            "tyros\n",
            "ubermenschen\n",
            "uglies\n",
            "uigurs\n",
            "ulnae\n",
            "ultimata\n",
            "umbilici\n",
            "umbones\n",
            "umbrae\n",
            "unci\n",
            "uncidia\n",
            "uredines\n",
            "uredinia\n",
            "uredosori\n",
            "urethrae\n",
            "urinalyses\n",
            "uteri\n",
            "utriculi\n",
            "uvulae\n",
            "vacua\n",
            "vagi\n",
            "vaginae\n",
            "valleculae\n",
            "vaporetti\n",
            "varices\n",
            "vasa\n",
            "vascula\n",
            "vela\n",
            "velamina\n",
            "velaria\n",
            "venae\n",
            "venae_cavae\n",
            "ventriculi\n",
            "vermes\n",
            "verrucae\n",
            "vertebrae\n",
            "vertices\n",
            "vertigines\n",
            "vertigoes\n",
            "vesicae\n",
            "vetoes\n",
            "vexilla\n",
            "viatica\n",
            "viatores\n",
            "vibracula\n",
            "vibrissae\n",
            "vice-chairman\n",
            "villi\n",
            "vimina\n",
            "vincula\n",
            "viragoes\n",
            "vires\n",
            "virtuosi\n",
            "vitae\n",
            "vitelli\n",
            "vittae\n",
            "vivaria\n",
            "voces\n",
            "volcanoes\n",
            "volkslieder\n",
            "volte\n",
            "volvae\n",
            "vorticellae\n",
            "vortices\n",
            "vulvae\n",
            "wagons-lits\n",
            "wahhabis\n",
            "wanderjahre\n",
            "weakfishes\n",
            "werewolves\n",
            "wharves\n",
            "whippers-in\n",
            "whitefishes\n",
            "wives\n",
            "wolffishes\n",
            "wolves\n",
            "woodlice\n",
            "wreckfishes\n",
            "wunderkinder\n",
            "xiphisterna\n",
            "yeshivahs\n",
            "yeshivoth\n",
            "yogin\n",
            "yourselves\n",
            "zamindaris\n",
            "zecchini\n",
            "zeroes\n",
            "zoa\n",
            "zoaeae\n",
            "zoeae\n",
            "zoeas\n",
            "zoonoses\n",
            "zoosporangia\n",
            "adj.exc\n",
            "acer\n",
            "after\n",
            "airier\n",
            "airiest\n",
            "all-arounder\n",
            "angrier\n",
            "angriest\n",
            "archer\n",
            "artier\n",
            "artiest\n",
            "ashier\n",
            "ashiest\n",
            "assaulter\n",
            "attacker\n",
            "backer\n",
            "baggier\n",
            "baggiest\n",
            "balkier\n",
            "balkiest\n",
            "balmier\n",
            "balmiest\n",
            "bandier\n",
            "bandiest\n",
            "bargainer\n",
            "barmier\n",
            "barmiest\n",
            "battier\n",
            "battiest\n",
            "baulkier\n",
            "baulkiest\n",
            "bawdier\n",
            "bawdiest\n",
            "bayer\n",
            "beadier\n",
            "beadiest\n",
            "beastlier\n",
            "beastliest\n",
            "beater\n",
            "beefier\n",
            "beefiest\n",
            "beerier\n",
            "beeriest\n",
            "bendier\n",
            "bendiest\n",
            "best\n",
            "better\n",
            "bigger\n",
            "biggest\n",
            "bitchier\n",
            "bitchiest\n",
            "biter\n",
            "bittier\n",
            "bittiest\n",
            "blearier\n",
            "bleariest\n",
            "bloodier\n",
            "bloodiest\n",
            "bloodthirstier\n",
            "bloodthirstiest\n",
            "blowier\n",
            "blowiest\n",
            "blowsier\n",
            "blowsiest\n",
            "blowzier\n",
            "blowziest\n",
            "bluer\n",
            "bluest\n",
            "boner\n",
            "bonier\n",
            "boniest\n",
            "bonnier\n",
            "bonniest\n",
            "boozier\n",
            "booziest\n",
            "boskier\n",
            "boskiest\n",
            "bossier\n",
            "bossiest\n",
            "botchier\n",
            "botchiest\n",
            "bother\n",
            "bouncier\n",
            "bounciest\n",
            "bounder\n",
            "bower\n",
            "brainier\n",
            "brainiest\n",
            "brashier\n",
            "brashiest\n",
            "brassier\n",
            "brassiest\n",
            "brawnier\n",
            "brawniest\n",
            "breathier\n",
            "breathiest\n",
            "breezier\n",
            "breeziest\n",
            "brinier\n",
            "briniest\n",
            "britisher\n",
            "broadcaster\n",
            "brooder\n",
            "broodier\n",
            "broodiest\n",
            "bubblier\n",
            "bubbliest\n",
            "buggier\n",
            "buggiest\n",
            "bulkier\n",
            "bulkiest\n",
            "bumpier\n",
            "bumpiest\n",
            "bunchier\n",
            "bunchiest\n",
            "burlier\n",
            "burliest\n",
            "burrier\n",
            "burriest\n",
            "burster\n",
            "bushier\n",
            "bushiest\n",
            "busier\n",
            "busiest\n",
            "buster\n",
            "bustier\n",
            "bustiest\n",
            "cagier\n",
            "cagiest\n",
            "camper\n",
            "cannier\n",
            "canniest\n",
            "canter\n",
            "cantier\n",
            "cantiest\n",
            "caster\n",
            "catchier\n",
            "catchiest\n",
            "cattier\n",
            "cattiest\n",
            "cer\n",
            "chancier\n",
            "chanciest\n",
            "charier\n",
            "chariest\n",
            "chattier\n",
            "chattiest\n",
            "cheekier\n",
            "cheekiest\n",
            "cheerier\n",
            "cheeriest\n",
            "cheesier\n",
            "cheesiest\n",
            "chestier\n",
            "chestiest\n",
            "chewier\n",
            "chewiest\n",
            "chillier\n",
            "chilliest\n",
            "chintzier\n",
            "chintziest\n",
            "chippier\n",
            "chippiest\n",
            "choosier\n",
            "choosiest\n",
            "choppier\n",
            "choppiest\n",
            "chubbier\n",
            "chubbiest\n",
            "chuffier\n",
            "chuffiest\n",
            "chummier\n",
            "chummiest\n",
            "chunkier\n",
            "chunkiest\n",
            "churchier\n",
            "churchiest\n",
            "clammier\n",
            "clammiest\n",
            "classier\n",
            "classiest\n",
            "cleanlier\n",
            "cleanliest\n",
            "clerklier\n",
            "clerkliest\n",
            "cloudier\n",
            "cloudiest\n",
            "clubbier\n",
            "clubbiest\n",
            "clumsier\n",
            "clumsiest\n",
            "cockier\n",
            "cockiest\n",
            "coder\n",
            "collier\n",
            "colliest\n",
            "comelier\n",
            "comeliest\n",
            "comfier\n",
            "comfiest\n",
            "cornier\n",
            "corniest\n",
            "cosier\n",
            "cosiest\n",
            "costlier\n",
            "costliest\n",
            "costumer\n",
            "counterfeiter\n",
            "courtlier\n",
            "courtliest\n",
            "cozier\n",
            "coziest\n",
            "crabbier\n",
            "crabbiest\n",
            "cracker\n",
            "craftier\n",
            "craftiest\n",
            "craggier\n",
            "craggiest\n",
            "crankier\n",
            "crankiest\n",
            "crasher\n",
            "crawlier\n",
            "crawliest\n",
            "crazier\n",
            "craziest\n",
            "creamer\n",
            "creamier\n",
            "creamiest\n",
            "creepier\n",
            "creepiest\n",
            "crispier\n",
            "crispiest\n",
            "crumbier\n",
            "crumbiest\n",
            "crumblier\n",
            "crumbliest\n",
            "crummier\n",
            "crummiest\n",
            "crustier\n",
            "crustiest\n",
            "curlier\n",
            "curliest\n",
            "customer\n",
            "cuter\n",
            "daffier\n",
            "daffiest\n",
            "daintier\n",
            "daintiest\n",
            "dandier\n",
            "dandiest\n",
            "deadlier\n",
            "deadliest\n",
            "dealer\n",
            "deserter\n",
            "dewier\n",
            "dewiest\n",
            "dicier\n",
            "diciest\n",
            "dimer\n",
            "dimmer\n",
            "dimmest\n",
            "dingier\n",
            "dingiest\n",
            "dinkier\n",
            "dinkiest\n",
            "dippier\n",
            "dippiest\n",
            "dirtier\n",
            "dirtiest\n",
            "dishier\n",
            "dishiest\n",
            "dizzier\n",
            "dizziest\n",
            "dodgier\n",
            "dodgiest\n",
            "dopier\n",
            "dopiest\n",
            "dottier\n",
            "dottiest\n",
            "doughier\n",
            "doughiest\n",
            "doughtier\n",
            "doughtiest\n",
            "dowdier\n",
            "dowdiest\n",
            "dowier\n",
            "dowiest\n",
            "downer\n",
            "downier\n",
            "downiest\n",
            "dozier\n",
            "doziest\n",
            "drabber\n",
            "drabbest\n",
            "draftier\n",
            "draftiest\n",
            "draggier\n",
            "draggiest\n",
            "draughtier\n",
            "draughtiest\n",
            "dreamier\n",
            "dreamiest\n",
            "drearier\n",
            "dreariest\n",
            "dreggier\n",
            "dreggiest\n",
            "dresser\n",
            "dressier\n",
            "dressiest\n",
            "drier\n",
            "driest\n",
            "drippier\n",
            "drippiest\n",
            "drowsier\n",
            "drowsiest\n",
            "dryer\n",
            "dryest\n",
            "dumpier\n",
            "dumpiest\n",
            "dunner\n",
            "dunnest\n",
            "duskier\n",
            "duskiest\n",
            "dustier\n",
            "dustiest\n",
            "earlier\n",
            "earliest\n",
            "earthier\n",
            "earthiest\n",
            "earthlier\n",
            "earthliest\n",
            "easier\n",
            "easiest\n",
            "easter\n",
            "eastsider\n",
            "edger\n",
            "edgier\n",
            "edgiest\n",
            "eerier\n",
            "eeriest\n",
            "emptier\n",
            "emptiest\n",
            "faker\n",
            "fancier\n",
            "fanciest\n",
            "fatter\n",
            "fattest\n",
            "fattier\n",
            "fattiest\n",
            "faultier\n",
            "faultiest\n",
            "feistier\n",
            "feistiest\n",
            "feller\n",
            "fiddlier\n",
            "fiddliest\n",
            "filmier\n",
            "filmiest\n",
            "filthier\n",
            "filthiest\n",
            "finnier\n",
            "finniest\n",
            "first-rater\n",
            "first-stringer\n",
            "fishier\n",
            "fishiest\n",
            "fitter\n",
            "fittest\n",
            "flabbier\n",
            "flabbiest\n",
            "flaggier\n",
            "flaggiest\n",
            "flakier\n",
            "flakiest\n",
            "flasher\n",
            "flashier\n",
            "flashiest\n",
            "flatter\n",
            "flattest\n",
            "flauntier\n",
            "flauntiest\n",
            "fledgier\n",
            "fledgiest\n",
            "fleecier\n",
            "fleeciest\n",
            "fleshier\n",
            "fleshiest\n",
            "fleshlier\n",
            "fleshliest\n",
            "flightier\n",
            "flightiest\n",
            "flimsier\n",
            "flimsiest\n",
            "flintier\n",
            "flintiest\n",
            "floatier\n",
            "floatiest\n",
            "floppier\n",
            "floppiest\n",
            "flossier\n",
            "flossiest\n",
            "fluffier\n",
            "fluffiest\n",
            "flukier\n",
            "flukiest\n",
            "foamier\n",
            "foamiest\n",
            "foggier\n",
            "foggiest\n",
            "folder\n",
            "folksier\n",
            "folksiest\n",
            "foolhardier\n",
            "foolhardiest\n",
            "fore-and-after\n",
            "foreigner\n",
            "forest\n",
            "founder\n",
            "foxier\n",
            "foxiest\n",
            "fratchier\n",
            "fratchiest\n",
            "freakier\n",
            "freakiest\n",
            "freer\n",
            "freest\n",
            "frenchier\n",
            "frenchiest\n",
            "friendlier\n",
            "friendliest\n",
            "friskier\n",
            "friskiest\n",
            "frizzier\n",
            "frizziest\n",
            "frizzlier\n",
            "frizzliest\n",
            "frostier\n",
            "frostiest\n",
            "frouzier\n",
            "frouziest\n",
            "frowsier\n",
            "frowsiest\n",
            "frowzier\n",
            "frowziest\n",
            "fruitier\n",
            "fruitiest\n",
            "funkier\n",
            "funkiest\n",
            "funnier\n",
            "funniest\n",
            "furrier\n",
            "furriest\n",
            "fussier\n",
            "fussiest\n",
            "fustier\n",
            "fustiest\n",
            "fuzzier\n",
            "fuzziest\n",
            "gabbier\n",
            "gabbiest\n",
            "gamier\n",
            "gamiest\n",
            "gammier\n",
            "gammiest\n",
            "gassier\n",
            "gassiest\n",
            "gaudier\n",
            "gaudiest\n",
            "gauzier\n",
            "gauziest\n",
            "gawkier\n",
            "gawkiest\n",
            "ghastlier\n",
            "ghastliest\n",
            "ghostlier\n",
            "ghostliest\n",
            "giddier\n",
            "giddiest\n",
            "gladder\n",
            "gladdest\n",
            "glassier\n",
            "glassiest\n",
            "glibber\n",
            "glibbest\n",
            "gloomier\n",
            "gloomiest\n",
            "glossier\n",
            "glossiest\n",
            "glummer\n",
            "glummest\n",
            "godlier\n",
            "godliest\n",
            "goer\n",
            "goner\n",
            "goodlier\n",
            "goodliest\n",
            "goofier\n",
            "goofiest\n",
            "gooier\n",
            "gooiest\n",
            "goosier\n",
            "goosiest\n",
            "gorier\n",
            "goriest\n",
            "gradelier\n",
            "gradeliest\n",
            "grader\n",
            "grainier\n",
            "grainiest\n",
            "grassier\n",
            "grassiest\n",
            "greasier\n",
            "greasiest\n",
            "greedier\n",
            "greediest\n",
            "grimmer\n",
            "grimmest\n",
            "grislier\n",
            "grisliest\n",
            "grittier\n",
            "grittiest\n",
            "grizzlier\n",
            "grizzliest\n",
            "groggier\n",
            "groggiest\n",
            "groovier\n",
            "grooviest\n",
            "grottier\n",
            "grottiest\n",
            "grounder\n",
            "grouper\n",
            "groutier\n",
            "groutiest\n",
            "grubbier\n",
            "grubbiest\n",
            "grumpier\n",
            "grumpiest\n",
            "guest\n",
            "guiltier\n",
            "guiltiest\n",
            "gummier\n",
            "gummiest\n",
            "gushier\n",
            "gushiest\n",
            "gustier\n",
            "gustiest\n",
            "gutsier\n",
            "gutsiest\n",
            "hairier\n",
            "hairiest\n",
            "halfways\n",
            "halter\n",
            "hammier\n",
            "hammiest\n",
            "handier\n",
            "handiest\n",
            "happier\n",
            "happiest\n",
            "hardier\n",
            "hardiest\n",
            "hastier\n",
            "hastiest\n",
            "haughtier\n",
            "haughtiest\n",
            "hazier\n",
            "haziest\n",
            "header\n",
            "headier\n",
            "headiest\n",
            "healthier\n",
            "healthiest\n",
            "heartier\n",
            "heartiest\n",
            "heavier\n",
            "heaviest\n",
            "heftier\n",
            "heftiest\n",
            "hepper\n",
            "heppest\n",
            "herbier\n",
            "herbiest\n",
            "hinder\n",
            "hipper\n",
            "hippest\n",
            "hippier\n",
            "hippiest\n",
            "hoarier\n",
            "hoariest\n",
            "holier\n",
            "holiest\n",
            "homelier\n",
            "homeliest\n",
            "homer\n",
            "homier\n",
            "homiest\n",
            "hornier\n",
            "horniest\n",
            "horsier\n",
            "horsiest\n",
            "hotter\n",
            "hottest\n",
            "humpier\n",
            "humpiest\n",
            "hunger\n",
            "hungrier\n",
            "hungriest\n",
            "huskier\n",
            "huskiest\n",
            "icier\n",
            "iciest\n",
            "inkier\n",
            "inkiest\n",
            "insider\n",
            "interest\n",
            "jaggier\n",
            "jaggiest\n",
            "jammier\n",
            "jammiest\n",
            "jauntier\n",
            "jauntiest\n",
            "jazzier\n",
            "jazziest\n",
            "jerkier\n",
            "jerkiest\n",
            "jointer\n",
            "jollier\n",
            "jolliest\n",
            "juicier\n",
            "juiciest\n",
            "jumpier\n",
            "jumpiest\n",
            "kindlier\n",
            "kindliest\n",
            "kinkier\n",
            "kinkiest\n",
            "knottier\n",
            "knottiest\n",
            "knurlier\n",
            "knurliest\n",
            "kookier\n",
            "kookiest\n",
            "lacier\n",
            "laciest\n",
            "lairier\n",
            "lairiest\n",
            "lakier\n",
            "lakiest\n",
            "lander\n",
            "lankier\n",
            "lankiest\n",
            "lathier\n",
            "lathiest\n",
            "layer\n",
            "lazier\n",
            "laziest\n",
            "leafier\n",
            "leafiest\n",
            "leakier\n",
            "leakiest\n",
            "learier\n",
            "leariest\n",
            "leer\n",
            "leerier\n",
            "leeriest\n",
            "left-hander\n",
            "left-winger\n",
            "leggier\n",
            "leggiest\n",
            "lengthier\n",
            "lengthiest\n",
            "ler\n",
            "leveler\n",
            "limier\n",
            "limiest\n",
            "lippier\n",
            "lippiest\n",
            "liter\n",
            "livelier\n",
            "liveliest\n",
            "liver\n",
            "loather\n",
            "loftier\n",
            "loftiest\n",
            "logier\n",
            "logiest\n",
            "lonelier\n",
            "loneliest\n",
            "loner\n",
            "loonier\n",
            "looniest\n",
            "loopier\n",
            "loopiest\n",
            "lordlier\n",
            "lordliest\n",
            "lousier\n",
            "lousiest\n",
            "lovelier\n",
            "loveliest\n",
            "lowlander\n",
            "lowlier\n",
            "lowliest\n",
            "luckier\n",
            "luckiest\n",
            "lumpier\n",
            "lumpiest\n",
            "lunier\n",
            "luniest\n",
            "lustier\n",
            "lustiest\n",
            "madder\n",
            "maddest\n",
            "mainer\n",
            "maligner\n",
            "maltier\n",
            "maltiest\n",
            "mangier\n",
            "mangiest\n",
            "mankier\n",
            "mankiest\n",
            "manlier\n",
            "manliest\n",
            "mariner\n",
            "marshier\n",
            "marshiest\n",
            "massier\n",
            "massiest\n",
            "matter\n",
            "maungier\n",
            "maungiest\n",
            "mazier\n",
            "maziest\n",
            "mealier\n",
            "mealiest\n",
            "measlier\n",
            "measliest\n",
            "meatier\n",
            "meatiest\n",
            "meeter\n",
            "merrier\n",
            "merriest\n",
            "messier\n",
            "messiest\n",
            "miffier\n",
            "miffiest\n",
            "mightier\n",
            "mightiest\n",
            "milcher\n",
            "milker\n",
            "milkier\n",
            "milkiest\n",
            "mingier\n",
            "mingiest\n",
            "minter\n",
            "mirkier\n",
            "mirkiest\n",
            "miser\n",
            "mistier\n",
            "mistiest\n",
            "mocker\n",
            "modeler\n",
            "modest\n",
            "moldier\n",
            "moldiest\n",
            "moodier\n",
            "moodiest\n",
            "moonier\n",
            "mooniest\n",
            "mothier\n",
            "mothiest\n",
            "mouldier\n",
            "mouldiest\n",
            "mousier\n",
            "mousiest\n",
            "mouthier\n",
            "mouthiest\n",
            "muckier\n",
            "muckiest\n",
            "muddier\n",
            "muddiest\n",
            "muggier\n",
            "muggiest\n",
            "multiplexer\n",
            "murkier\n",
            "murkiest\n",
            "mushier\n",
            "mushiest\n",
            "muskier\n",
            "muskiest\n",
            "muster\n",
            "mustier\n",
            "mustiest\n",
            "muzzier\n",
            "muzziest\n",
            "nappier\n",
            "nappiest\n",
            "nastier\n",
            "nastiest\n",
            "nattier\n",
            "nattiest\n",
            "naughtier\n",
            "naughtiest\n",
            "needier\n",
            "neediest\n",
            "nervier\n",
            "nerviest\n",
            "newsier\n",
            "newsiest\n",
            "niftier\n",
            "niftiest\n",
            "nippier\n",
            "nippiest\n",
            "nittier\n",
            "nittiest\n",
            "noisier\n",
            "noisiest\n",
            "northeasterner\n",
            "norther\n",
            "northerner\n",
            "nosier\n",
            "nosiest\n",
            "number\n",
            "nuttier\n",
            "nuttiest\n",
            "offer\n",
            "offer\n",
            "oilier\n",
            "oiliest\n",
            "old-timer\n",
            "oliver\n",
            "oozier\n",
            "ooziest\n",
            "opener\n",
            "outsider\n",
            "overcomer\n",
            "overnighter\n",
            "owner\n",
            "pallier\n",
            "palliest\n",
            "palmier\n",
            "palmiest\n",
            "paltrier\n",
            "paltriest\n",
            "pappier\n",
            "pappiest\n",
            "parkier\n",
            "parkiest\n",
            "part-timer\n",
            "passer\n",
            "paster\n",
            "pastier\n",
            "pastiest\n",
            "patchier\n",
            "patchiest\n",
            "pater\n",
            "pawkier\n",
            "pawkiest\n",
            "peachier\n",
            "peachiest\n",
            "pearler\n",
            "pearlier\n",
            "pearliest\n",
            "pedaler\n",
            "peppier\n",
            "peppiest\n",
            "perkier\n",
            "perkiest\n",
            "peskier\n",
            "peskiest\n",
            "peter\n",
            "pettier\n",
            "pettiest\n",
            "phonier\n",
            "phoniest\n",
            "pickier\n",
            "pickiest\n",
            "piggier\n",
            "piggiest\n",
            "pinier\n",
            "piniest\n",
            "pitchier\n",
            "pitchiest\n",
            "pithier\n",
            "pithiest\n",
            "planer\n",
            "plashier\n",
            "plashiest\n",
            "platier\n",
            "platiest\n",
            "player\n",
            "pluckier\n",
            "pluckiest\n",
            "plumber\n",
            "plumier\n",
            "plumiest\n",
            "plummier\n",
            "plummiest\n",
            "podgier\n",
            "podgiest\n",
            "pokier\n",
            "pokiest\n",
            "polisher\n",
            "porkier\n",
            "porkiest\n",
            "porter\n",
            "portlier\n",
            "portliest\n",
            "poster\n",
            "pottier\n",
            "pottiest\n",
            "preachier\n",
            "preachiest\n",
            "presenter\n",
            "pretender\n",
            "prettier\n",
            "prettiest\n",
            "pricier\n",
            "priciest\n",
            "pricklier\n",
            "prickliest\n",
            "priestlier\n",
            "priestliest\n",
            "primer\n",
            "primmer\n",
            "primmest\n",
            "princelier\n",
            "princeliest\n",
            "printer\n",
            "prissier\n",
            "prissiest\n",
            "privateer\n",
            "privier\n",
            "priviest\n",
            "prompter\n",
            "prosier\n",
            "prosiest\n",
            "pudgier\n",
            "pudgiest\n",
            "puffer\n",
            "puffier\n",
            "puffiest\n",
            "pulpier\n",
            "pulpiest\n",
            "punchier\n",
            "punchiest\n",
            "punier\n",
            "puniest\n",
            "pushier\n",
            "pushiest\n",
            "pussier\n",
            "pussiest\n",
            "quaggier\n",
            "quaggiest\n",
            "quakier\n",
            "quakiest\n",
            "queasier\n",
            "queasiest\n",
            "queenlier\n",
            "queenliest\n",
            "racier\n",
            "raciest\n",
            "rainier\n",
            "rainiest\n",
            "randier\n",
            "randiest\n",
            "rangier\n",
            "rangiest\n",
            "ranker\n",
            "rattier\n",
            "rattiest\n",
            "rattlier\n",
            "rattliest\n",
            "raunchier\n",
            "raunchiest\n",
            "readier\n",
            "readiest\n",
            "recorder\n",
            "redder\n",
            "reddest\n",
            "reedier\n",
            "reediest\n",
            "renter\n",
            "retailer\n",
            "right-hander\n",
            "right-winger\n",
            "rimier\n",
            "rimiest\n",
            "riskier\n",
            "riskiest\n",
            "ritzier\n",
            "ritziest\n",
            "roaster\n",
            "rockier\n",
            "rockiest\n",
            "roilier\n",
            "roiliest\n",
            "rookier\n",
            "rookiest\n",
            "roomier\n",
            "roomiest\n",
            "ropier\n",
            "ropiest\n",
            "rosier\n",
            "rosiest\n",
            "rowdier\n",
            "rowdiest\n",
            "ruddier\n",
            "ruddiest\n",
            "runnier\n",
            "runniest\n",
            "rusher\n",
            "rushier\n",
            "rushiest\n",
            "rustier\n",
            "rustiest\n",
            "ruttier\n",
            "ruttiest\n",
            "sadder\n",
            "saddest\n",
            "salter\n",
            "saltier\n",
            "saltiest\n",
            "sampler\n",
            "sandier\n",
            "sandiest\n",
            "sappier\n",
            "sappiest\n",
            "sassier\n",
            "sassiest\n",
            "saucier\n",
            "sauciest\n",
            "savvier\n",
            "savviest\n",
            "scabbier\n",
            "scabbiest\n",
            "scalier\n",
            "scaliest\n",
            "scantier\n",
            "scantiest\n",
            "scarier\n",
            "scariest\n",
            "scraggier\n",
            "scraggiest\n",
            "scragglier\n",
            "scraggliest\n",
            "scraper\n",
            "scrappier\n",
            "scrappiest\n",
            "scrawnier\n",
            "scrawniest\n",
            "screwier\n",
            "screwiest\n",
            "scrubbier\n",
            "scrubbiest\n",
            "scruffier\n",
            "scruffiest\n",
            "scungier\n",
            "scungiest\n",
            "scurvier\n",
            "scurviest\n",
            "seamier\n",
            "seamiest\n",
            "second-rater\n",
            "seconder\n",
            "seedier\n",
            "seediest\n",
            "seemlier\n",
            "seemliest\n",
            "serer\n",
            "sexier\n",
            "sexiest\n",
            "shabbier\n",
            "shabbiest\n",
            "shadier\n",
            "shadiest\n",
            "shaggier\n",
            "shaggiest\n",
            "shakier\n",
            "shakiest\n",
            "shapelier\n",
            "shapeliest\n",
            "shier\n",
            "shiest\n",
            "shiftier\n",
            "shiftiest\n",
            "shinier\n",
            "shiniest\n",
            "shirtier\n",
            "shirtiest\n",
            "shoddier\n",
            "shoddiest\n",
            "showier\n",
            "showiest\n",
            "shrubbier\n",
            "shrubbiest\n",
            "shyer\n",
            "shyest\n",
            "sicklier\n",
            "sickliest\n",
            "sightlier\n",
            "sightliest\n",
            "signaler\n",
            "signer\n",
            "silkier\n",
            "silkiest\n",
            "sillier\n",
            "silliest\n",
            "sketchier\n",
            "sketchiest\n",
            "skewer\n",
            "skimpier\n",
            "skimpiest\n",
            "skinnier\n",
            "skinniest\n",
            "slaphappier\n",
            "slaphappiest\n",
            "slatier\n",
            "slatiest\n",
            "slaver\n",
            "sleazier\n",
            "sleaziest\n",
            "sleepier\n",
            "sleepiest\n",
            "slier\n",
            "sliest\n",
            "slimier\n",
            "slimiest\n",
            "slimmer\n",
            "slimmest\n",
            "slimsier\n",
            "slimsiest\n",
            "slinkier\n",
            "slinkiest\n",
            "slippier\n",
            "slippiest\n",
            "sloppier\n",
            "sloppiest\n",
            "slyer\n",
            "slyest\n",
            "smarmier\n",
            "smarmiest\n",
            "smellier\n",
            "smelliest\n",
            "smokier\n",
            "smokiest\n",
            "smugger\n",
            "smuggest\n",
            "snakier\n",
            "snakiest\n",
            "snappier\n",
            "snappiest\n",
            "snatchier\n",
            "snatchiest\n",
            "snazzier\n",
            "snazziest\n",
            "sneaker\n",
            "sniffier\n",
            "sniffiest\n",
            "snootier\n",
            "snootiest\n",
            "snottier\n",
            "snottiest\n",
            "snowier\n",
            "snowiest\n",
            "snuffer\n",
            "snuffier\n",
            "snuffiest\n",
            "snugger\n",
            "snuggest\n",
            "soapier\n",
            "soapiest\n",
            "soggier\n",
            "soggiest\n",
            "solder\n",
            "sonsier\n",
            "sonsiest\n",
            "sootier\n",
            "sootiest\n",
            "soppier\n",
            "soppiest\n",
            "sorrier\n",
            "sorriest\n",
            "soupier\n",
            "soupiest\n",
            "souther\n",
            "southerner\n",
            "speedier\n",
            "speediest\n",
            "spicier\n",
            "spiciest\n",
            "spiffier\n",
            "spiffiest\n",
            "spikier\n",
            "spikiest\n",
            "spindlier\n",
            "spindliest\n",
            "spinier\n",
            "spiniest\n",
            "splashier\n",
            "splashiest\n",
            "spongier\n",
            "spongiest\n",
            "spookier\n",
            "spookiest\n",
            "spoonier\n",
            "spooniest\n",
            "sportier\n",
            "sportiest\n",
            "spottier\n",
            "spottiest\n",
            "spreader\n",
            "sprier\n",
            "spriest\n",
            "sprightlier\n",
            "sprightliest\n",
            "springer\n",
            "springier\n",
            "springiest\n",
            "squashier\n",
            "squashiest\n",
            "squatter\n",
            "squattest\n",
            "squattier\n",
            "squattiest\n",
            "squiffier\n",
            "squiffiest\n",
            "stagier\n",
            "stagiest\n",
            "stalkier\n",
            "stalkiest\n",
            "stapler\n",
            "starchier\n",
            "starchiest\n",
            "starer\n",
            "starest\n",
            "starrier\n",
            "starriest\n",
            "statelier\n",
            "stateliest\n",
            "steadier\n",
            "steadiest\n",
            "stealthier\n",
            "stealthiest\n",
            "steamier\n",
            "steamiest\n",
            "stingier\n",
            "stingiest\n",
            "stiper\n",
            "stocker\n",
            "stockier\n",
            "stockiest\n",
            "stodgier\n",
            "stodgiest\n",
            "stonier\n",
            "stoniest\n",
            "stormier\n",
            "stormiest\n",
            "streakier\n",
            "streakiest\n",
            "streamier\n",
            "streamiest\n",
            "stretcher\n",
            "stretchier\n",
            "stretchiest\n",
            "stringier\n",
            "stringiest\n",
            "stripier\n",
            "stripiest\n",
            "stronger\n",
            "strongest\n",
            "stroppier\n",
            "stroppiest\n",
            "stuffier\n",
            "stuffiest\n",
            "stumpier\n",
            "stumpiest\n",
            "sturdier\n",
            "sturdiest\n",
            "submariner\n",
            "sulkier\n",
            "sulkiest\n",
            "sultrier\n",
            "sultriest\n",
            "sunnier\n",
            "sunniest\n",
            "surlier\n",
            "surliest\n",
            "swagger\n",
            "swankier\n",
            "swankiest\n",
            "swarthier\n",
            "swarthiest\n",
            "sweatier\n",
            "sweatiest\n",
            "tackier\n",
            "tackiest\n",
            "talkier\n",
            "talkiest\n",
            "tangier\n",
            "tangiest\n",
            "tanner\n",
            "tannest\n",
            "tardier\n",
            "tardiest\n",
            "tastier\n",
            "tastiest\n",
            "tattier\n",
            "tattiest\n",
            "tawdrier\n",
            "tawdriest\n",
            "techier\n",
            "techiest\n",
            "teenager\n",
            "teenier\n",
            "teeniest\n",
            "teetotaler\n",
            "tester\n",
            "testier\n",
            "testiest\n",
            "tetchier\n",
            "tetchiest\n",
            "thinner\n",
            "thinnest\n",
            "third-rater\n",
            "thirstier\n",
            "thirstiest\n",
            "thornier\n",
            "thorniest\n",
            "threadier\n",
            "threadiest\n",
            "thriftier\n",
            "thriftiest\n",
            "throatier\n",
            "throatiest\n",
            "tidier\n",
            "tidiest\n",
            "timelier\n",
            "timeliest\n",
            "tinier\n",
            "tiniest\n",
            "tinnier\n",
            "tinniest\n",
            "tipsier\n",
            "tipsiest\n",
            "tonier\n",
            "toniest\n",
            "toothier\n",
            "toothiest\n",
            "toper\n",
            "touchier\n",
            "touchiest\n",
            "trader\n",
            "trashier\n",
            "trashiest\n",
            "trendier\n",
            "trendiest\n",
            "trickier\n",
            "trickiest\n",
            "tricksier\n",
            "tricksiest\n",
            "trimer\n",
            "trimmer\n",
            "trimmest\n",
            "truer\n",
            "truest\n",
            "trustier\n",
            "trustiest\n",
            "tubbier\n",
            "tubbiest\n",
            "turfier\n",
            "turfiest\n",
            "tweedier\n",
            "tweediest\n",
            "twiggier\n",
            "twiggiest\n",
            "uglier\n",
            "ugliest\n",
            "unfriendlier\n",
            "unfriendliest\n",
            "ungainlier\n",
            "ungainliest\n",
            "ungodlier\n",
            "ungodliest\n",
            "unhappier\n",
            "unhappiest\n",
            "unhealthier\n",
            "unhealthiest\n",
            "unholier\n",
            "unholiest\n",
            "unrulier\n",
            "unruliest\n",
            "untidier\n",
            "untidiest\n",
            "vastier\n",
            "vastiest\n",
            "vest\n",
            "viewier\n",
            "viewiest\n",
            "wackier\n",
            "wackiest\n",
            "wanner\n",
            "wannest\n",
            "warier\n",
            "wariest\n",
            "washier\n",
            "washiest\n",
            "waster\n",
            "wavier\n",
            "waviest\n",
            "waxier\n",
            "waxiest\n",
            "weaklier\n",
            "weakliest\n",
            "wealthier\n",
            "wealthiest\n",
            "wearier\n",
            "weariest\n",
            "webbier\n",
            "webbiest\n",
            "weedier\n",
            "weediest\n",
            "weenier\n",
            "weeniest\n",
            "weensier\n",
            "weensiest\n",
            "weepier\n",
            "weepiest\n",
            "weightier\n",
            "weightiest\n",
            "welsher\n",
            "wetter\n",
            "wettest\n",
            "whackier\n",
            "whackiest\n",
            "whimsier\n",
            "whimsiest\n",
            "wholesaler\n",
            "wieldier\n",
            "wieldiest\n",
            "wilier\n",
            "wiliest\n",
            "windier\n",
            "windiest\n",
            "winier\n",
            "winiest\n",
            "winterier\n",
            "winteriest\n",
            "wintrier\n",
            "wintriest\n",
            "wirier\n",
            "wiriest\n",
            "wispier\n",
            "wispiest\n",
            "wittier\n",
            "wittiest\n",
            "wonkier\n",
            "wonkiest\n",
            "woodier\n",
            "woodiest\n",
            "woodsier\n",
            "woodsiest\n",
            "woollier\n",
            "woolliest\n",
            "woozier\n",
            "wooziest\n",
            "wordier\n",
            "wordiest\n",
            "worldlier\n",
            "worldliest\n",
            "wormier\n",
            "wormiest\n",
            "worse\n",
            "worst\n",
            "worthier\n",
            "worthiest\n",
            "wrier\n",
            "wriest\n",
            "wryer\n",
            "wryest\n",
            "yarer\n",
            "yarest\n",
            "yeastier\n",
            "yeastiest\n",
            "younger\n",
            "youngest\n",
            "yummier\n",
            "yummiest\n",
            "zanier\n",
            "zaniest\n",
            "zippier\n",
            "zippiest\n",
            "adv.exc\n",
            "best\n",
            "better\n",
            "deeper\n",
            "farther\n",
            "further\n",
            "harder\n",
            "hardest\n",
            "verb.exc\n",
            "abetted\n",
            "abetting\n",
            "abhorred\n",
            "abhorring\n",
            "abode\n",
            "abought\n",
            "about-shipped\n",
            "about-shipping\n",
            "abutted\n",
            "abutting\n",
            "abye\n",
            "accompanied\n",
            "acetified\n",
            "acidified\n",
            "acquitted\n",
            "acquitting\n",
            "ad-libbed\n",
            "ad-libbing\n",
            "addrest\n",
            "admitted\n",
            "admitting\n",
            "aerified\n",
            "air-dried\n",
            "airdropped\n",
            "airdropping\n",
            "alkalified\n",
            "allied\n",
            "allotted\n",
            "allotting\n",
            "allowed_for\n",
            "allowing_for\n",
            "allows_for\n",
            "am\n",
            "ammonified\n",
            "amnestied\n",
            "amplified\n",
            "anglified\n",
            "annulled\n",
            "annulling\n",
            "appalled\n",
            "appalling\n",
            "applied\n",
            "arcked\n",
            "arcking\n",
            "are\n",
            "argufied\n",
            "arisen\n",
            "arose\n",
            "ate\n",
            "atrophied\n",
            "averred\n",
            "averring\n",
            "awoke\n",
            "awoken\n",
            "babied\n",
            "baby-sat\n",
            "baby-sitting\n",
            "back-pedalled\n",
            "back-pedalling\n",
            "backbit\n",
            "backbitten\n",
            "backslid\n",
            "backslidden\n",
            "bade\n",
            "bagged\n",
            "bagging\n",
            "ballyragged\n",
            "ballyragging\n",
            "bandied\n",
            "banned\n",
            "banning\n",
            "barred\n",
            "barrelled\n",
            "barrelling\n",
            "barring\n",
            "basified\n",
            "batted\n",
            "batting\n",
            "bayonetted\n",
            "bayonetting\n",
            "beaten\n",
            "beatified\n",
            "beautified\n",
            "became\n",
            "became_known\n",
            "becomes_known\n",
            "bed\n",
            "bedded\n",
            "bedding\n",
            "bedevilled\n",
            "bedevilling\n",
            "bedimmed\n",
            "bedimming\n",
            "been\n",
            "befallen\n",
            "befell\n",
            "befitted\n",
            "befitting\n",
            "befogged\n",
            "befogging\n",
            "began\n",
            "begat\n",
            "begetting\n",
            "begged\n",
            "begging\n",
            "beginning\n",
            "begirt\n",
            "begot\n",
            "begotten\n",
            "begun\n",
            "beheld\n",
            "beholden\n",
            "bejewelled\n",
            "bejewelling\n",
            "bellied\n",
            "belly-flopped\n",
            "belly-flopping\n",
            "belying\n",
            "benefitted\n",
            "benefitting\n",
            "benempt\n",
            "bent\n",
            "berried\n",
            "besetting\n",
            "besought\n",
            "bespoke\n",
            "bespoken\n",
            "bestirred\n",
            "bestirring\n",
            "bestrewn\n",
            "bestrid\n",
            "bestridden\n",
            "bestrode\n",
            "betaken\n",
            "bethought\n",
            "betook\n",
            "betted\n",
            "betting\n",
            "bevelled\n",
            "bevelling\n",
            "biassed\n",
            "biassing\n",
            "bidden\n",
            "bidding\n",
            "bing\n",
            "binned\n",
            "binning\n",
            "bird-dogged\n",
            "bird-dogging\n",
            "bit\n",
            "bitted\n",
            "bitten\n",
            "bitting\n",
            "bivouacked\n",
            "bivouacking\n",
            "blabbed\n",
            "blabbing\n",
            "blackberried\n",
            "blacklegged\n",
            "blacklegging\n",
            "blatted\n",
            "blatting\n",
            "bled\n",
            "blest\n",
            "blew\n",
            "blew_one's_nose\n",
            "blipped\n",
            "blipping\n",
            "blobbed\n",
            "blobbing\n",
            "bloodied\n",
            "blotted\n",
            "blotting\n",
            "blowing_one's_nose\n",
            "blown\n",
            "blows_one's_nose\n",
            "blubbed\n",
            "blubbing\n",
            "blue-pencilled\n",
            "blue-pencilling\n",
            "blurred\n",
            "blurring\n",
            "bobbed\n",
            "bobbing\n",
            "bodied\n",
            "bogged-down\n",
            "bogged_down\n",
            "bogging-down\n",
            "bogging_down\n",
            "bogs-down\n",
            "bogs_down\n",
            "booby-trapped\n",
            "booby-trapping\n",
            "bootlegged\n",
            "bootlegging\n",
            "bopped\n",
            "bopping\n",
            "bore\n",
            "born\n",
            "borne\n",
            "bottle-fed\n",
            "bought\n",
            "bound\n",
            "bragged\n",
            "bragging\n",
            "breast-fed\n",
            "bred\n",
            "brevetted\n",
            "brevetting\n",
            "brimmed\n",
            "brimming\n",
            "broke\n",
            "broken\n",
            "brought\n",
            "browbeaten\n",
            "brutified\n",
            "budded\n",
            "budding\n",
            "bugged\n",
            "bugging\n",
            "built\n",
            "bulldogging\n",
            "bullied\n",
            "bullshitted\n",
            "bullshitting\n",
            "bullwhipped\n",
            "bullwhipping\n",
            "bullyragged\n",
            "bullyragging\n",
            "bummed\n",
            "bumming\n",
            "buried\n",
            "burnt\n",
            "burred\n",
            "burring\n",
            "bushelled\n",
            "bushelling\n",
            "busied\n",
            "bypast\n",
            "caballed\n",
            "caballing\n",
            "caddied\n",
            "caddies\n",
            "caddying\n",
            "calcified\n",
            "came\n",
            "canalled\n",
            "canalling\n",
            "cancelled\n",
            "cancelling\n",
            "candied\n",
            "canned\n",
            "canning\n",
            "canopied\n",
            "capped\n",
            "capping\n",
            "carburetted\n",
            "carburetting\n",
            "carillonned\n",
            "carillonning\n",
            "carnied\n",
            "carnified\n",
            "carolled\n",
            "carolling\n",
            "carried\n",
            "casefied\n",
            "catnapped\n",
            "catnapping\n",
            "catted\n",
            "catting\n",
            "caught\n",
            "cavilled\n",
            "cavilling\n",
            "certified\n",
            "channelled\n",
            "channelling\n",
            "chapped\n",
            "chapping\n",
            "charred\n",
            "charring\n",
            "chatted\n",
            "chatting\n",
            "chevied\n",
            "chevies\n",
            "chevying\n",
            "chid\n",
            "chidden\n",
            "chinned\n",
            "chinning\n",
            "chipped\n",
            "chipping\n",
            "chiselled\n",
            "chiselling\n",
            "chitchatted\n",
            "chitchatting\n",
            "chivied\n",
            "chivved\n",
            "chivvied\n",
            "chivvies\n",
            "chivving\n",
            "chivvying\n",
            "chondrified\n",
            "chopped\n",
            "chopping\n",
            "chose\n",
            "chosen\n",
            "chugged\n",
            "chugging\n",
            "chummed\n",
            "chumming\n",
            "citified\n",
            "clad\n",
            "cladding\n",
            "clammed\n",
            "clamming\n",
            "clapped\n",
            "clapping\n",
            "clarified\n",
            "classified\n",
            "cleft\n",
            "clemmed\n",
            "clemming\n",
            "clept\n",
            "clipped\n",
            "clipping\n",
            "clogged\n",
            "clogging\n",
            "clopped\n",
            "clopping\n",
            "clotted\n",
            "clotting\n",
            "clove\n",
            "cloven\n",
            "clubbed\n",
            "clubbing\n",
            "clung\n",
            "co-opted\n",
            "co-opting\n",
            "co-opts\n",
            "co-ordinate\n",
            "co-ordinated\n",
            "co-ordinates\n",
            "co-ordinating\n",
            "co-starred\n",
            "co-starring\n",
            "cockneyfied\n",
            "codded\n",
            "codding\n",
            "codified\n",
            "cogged\n",
            "cogging\n",
            "coiffed\n",
            "coiffing\n",
            "collied\n",
            "combatted\n",
            "combatting\n",
            "committed\n",
            "committing\n",
            "compelled\n",
            "compelling\n",
            "complied\n",
            "complotted\n",
            "complotting\n",
            "concurred\n",
            "concurring\n",
            "confabbed\n",
            "confabbing\n",
            "conferred\n",
            "conferring\n",
            "conned\n",
            "conning\n",
            "controlled\n",
            "controlling\n",
            "copied\n",
            "copped\n",
            "copping\n",
            "coquetted\n",
            "coquetting\n",
            "corralled\n",
            "corralling\n",
            "counselled\n",
            "counselling\n",
            "counterplotted\n",
            "counterplotting\n",
            "countersank\n",
            "countersunk\n",
            "court-martialled\n",
            "court-martialling\n",
            "crabbed\n",
            "crabbing\n",
            "crammed\n",
            "cramming\n",
            "crapped\n",
            "crapping\n",
            "crept\n",
            "cribbed\n",
            "cribbing\n",
            "cried\n",
            "cropped\n",
            "cropping\n",
            "crossbred\n",
            "crosscutting\n",
            "crucified\n",
            "cubbed\n",
            "cubbing\n",
            "cudgelled\n",
            "cudgelling\n",
            "cupelled\n",
            "cupelling\n",
            "cupped\n",
            "cupping\n",
            "curetted\n",
            "curettes\n",
            "curetting\n",
            "curried\n",
            "curst\n",
            "curtsied\n",
            "curvetted\n",
            "curvetting\n",
            "cutting\n",
            "dabbed\n",
            "dabbing\n",
            "dagged\n",
            "dagging\n",
            "dallied\n",
            "dammed\n",
            "damming\n",
            "damnified\n",
            "dandified\n",
            "dapped\n",
            "dapping\n",
            "dealt\n",
            "debarred\n",
            "debarring\n",
            "debugged\n",
            "debugging\n",
            "debussed\n",
            "debusses\n",
            "debussing\n",
            "decalcified\n",
            "declassified\n",
            "decontrolled\n",
            "decontrolling\n",
            "decried\n",
            "deep-freeze\n",
            "deep-freezed\n",
            "deep-freezes\n",
            "deep-fried\n",
            "deferred\n",
            "deferring\n",
            "defied\n",
            "degassed\n",
            "degasses\n",
            "degassing\n",
            "dehumidified\n",
            "deified\n",
            "demitted\n",
            "demitting\n",
            "demobbed\n",
            "demobbing\n",
            "demulsified\n",
            "demurred\n",
            "demurring\n",
            "demystified\n",
            "denazified\n",
            "denied\n",
            "denitrified\n",
            "denned\n",
            "denning\n",
            "descried\n",
            "deterred\n",
            "deterring\n",
            "detoxified\n",
            "devilled\n",
            "devilling\n",
            "devitrified\n",
            "diagrammed\n",
            "diagramming\n",
            "dialled\n",
            "dialling\n",
            "dibbed\n",
            "dibbing\n",
            "did\n",
            "digging\n",
            "dignified\n",
            "dilly-dallied\n",
            "dimmed\n",
            "dimming\n",
            "dinned\n",
            "dinning\n",
            "dipped\n",
            "dipping\n",
            "dirtied\n",
            "disannulled\n",
            "disannulling\n",
            "disbarred\n",
            "disbarring\n",
            "disbudded\n",
            "disbudding\n",
            "disembodied\n",
            "disembowelled\n",
            "disembowelling\n",
            "disenthralled\n",
            "disenthralling\n",
            "disenthralls\n",
            "disenthrals\n",
            "dishevelled\n",
            "dishevelling\n",
            "disinterred\n",
            "disinterring\n",
            "dispelled\n",
            "dispelling\n",
            "disqualified\n",
            "dissatisfied\n",
            "distilled\n",
            "distilling\n",
            "diversified\n",
            "divvied\n",
            "dizzied\n",
            "dogged\n",
            "dogging\n",
            "doglegged\n",
            "doglegging\n",
            "dollied\n",
            "done\n",
            "donned\n",
            "donning\n",
            "dotted\n",
            "dotting\n",
            "dought\n",
            "dove\n",
            "drabbed\n",
            "drabbing\n",
            "dragged\n",
            "dragging\n",
            "drank\n",
            "drawn\n",
            "dreamt\n",
            "drew\n",
            "dried\n",
            "dripped\n",
            "dripping\n",
            "drivelled\n",
            "drivelling\n",
            "driven\n",
            "dropped\n",
            "dropping\n",
            "drove\n",
            "drubbed\n",
            "drubbing\n",
            "drugged\n",
            "drugging\n",
            "drummed\n",
            "drumming\n",
            "drunk\n",
            "dubbed\n",
            "dubbing\n",
            "duelled\n",
            "duelling\n",
            "dug\n",
            "dulcified\n",
            "dummied\n",
            "dunned\n",
            "dunning\n",
            "dwelt\n",
            "dying\n",
            "easied\n",
            "eaten\n",
            "eavesdropped\n",
            "eavesdropping\n",
            "eddied\n",
            "edified\n",
            "ego-tripped\n",
            "ego-tripping\n",
            "electrified\n",
            "embedded\n",
            "embedding\n",
            "embodied\n",
            "embussed\n",
            "embusses\n",
            "embussing\n",
            "emitted\n",
            "emitting\n",
            "empanelled\n",
            "empanelling\n",
            "emptied\n",
            "emulsified\n",
            "enamelled\n",
            "enamelling\n",
            "englutted\n",
            "englutting\n",
            "enrolled\n",
            "enrolling\n",
            "enthralled\n",
            "enthralling\n",
            "entrammelled\n",
            "entrammelling\n",
            "entrapped\n",
            "entrapping\n",
            "envied\n",
            "enwound\n",
            "enwrapped\n",
            "enwrapping\n",
            "equalled\n",
            "equalling\n",
            "equipped\n",
            "equipping\n",
            "espied\n",
            "esterified\n",
            "estopped\n",
            "estopping\n",
            "etherified\n",
            "excelled\n",
            "excelling\n",
            "exemplified\n",
            "expelled\n",
            "expelling\n",
            "extolled\n",
            "extolling\n",
            "facetted\n",
            "facetting\n",
            "fagged\n",
            "fagging\n",
            "fallen\n",
            "falsified\n",
            "fancied\n",
            "fanned\n",
            "fanning\n",
            "fantasied\n",
            "fatted\n",
            "fatting\n",
            "featherbedded\n",
            "featherbedding\n",
            "fed\n",
            "feed\n",
            "fell\n",
            "felt\n",
            "ferried\n",
            "fibbed\n",
            "fibbing\n",
            "figged\n",
            "figging\n",
            "filled_up\n",
            "fine-drawn\n",
            "fine-drew\n",
            "finned\n",
            "finning\n",
            "fitted\n",
            "fitting\n",
            "flagged\n",
            "flagging\n",
            "flammed\n",
            "flamming\n",
            "flannelled\n",
            "flannelling\n",
            "flapped\n",
            "flapping\n",
            "flatted\n",
            "flatting\n",
            "fled\n",
            "flew\n",
            "flimflammed\n",
            "flimflamming\n",
            "flip-flopped\n",
            "flip-flopping\n",
            "flipped\n",
            "flipping\n",
            "flitted\n",
            "flitting\n",
            "flogged\n",
            "flogging\n",
            "floodlit\n",
            "flopped\n",
            "flopping\n",
            "flown\n",
            "flubbed\n",
            "flubbing\n",
            "flung\n",
            "flurried\n",
            "flyblew\n",
            "flyblown\n",
            "fobbed\n",
            "fobbing\n",
            "fogged\n",
            "fogging\n",
            "footslogged\n",
            "footslogging\n",
            "forbad\n",
            "forbade\n",
            "forbidden\n",
            "forbidding\n",
            "forbore\n",
            "forborne\n",
            "force-fed\n",
            "fordid\n",
            "fordone\n",
            "foredid\n",
            "foredone\n",
            "foregone\n",
            "foreknew\n",
            "foreknown\n",
            "foreran\n",
            "forerunning\n",
            "foresaw\n",
            "foreseen\n",
            "foreshown\n",
            "forespoke\n",
            "forespoken\n",
            "foretold\n",
            "forewent\n",
            "forgave\n",
            "forgetting\n",
            "forgiven\n",
            "forgone\n",
            "forgot\n",
            "forgotten\n",
            "formatted\n",
            "formatting\n",
            "forsaken\n",
            "forsook\n",
            "forspoke\n",
            "forspoken\n",
            "forswore\n",
            "forsworn\n",
            "fortified\n",
            "forwent\n",
            "fought\n",
            "found\n",
            "foxtrotted\n",
            "foxtrotting\n",
            "frapped\n",
            "frapping\n",
            "freeze-dried\n",
            "frenchified\n",
            "frenzied\n",
            "fretted\n",
            "fretting\n",
            "fried\n",
            "frigged\n",
            "frigging\n",
            "fritted\n",
            "fritting\n",
            "frivolled\n",
            "frivolling\n",
            "frogged\n",
            "frogging\n",
            "frolicked\n",
            "frolicking\n",
            "froze\n",
            "frozen\n",
            "fructified\n",
            "fuelled\n",
            "fuelling\n",
            "fulfilled\n",
            "fulfilling\n",
            "funned\n",
            "funnelled\n",
            "funnelling\n",
            "funning\n",
            "furred\n",
            "furring\n",
            "gadded\n",
            "gadding\n",
            "gagged\n",
            "gagging\n",
            "gainsaid\n",
            "gambolled\n",
            "gambolling\n",
            "gammed\n",
            "gamming\n",
            "gan\n",
            "ganned\n",
            "ganning\n",
            "gapped\n",
            "gapping\n",
            "gasified\n",
            "gassed\n",
            "gasses\n",
            "gassing\n",
            "gave\n",
            "gelled\n",
            "gelling\n",
            "gelt\n",
            "gemmed\n",
            "gemming\n",
            "genned-up\n",
            "genning-up\n",
            "gens-up\n",
            "gets_lost\n",
            "gets_started\n",
            "getting\n",
            "getting_lost\n",
            "getting_started\n",
            "ghostwritten\n",
            "ghostwrote\n",
            "gibbed\n",
            "gibbing\n",
            "giddied\n",
            "giftwrapped\n",
            "giftwrapping\n",
            "gigged\n",
            "gigging\n",
            "gilt\n",
            "ginned\n",
            "ginning\n",
            "gipped\n",
            "gipping\n",
            "girt\n",
            "given\n",
            "glommed\n",
            "glomming\n",
            "gloried\n",
            "glorified\n",
            "glutted\n",
            "glutting\n",
            "gnawn\n",
            "goes_deep\n",
            "going_deep\n",
            "gollied\n",
            "gone\n",
            "gone_deep\n",
            "goose-stepped\n",
            "goose-stepping\n",
            "got\n",
            "got_lost\n",
            "got_started\n",
            "gotten\n",
            "gotten_lost\n",
            "grabbed\n",
            "grabbing\n",
            "gratified\n",
            "gravelled\n",
            "gravelling\n",
            "graven\n",
            "grew\n",
            "grinned\n",
            "grinning\n",
            "gripped\n",
            "gripping\n",
            "gript\n",
            "gritted\n",
            "gritting\n",
            "ground\n",
            "grovelled\n",
            "grovelling\n",
            "grown\n",
            "grubbed\n",
            "grubbing\n",
            "guarantied\n",
            "gullied\n",
            "gummed\n",
            "gumming\n",
            "gunned\n",
            "gunning\n",
            "gypped\n",
            "gypping\n",
            "hacksawn\n",
            "had\n",
            "had_a_feeling\n",
            "had_left\n",
            "had_the_feeling\n",
            "hammed\n",
            "hamming\n",
            "hamstrung\n",
            "hand-knitted\n",
            "hand-knitting\n",
            "handfed\n",
            "handicapped\n",
            "handicapping\n",
            "handselled\n",
            "handselling\n",
            "harried\n",
            "has\n",
            "has_a_feeling\n",
            "has_left\n",
            "has_the_feeling\n",
            "hatchelled\n",
            "hatchelling\n",
            "hatted\n",
            "hatting\n",
            "having_a_feeling\n",
            "having_left\n",
            "having_the_feeling\n",
            "heard\n",
            "hedgehopped\n",
            "hedgehopping\n",
            "held\n",
            "hemmed\n",
            "hemming\n",
            "hewn\n",
            "hiccupped\n",
            "hiccupping\n",
            "hid\n",
            "hidden\n",
            "high-hatted\n",
            "high-hatting\n",
            "hinnied\n",
            "hitting\n",
            "hobbed\n",
            "hobbing\n",
            "hobnobbed\n",
            "hobnobbing\n",
            "hocus-pocussed\n",
            "hocus-pocussing\n",
            "hocussed\n",
            "hocussing\n",
            "hogged\n",
            "hogging\n",
            "hogtying\n",
            "honied\n",
            "hopped\n",
            "hopping\n",
            "horrified\n",
            "horsewhipped\n",
            "horsewhipping\n",
            "houselled\n",
            "houselling\n",
            "hove\n",
            "hovelled\n",
            "hovelling\n",
            "hugged\n",
            "hugging\n",
            "humbugged\n",
            "humbugging\n",
            "humidified\n",
            "hummed\n",
            "humming\n",
            "hung\n",
            "hurried\n",
            "hypertrophied\n",
            "identified\n",
            "imbedded\n",
            "imbedding\n",
            "impanelled\n",
            "impanelling\n",
            "impelled\n",
            "impelling\n",
            "implied\n",
            "inbred\n",
            "incurred\n",
            "incurring\n",
            "indemnified\n",
            "indwelt\n",
            "inferred\n",
            "inferring\n",
            "initialled\n",
            "initialling\n",
            "inlaid\n",
            "insetting\n",
            "inspanned\n",
            "inspanning\n",
            "installed\n",
            "installing\n",
            "intensified\n",
            "interbred\n",
            "intercropped\n",
            "intercropping\n",
            "intercutting\n",
            "interlaid\n",
            "interlapped\n",
            "interlapping\n",
            "intermarried\n",
            "intermitted\n",
            "intermitting\n",
            "interpled\n",
            "interred\n",
            "interring\n",
            "interstratified\n",
            "interwove\n",
            "interwoven\n",
            "intromitted\n",
            "intromitting\n",
            "inwove\n",
            "inwoven\n",
            "inwrapped\n",
            "inwrapping\n",
            "is\n",
            "jabbed\n",
            "jabbing\n",
            "jagged\n",
            "jagging\n",
            "jammed\n",
            "jamming\n",
            "japanned\n",
            "japanning\n",
            "jarred\n",
            "jarring\n",
            "jellied\n",
            "jellified\n",
            "jemmied\n",
            "jerry-built\n",
            "jetted\n",
            "jetting\n",
            "jewelled\n",
            "jewelling\n",
            "jibbed\n",
            "jibbing\n",
            "jigged\n",
            "jigging\n",
            "jimmied\n",
            "jitterbugged\n",
            "jitterbugging\n",
            "jobbed\n",
            "jobbing\n",
            "jog-trotted\n",
            "jog-trotting\n",
            "jogged\n",
            "jogging\n",
            "joined_battle\n",
            "joined_forces\n",
            "joining_battle\n",
            "joining_forces\n",
            "joins_battle\n",
            "joins_forces\n",
            "jollied\n",
            "jollified\n",
            "jotted\n",
            "jotting\n",
            "joy-ridden\n",
            "joy-rode\n",
            "joypopped\n",
            "joypopping\n",
            "jugged\n",
            "jugging\n",
            "jumped_off\n",
            "jumping_off\n",
            "jumps_off\n",
            "justified\n",
            "jutted\n",
            "jutting\n",
            "kenned\n",
            "kennelled\n",
            "kennelling\n",
            "kenning\n",
            "kent\n",
            "kept\n",
            "kernelled\n",
            "kernelling\n",
            "kidded\n",
            "kidding\n",
            "kidnapped\n",
            "kidnapping\n",
            "kipped\n",
            "kipping\n",
            "knapped\n",
            "knapping\n",
            "kneecapped\n",
            "kneecapping\n",
            "knelt\n",
            "knew\n",
            "knitted\n",
            "knitting\n",
            "knobbed\n",
            "knobbing\n",
            "knotted\n",
            "knotting\n",
            "known\n",
            "ko'd\n",
            "ko'ing\n",
            "ko's\n",
            "labelled\n",
            "labelling\n",
            "laden\n",
            "ladyfied\n",
            "ladyfies\n",
            "ladyfying\n",
            "lagged\n",
            "lagging\n",
            "laid\n",
            "lain\n",
            "lallygagged\n",
            "lallygagging\n",
            "lammed\n",
            "lamming\n",
            "lapidified\n",
            "lapped\n",
            "lapping\n",
            "laurelled\n",
            "laurelling\n",
            "lay\n",
            "layed_for\n",
            "laying_for\n",
            "lays_for\n",
            "leant\n",
            "leapfrogged\n",
            "leapfrogging\n",
            "leapt\n",
            "learnt\n",
            "leaves_undone\n",
            "leaving_undone\n",
            "led\n",
            "left\n",
            "left_undone\n",
            "lent\n",
            "letting\n",
            "levelled\n",
            "levelling\n",
            "levied\n",
            "libelled\n",
            "libelling\n",
            "lignified\n",
            "lipped\n",
            "lipping\n",
            "liquefied\n",
            "liquified\n",
            "lit\n",
            "lobbed\n",
            "lobbied\n",
            "lobbing\n",
            "logged\n",
            "logging\n",
            "looked_towards\n",
            "looking_towards\n",
            "looks_towards\n",
            "lopped\n",
            "lopping\n",
            "lost\n",
            "lotted\n",
            "lotting\n",
            "lugged\n",
            "lugging\n",
            "lullabied\n",
            "lying\n",
            "machine-gunned\n",
            "machine-gunning\n",
            "madded\n",
            "madding\n",
            "made\n",
            "magnified\n",
            "manned\n",
            "manning\n",
            "manumitted\n",
            "manumitting\n",
            "mapped\n",
            "mapping\n",
            "marcelled\n",
            "marcelling\n",
            "marred\n",
            "married\n",
            "marring\n",
            "marshalled\n",
            "marshalling\n",
            "marvelled\n",
            "marvelling\n",
            "matted\n",
            "matting\n",
            "meant\n",
            "medalled\n",
            "medalling\n",
            "met\n",
            "metalled\n",
            "metalling\n",
            "metrified\n",
            "might\n",
            "militated_against\n",
            "militates_against\n",
            "militating_against\n",
            "mimicked\n",
            "mimicking\n",
            "minified\n",
            "misapplied\n",
            "misbecame\n",
            "miscarried\n",
            "misdealt\n",
            "misfitted\n",
            "misfitting\n",
            "misgave\n",
            "misgiven\n",
            "mishitting\n",
            "mislaid\n",
            "misled\n",
            "mispled\n",
            "misspelt\n",
            "misspent\n",
            "mistaken\n",
            "mistook\n",
            "misunderstood\n",
            "mobbed\n",
            "mobbing\n",
            "modelled\n",
            "modelling\n",
            "modified\n",
            "mollified\n",
            "molten\n",
            "mopped\n",
            "mopping\n",
            "mortified\n",
            "mown\n",
            "mudded\n",
            "muddied\n",
            "mudding\n",
            "mugged\n",
            "mugging\n",
            "multiplied\n",
            "mummed\n",
            "mummified\n",
            "mumming\n",
            "mutinied\n",
            "mystified\n",
            "nabbed\n",
            "nabbing\n",
            "nagged\n",
            "nagging\n",
            "napped\n",
            "napping\n",
            "netted\n",
            "netting\n",
            "nibbed\n",
            "nibbing\n",
            "nickelled\n",
            "nickelling\n",
            "nid-nodded\n",
            "nid-nodding\n",
            "nidified\n",
            "nigrified\n",
            "nipped\n",
            "nipping\n",
            "nitrified\n",
            "nodded\n",
            "nodding\n",
            "non-prossed\n",
            "non-prosses\n",
            "non-prossing\n",
            "nonplussed\n",
            "nonplusses\n",
            "nonplussing\n",
            "notified\n",
            "nullified\n",
            "nutted\n",
            "nutting\n",
            "objectified\n",
            "occupied\n",
            "occurred\n",
            "occurring\n",
            "offsetting\n",
            "omitted\n",
            "omitting\n",
            "ossified\n",
            "outbidden\n",
            "outbidding\n",
            "outbred\n",
            "outcried\n",
            "outcropped\n",
            "outcropping\n",
            "outdid\n",
            "outdone\n",
            "outdrawn\n",
            "outdrew\n",
            "outfitted\n",
            "outfitting\n",
            "outfought\n",
            "outgassed\n",
            "outgasses\n",
            "outgassing\n",
            "outgeneralled\n",
            "outgeneralling\n",
            "outgone\n",
            "outgrew\n",
            "outgrown\n",
            "outlaid\n",
            "outmanned\n",
            "outmanning\n",
            "outputted\n",
            "outputting\n",
            "outran\n",
            "outridden\n",
            "outrode\n",
            "outrunning\n",
            "outshone\n",
            "outshot\n",
            "outsold\n",
            "outspanned\n",
            "outspanning\n",
            "outstood\n",
            "outstripped\n",
            "outstripping\n",
            "outthought\n",
            "outwent\n",
            "outwitted\n",
            "outwitting\n",
            "outwore\n",
            "outworn\n",
            "overbidden\n",
            "overbidding\n",
            "overblew\n",
            "overblown\n",
            "overbore\n",
            "overborne\n",
            "overbuilt\n",
            "overcame\n",
            "overcropped\n",
            "overcropping\n",
            "overdid\n",
            "overdone\n",
            "overdrawn\n",
            "overdrew\n",
            "overdriven\n",
            "overdrove\n",
            "overflew\n",
            "overflown\n",
            "overgrew\n",
            "overgrown\n",
            "overheard\n",
            "overhung\n",
            "overlaid\n",
            "overlain\n",
            "overlapped\n",
            "overlapping\n",
            "overlay\n",
            "overlying\n",
            "overmanned\n",
            "overmanning\n",
            "overpaid\n",
            "overpast\n",
            "overran\n",
            "overridden\n",
            "overrode\n",
            "overrunning\n",
            "oversaw\n",
            "overseen\n",
            "oversetting\n",
            "oversewn\n",
            "overshot\n",
            "oversimplified\n",
            "overslept\n",
            "oversold\n",
            "overspent\n",
            "overspilt\n",
            "overstepped\n",
            "overstepping\n",
            "overtaken\n",
            "overthrew\n",
            "overthrown\n",
            "overtook\n",
            "overtopped\n",
            "overtopping\n",
            "overwound\n",
            "overwritten\n",
            "overwrote\n",
            "pacified\n",
            "padded\n",
            "padding\n",
            "paid\n",
            "palled\n",
            "palling\n",
            "palsied\n",
            "pandied\n",
            "panelled\n",
            "panelling\n",
            "panicked\n",
            "panicking\n",
            "panned\n",
            "panning\n",
            "parallelled\n",
            "parallelling\n",
            "parcelled\n",
            "parcelling\n",
            "parodied\n",
            "parried\n",
            "partaken\n",
            "partook\n",
            "pasquil\n",
            "pasquilled\n",
            "pasquilling\n",
            "pasquils\n",
            "patrolled\n",
            "patrolling\n",
            "patted\n",
            "patting\n",
            "pedalled\n",
            "pedalling\n",
            "pegged\n",
            "pegging\n",
            "pencilled\n",
            "pencilling\n",
            "penned\n",
            "penning\n",
            "pent\n",
            "pepped\n",
            "pepping\n",
            "permitted\n",
            "permitting\n",
            "personified\n",
            "petrified\n",
            "petted\n",
            "pettifogged\n",
            "pettifogging\n",
            "petting\n",
            "phantasied\n",
            "photocopied\n",
            "photomapped\n",
            "photomapping\n",
            "photosetting\n",
            "physicked\n",
            "physicking\n",
            "picnicked\n",
            "picnicking\n",
            "pigged\n",
            "pigging\n",
            "pilloried\n",
            "pinch-hitting\n",
            "pinned\n",
            "pinning\n",
            "pipped\n",
            "pipping\n",
            "pistol-whipped\n",
            "pistol-whipping\n",
            "pistolled\n",
            "pistolling\n",
            "pitapatted\n",
            "pitapatting\n",
            "pitied\n",
            "pitted\n",
            "pitting\n",
            "planned\n",
            "planning\n",
            "platted\n",
            "platting\n",
            "played_a_part\n",
            "playing_a_part\n",
            "plays_a_part\n",
            "pled\n",
            "plied\n",
            "plodded\n",
            "plodding\n",
            "plopped\n",
            "plopping\n",
            "plotted\n",
            "plotting\n",
            "plugged\n",
            "plugging\n",
            "podded\n",
            "podding\n",
            "pommelled\n",
            "pommelling\n",
            "popes\n",
            "popped\n",
            "popping\n",
            "potted\n",
            "potting\n",
            "preachified\n",
            "precancelled\n",
            "precancelling\n",
            "preferred\n",
            "preferring\n",
            "preoccupied\n",
            "prepaid\n",
            "presignified\n",
            "pretermitted\n",
            "pretermitting\n",
            "prettied\n",
            "prettified\n",
            "pried\n",
            "prigged\n",
            "prigging\n",
            "primmed\n",
            "primming\n",
            "prodded\n",
            "prodding\n",
            "programmed\n",
            "programmes\n",
            "programming\n",
            "prologed\n",
            "prologing\n",
            "prologs\n",
            "propelled\n",
            "propelling\n",
            "prophesied\n",
            "propped\n",
            "propping\n",
            "proven\n",
            "pubbed\n",
            "pubbing\n",
            "pugged\n",
            "pugging\n",
            "pummelled\n",
            "pummelling\n",
            "punned\n",
            "punning\n",
            "pupped\n",
            "pupping\n",
            "purified\n",
            "put-putted\n",
            "put-putting\n",
            "putrefied\n",
            "puttied\n",
            "putting\n",
            "qualified\n",
            "quantified\n",
            "quarrelled\n",
            "quarrelling\n",
            "quarried\n",
            "quartersawn\n",
            "queried\n",
            "quick-froze\n",
            "quick-frozen\n",
            "quickstepped\n",
            "quickstepping\n",
            "quipped\n",
            "quipping\n",
            "quitted\n",
            "quitting\n",
            "quizzed\n",
            "quizzes\n",
            "quizzing\n",
            "ragged\n",
            "ragging\n",
            "rallied\n",
            "ramified\n",
            "rammed\n",
            "ramming\n",
            "ran\n",
            "rang\n",
            "rapped\n",
            "rappelled\n",
            "rappelling\n",
            "rapping\n",
            "rarefied\n",
            "ratified\n",
            "ratted\n",
            "ratting\n",
            "ravelled\n",
            "ravelling\n",
            "razor-cutting\n",
            "re-trod\n",
            "re-trodden\n",
            "rebelled\n",
            "rebelling\n",
            "rebuilt\n",
            "rebutted\n",
            "rebutting\n",
            "recapped\n",
            "recapping\n",
            "reclassified\n",
            "recommitted\n",
            "recommitting\n",
            "recopied\n",
            "rectified\n",
            "recurred\n",
            "recurring\n",
            "red\n",
            "red-pencilled\n",
            "red-pencilling\n",
            "redded\n",
            "redding\n",
            "redid\n",
            "redone\n",
            "referred\n",
            "referring\n",
            "refitted\n",
            "refitting\n",
            "reft\n",
            "refuelled\n",
            "refuelling\n",
            "regretted\n",
            "regretting\n",
            "reheard\n",
            "reified\n",
            "relied\n",
            "remade\n",
            "remarried\n",
            "remitted\n",
            "remitting\n",
            "rent\n",
            "repaid\n",
            "repelled\n",
            "repelling\n",
            "replevied\n",
            "replied\n",
            "repotted\n",
            "repotting\n",
            "reran\n",
            "rerunning\n",
            "resat\n",
            "resetting\n",
            "resewn\n",
            "resitting\n",
            "retaken\n",
            "rethought\n",
            "retold\n",
            "retook\n",
            "retransmitted\n",
            "retransmitting\n",
            "retried\n",
            "retrofitted\n",
            "retrofitting\n",
            "retted\n",
            "retting\n",
            "reunified\n",
            "revelled\n",
            "revelling\n",
            "revetted\n",
            "revetting\n",
            "revivified\n",
            "revved\n",
            "revving\n",
            "rewound\n",
            "rewritten\n",
            "rewrote\n",
            "ribbed\n",
            "ribbing\n",
            "ricochetted\n",
            "ricochetting\n",
            "ridded\n",
            "ridden\n",
            "ridding\n",
            "rigged\n",
            "rigging\n",
            "rigidified\n",
            "rimmed\n",
            "rimming\n",
            "ripped\n",
            "ripping\n",
            "risen\n",
            "rivalled\n",
            "rivalling\n",
            "riven\n",
            "robbed\n",
            "robbing\n",
            "rode\n",
            "rose\n",
            "rotted\n",
            "rotting\n",
            "rough-dried\n",
            "rough-hewn\n",
            "rove\n",
            "rowelled\n",
            "rowelling\n",
            "rubbed\n",
            "rubbing\n",
            "rung\n",
            "running\n",
            "rutted\n",
            "rutting\n",
            "saccharified\n",
            "sagged\n",
            "sagging\n",
            "said\n",
            "salaried\n",
            "salified\n",
            "sallied\n",
            "sanctified\n",
            "sandbagged\n",
            "sandbagging\n",
            "sang\n",
            "sank\n",
            "saponified\n",
            "sapped\n",
            "sapping\n",
            "sat\n",
            "satisfied\n",
            "savvied\n",
            "saw\n",
            "sawn\n",
            "scagged\n",
            "scagging\n",
            "scanned\n",
            "scanning\n",
            "scarified\n",
            "scarred\n",
            "scarring\n",
            "scatted\n",
            "scatting\n",
            "scorified\n",
            "scragged\n",
            "scragging\n",
            "scrammed\n",
            "scramming\n",
            "scrapped\n",
            "scrapping\n",
            "scried\n",
            "scrubbed\n",
            "scrubbing\n",
            "scrummed\n",
            "scrumming\n",
            "scudded\n",
            "scudding\n",
            "scummed\n",
            "scumming\n",
            "scurried\n",
            "seed\n",
            "seen\n",
            "sent\n",
            "setting\n",
            "sewn\n",
            "shagged\n",
            "shagging\n",
            "shaken\n",
            "shaken_hands\n",
            "shakes_hands\n",
            "shaking_hands\n",
            "shammed\n",
            "shamming\n",
            "sharecropped\n",
            "sharecropping\n",
            "shat\n",
            "shaven\n",
            "shed\n",
            "shedding\n",
            "shellacked\n",
            "shellacking\n",
            "shent\n",
            "shewn\n",
            "shied\n",
            "shikarred\n",
            "shikarring\n",
            "shillyshallied\n",
            "shimmed\n",
            "shimmied\n",
            "shimming\n",
            "shinned\n",
            "shinning\n",
            "shipped\n",
            "shipping\n",
            "shitted\n",
            "shitting\n",
            "shod\n",
            "shone\n",
            "shook\n",
            "shook_hands\n",
            "shopped\n",
            "shopping\n",
            "shot\n",
            "shotgunned\n",
            "shotgunning\n",
            "shotted\n",
            "shotting\n",
            "shovelled\n",
            "shovelling\n",
            "shown\n",
            "shrank\n",
            "shredded\n",
            "shredding\n",
            "shrink-wrapped\n",
            "shrink-wrapping\n",
            "shrivelled\n",
            "shrivelling\n",
            "shriven\n",
            "shrove\n",
            "shrugged\n",
            "shrugging\n",
            "shrunk\n",
            "shrunken\n",
            "shunned\n",
            "shunning\n",
            "shutting\n",
            "sicked\n",
            "sicking\n",
            "sideslipped\n",
            "sideslipping\n",
            "sidestepped\n",
            "sidestepping\n",
            "sightsaw\n",
            "sightseen\n",
            "signalled\n",
            "signalling\n",
            "signified\n",
            "silicified\n",
            "simplified\n",
            "singing\n",
            "single-stepped\n",
            "single-stepping\n",
            "sinned\n",
            "sinning\n",
            "sipped\n",
            "sipping\n",
            "sitting\n",
            "skellied\n",
            "skenned\n",
            "skenning\n",
            "sketted\n",
            "sketting\n",
            "ski'd\n",
            "skidded\n",
            "skidding\n",
            "skimmed\n",
            "skimming\n",
            "skin-popped\n",
            "skin-popping\n",
            "skinned\n",
            "skinning\n",
            "skinny-dipped\n",
            "skinny-dipping\n",
            "skipped\n",
            "skipping\n",
            "skivvied\n",
            "skydove\n",
            "slabbed\n",
            "slabbing\n",
            "slagged\n",
            "slagging\n",
            "slain\n",
            "slammed\n",
            "slamming\n",
            "slapped\n",
            "slapping\n",
            "slatted\n",
            "slatting\n",
            "sledding\n",
            "slept\n",
            "slew\n",
            "slid\n",
            "slidden\n",
            "slipped\n",
            "slipping\n",
            "slitting\n",
            "slogged\n",
            "slogging\n",
            "slopped\n",
            "slopping\n",
            "slotted\n",
            "slotting\n",
            "slugged\n",
            "slugging\n",
            "slummed\n",
            "slumming\n",
            "slung\n",
            "slunk\n",
            "slurred\n",
            "slurring\n",
            "smelt\n",
            "smit\n",
            "smitten\n",
            "smote\n",
            "smutted\n",
            "smutting\n",
            "snagged\n",
            "snagging\n",
            "snapped\n",
            "snapping\n",
            "snedded\n",
            "snedding\n",
            "snipped\n",
            "snipping\n",
            "snivelled\n",
            "snivelling\n",
            "snogged\n",
            "snogging\n",
            "snubbed\n",
            "snubbing\n",
            "snuck\n",
            "snugged\n",
            "snugging\n",
            "sobbed\n",
            "sobbing\n",
            "sodded\n",
            "sodding\n",
            "soft-pedalled\n",
            "soft-pedalling\n",
            "sold\n",
            "solemnified\n",
            "solidified\n",
            "soothsaid\n",
            "sopped\n",
            "sopping\n",
            "sought\n",
            "sown\n",
            "spagged\n",
            "spagging\n",
            "spancelled\n",
            "spancelling\n",
            "spanned\n",
            "spanning\n",
            "sparred\n",
            "sparring\n",
            "spat\n",
            "spatted\n",
            "spatting\n",
            "specified\n",
            "sped\n",
            "speechified\n",
            "spellbound\n",
            "spelt\n",
            "spent\n",
            "spied\n",
            "spilt\n",
            "spin-dried\n",
            "spinning\n",
            "spiralled\n",
            "spiralling\n",
            "spitted\n",
            "spitting\n",
            "splitting\n",
            "spoilt\n",
            "spoke\n",
            "spoken\n",
            "spoon-fed\n",
            "spotlit\n",
            "spotted\n",
            "spotting\n",
            "sprang\n",
            "sprigged\n",
            "sprigging\n",
            "sprung\n",
            "spudded\n",
            "spudding\n",
            "spun\n",
            "spurred\n",
            "spurring\n",
            "squatted\n",
            "squatting\n",
            "squibbed\n",
            "squibbing\n",
            "squidded\n",
            "squidding\n",
            "squilgee\n",
            "stabbed\n",
            "stabbing\n",
            "stall-fed\n",
            "stank\n",
            "starred\n",
            "starring\n",
            "steadied\n",
            "stellified\n",
            "stemmed\n",
            "stemming\n",
            "stems_from\n",
            "stencilled\n",
            "stencilling\n",
            "stepped\n",
            "stepping\n",
            "stetted\n",
            "stetting\n",
            "stied\n",
            "stilettoeing\n",
            "stirred\n",
            "stirring\n",
            "stole\n",
            "stolen\n",
            "stood\n",
            "stopped\n",
            "stopping\n",
            "storied\n",
            "stotted\n",
            "stotting\n",
            "stove\n",
            "strapped\n",
            "strapping\n",
            "stratified\n",
            "strewn\n",
            "stridden\n",
            "stripped\n",
            "stripping\n",
            "striven\n",
            "strode\n",
            "stropped\n",
            "stropping\n",
            "strove\n",
            "strown\n",
            "struck\n",
            "strummed\n",
            "strumming\n",
            "strung\n",
            "strutted\n",
            "strutting\n",
            "stubbed\n",
            "stubbing\n",
            "stuck\n",
            "studded\n",
            "studding\n",
            "studied\n",
            "stultified\n",
            "stummed\n",
            "stumming\n",
            "stung\n",
            "stunk\n",
            "stunned\n",
            "stunning\n",
            "stupefied\n",
            "stymying\n",
            "subbed\n",
            "subbing\n",
            "subjectified\n",
            "subletting\n",
            "submitted\n",
            "submitting\n",
            "subtotalled\n",
            "subtotalling\n",
            "sullied\n",
            "sulphuretted\n",
            "sulphuretting\n",
            "summed\n",
            "summing\n",
            "sung\n",
            "sunk\n",
            "sunken\n",
            "sunned\n",
            "sunning\n",
            "supped\n",
            "supping\n",
            "supplied\n",
            "swabbed\n",
            "swabbing\n",
            "swagged\n",
            "swagging\n",
            "swam\n",
            "swapped\n",
            "swapping\n",
            "swatted\n",
            "swatting\n",
            "swept\n",
            "swigged\n",
            "swigging\n",
            "swimming\n",
            "swivelled\n",
            "swivelling\n",
            "swollen\n",
            "swopped\n",
            "swopping\n",
            "swops\n",
            "swore\n",
            "sworn\n",
            "swotted\n",
            "swotting\n",
            "swum\n",
            "swung\n",
            "syllabified\n",
            "symbolled\n",
            "symbolling\n",
            "tabbed\n",
            "tabbing\n",
            "tagged\n",
            "tagging\n",
            "taken\n",
            "taken_a_side\n",
            "taken_pains\n",
            "taken_steps\n",
            "takes_a_side\n",
            "takes_pains\n",
            "takes_steps\n",
            "taking_a_side\n",
            "taking_pains\n",
            "taking_steps\n",
            "talcked\n",
            "talcking\n",
            "tallied\n",
            "tally-ho'd\n",
            "tammied\n",
            "tanned\n",
            "tanning\n",
            "tapped\n",
            "tapping\n",
            "tarred\n",
            "tarried\n",
            "tarring\n",
            "tasselled\n",
            "tasselling\n",
            "tatted\n",
            "tatting\n",
            "taught\n",
            "taxis\n",
            "taxying\n",
            "teaselled\n",
            "teaselling\n",
            "tedded\n",
            "tedding\n",
            "tepefied\n",
            "terrified\n",
            "testes\n",
            "testified\n",
            "thinking_the_world_of\n",
            "thinks_the_world_of\n",
            "thinned\n",
            "thinning\n",
            "thought\n",
            "thought_the_world_of\n",
            "threw\n",
            "threw_out\n",
            "thriven\n",
            "throbbed\n",
            "throbbing\n",
            "throve\n",
            "throwing_out\n",
            "thrown\n",
            "thrown_out\n",
            "throws_out\n",
            "thrummed\n",
            "thrumming\n",
            "thudded\n",
            "thudding\n",
            "tidied\n",
            "tinned\n",
            "tinning\n",
            "tinselled\n",
            "tinselling\n",
            "tipped\n",
            "tipping\n",
            "tittupped\n",
            "tittupping\n",
            "toadied\n",
            "togged\n",
            "togging\n",
            "told\n",
            "took\n",
            "took_a_side\n",
            "took_pains\n",
            "took_steps\n",
            "topped\n",
            "topping\n",
            "tore\n",
            "torn\n",
            "torrefied\n",
            "torrify\n",
            "totalled\n",
            "totalling\n",
            "totted\n",
            "totting\n",
            "towelled\n",
            "towelling\n",
            "trafficked\n",
            "trafficking\n",
            "trameled\n",
            "trameling\n",
            "tramelled\n",
            "tramelling\n",
            "tramels\n",
            "trammed\n",
            "tramming\n",
            "transferred\n",
            "transferring\n",
            "transfixt\n",
            "tranship\n",
            "transhipped\n",
            "transhipping\n",
            "transmitted\n",
            "transmitting\n",
            "transmogrified\n",
            "transshipped\n",
            "transshipping\n",
            "trapanned\n",
            "trapanning\n",
            "trapped\n",
            "trapping\n",
            "travelled\n",
            "travelling\n",
            "travestied\n",
            "trekked\n",
            "trekking\n",
            "trepanned\n",
            "trepanning\n",
            "tried\n",
            "trigged\n",
            "trigging\n",
            "trimmed\n",
            "trimming\n",
            "tripped\n",
            "tripping\n",
            "trod\n",
            "trodden\n",
            "trogged\n",
            "trogging\n",
            "trotted\n",
            "trotting\n",
            "trowelled\n",
            "trowelling\n",
            "tugged\n",
            "tugging\n",
            "tumefied\n",
            "tunned\n",
            "tunnelled\n",
            "tunnelling\n",
            "tunning\n",
            "tupped\n",
            "tupping\n",
            "tut-tutted\n",
            "tut-tutting\n",
            "twigged\n",
            "twigging\n",
            "twinned\n",
            "twinning\n",
            "twitted\n",
            "twitting\n",
            "tying\n",
            "typesetting\n",
            "typewritten\n",
            "typewrote\n",
            "typified\n",
            "uglified\n",
            "unbarred\n",
            "unbarring\n",
            "unbent\n",
            "unbound\n",
            "uncapped\n",
            "uncapping\n",
            "unclad\n",
            "unclogged\n",
            "unclogging\n",
            "underbidding\n",
            "underbought\n",
            "undercutting\n",
            "underfed\n",
            "undergirt\n",
            "undergone\n",
            "underlaid\n",
            "underlain\n",
            "underlay\n",
            "underletting\n",
            "underlying\n",
            "underpaid\n",
            "underpinned\n",
            "underpinning\n",
            "underpropped\n",
            "underpropping\n",
            "undersetting\n",
            "undershot\n",
            "undersold\n",
            "understood\n",
            "understudied\n",
            "undertaken\n",
            "undertook\n",
            "underwent\n",
            "underwritten\n",
            "underwrote\n",
            "undid\n",
            "undone\n",
            "unfitted\n",
            "unfitting\n",
            "unfroze\n",
            "unfrozen\n",
            "unified\n",
            "unkennelled\n",
            "unkennelling\n",
            "unknitted\n",
            "unknitting\n",
            "unlaid\n",
            "unlearnt\n",
            "unmade\n",
            "unmanned\n",
            "unmanning\n",
            "unpegged\n",
            "unpegging\n",
            "unpinned\n",
            "unpinning\n",
            "unplugged\n",
            "unplugging\n",
            "unravelled\n",
            "unravelling\n",
            "unrigged\n",
            "unrigging\n",
            "unripped\n",
            "unripping\n",
            "unrove\n",
            "unsaid\n",
            "unshipped\n",
            "unshipping\n",
            "unslung\n",
            "unsnapped\n",
            "unsnapping\n",
            "unspoke\n",
            "unspoken\n",
            "unsteadied\n",
            "unstepped\n",
            "unstepping\n",
            "unstopped\n",
            "unstopping\n",
            "unstrung\n",
            "unstuck\n",
            "unswore\n",
            "unsworn\n",
            "untaught\n",
            "unthought\n",
            "untidied\n",
            "untrod\n",
            "untrodden\n",
            "untying\n",
            "unwound\n",
            "unwrapped\n",
            "unwrapping\n",
            "unzipped\n",
            "unzipping\n",
            "upbuilt\n",
            "upheld\n",
            "uphove\n",
            "upped\n",
            "uppercutting\n",
            "upping\n",
            "uprisen\n",
            "uprose\n",
            "upsetting\n",
            "upsprang\n",
            "upsprung\n",
            "upswept\n",
            "upswollen\n",
            "upswung\n",
            "vagged\n",
            "vagging\n",
            "varied\n",
            "vatted\n",
            "vatting\n",
            "verbified\n",
            "verified\n",
            "versified\n",
            "vetted\n",
            "vetting\n",
            "victualled\n",
            "victualling\n",
            "vilified\n",
            "vitrified\n",
            "vitriolled\n",
            "vitriolling\n",
            "vivified\n",
            "vying\n",
            "wadded\n",
            "waddied\n",
            "wadding\n",
            "wadsetted\n",
            "wadsetting\n",
            "wagged\n",
            "wagging\n",
            "wanned\n",
            "wanning\n",
            "warred\n",
            "warring\n",
            "was\n",
            "water-ski'd\n",
            "waylaid\n",
            "wearied\n",
            "weatherstripped\n",
            "weatherstripping\n",
            "webbed\n",
            "webbing\n",
            "wedded\n",
            "wedding\n",
            "weed\n",
            "went\n",
            "went_deep\n",
            "wept\n",
            "were\n",
            "wetted\n",
            "wetting\n",
            "whammed\n",
            "whamming\n",
            "whapped\n",
            "whapping\n",
            "whetted\n",
            "whetting\n",
            "whinnied\n",
            "whipped\n",
            "whipping\n",
            "whipsawn\n",
            "whirred\n",
            "whirring\n",
            "whistle-stopped\n",
            "whistle-stopping\n",
            "whizzed\n",
            "whizzes\n",
            "whizzing\n",
            "whopped\n",
            "whopping\n",
            "wigged\n",
            "wigging\n",
            "wigwagged\n",
            "wigwagging\n",
            "wildcatted\n",
            "wildcatting\n",
            "window-shopped\n",
            "window-shopping\n",
            "winning\n",
            "winterfed\n",
            "wiredrawn\n",
            "wiredrew\n",
            "withdrawn\n",
            "withdrew\n",
            "withheld\n",
            "withstood\n",
            "woke\n",
            "woken\n",
            "won\n",
            "wonned\n",
            "wonning\n",
            "wore\n",
            "worn\n",
            "worried\n",
            "worshipped\n",
            "worshipping\n",
            "wound\n",
            "wove\n",
            "woven\n",
            "wrapped\n",
            "wrapping\n",
            "wried\n",
            "written\n",
            "wrote\n",
            "wrought\n",
            "wrung\n",
            "yakked\n",
            "yakking\n",
            "yapped\n",
            "yapping\n",
            "ycleped\n",
            "yclept\n",
            "yenned\n",
            "yenning\n",
            "yodelled\n",
            "yodelling\n",
            "zapped\n",
            "zapping\n",
            "zigzagged\n",
            "zigzagging\n",
            "zipped\n",
            "zipping\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorboardX"
      ],
      "metadata": {
        "id": "asPxLctOjCi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tensorboardX"
      ],
      "metadata": {
        "id": "uo3fZ0CUjAIp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b98b8eb-5c8a-4ae5-b29a-6e2f0bdcdde7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.8/dist-packages (2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorboardX) (23.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Copy Source Codes"
      ],
      "metadata": {
        "id": "lspwNGu7jLzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/Fast_Campus/ExSum/SRC ."
      ],
      "metadata": {
        "id": "V9ksOpSnjEhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b93692b-0397-447a-8704-8cc510d5a233"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/drive/MyDrive/Fast_Campus/ExSum/SRC': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 Import "
      ],
      "metadata": {
        "id": "GkXfgw6JjPjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import easydict"
      ],
      "metadata": {
        "id": "pRAwyKjJjN3I"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터\n",
        "\n",
        "\n",
        "###원본 데이터 탐색\n",
        "- 데이터 구성\n",
        "  - 원문 데이터 40만 건 (신문기사 30만 건, 기고문 6만 건, 잡지기사 1만 건, 법원 판결문 3만 건)을 활용하여 각각 추출요약 40만 건, 생성요약 40만 건, 총 80만 건의 요약문 도출\n",
        "  - 원문으로부터 변형 없이 그대로 선택된 3개 문장으로 추출요약문 생성\n",
        "  - 원문의 내용을 바탕으로 재작성된 생성요약문 생성"
      ],
      "metadata": {
        "id": "oyUWdh3CnSei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAPATH = '/content/drive/MyDrive/인공지능/추출요약/data/raw_data/train'\n",
        "filenames = [x for x in os.listdir (DATAPATH) if x.endswith('json')]\n",
        "filenames.sort()\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4aLJuzSjShA",
        "outputId": "a7b5a2cf-3e1c-4f22-cd0d-b670470da042"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_original_law.json',\n",
              " 'train_original_news.json',\n",
              " 'train_original_opinion.json']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = filenames[0]\n",
        "filelocation = os.path.join(DATAPATH, file)\n",
        "\n",
        "with open(filelocation, 'r') as json_file:\n",
        "  data = json.load(json_file) ['documents']"
      ],
      "metadata": {
        "id": "Gq0TJ2fFnzdL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmQwzvMgoBS8",
        "outputId": "5e977be2-765d-4e5c-db6e-e0008d372eff"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '100004',\n",
              " 'category': '일반행정',\n",
              " 'size': 'small',\n",
              " 'char_count': 377,\n",
              " 'publish_date': '19841226',\n",
              " 'title': '부당노동행위구제재심판정취소',\n",
              " 'text': [[{'index': 0,\n",
              "    'sentence': '원고가 소속회사의 노동조합에서 분규가 발생하자 노조활동을 구실로 정상적인 근무를 해태하고,',\n",
              "    'highlight_indices': ''},\n",
              "   {'index': 1, 'sentence': '노조조합장이 사임한 경우,', 'highlight_indices': ''},\n",
              "   {'index': 2,\n",
              "    'sentence': '노동조합규약에 동 조합장의 직무를 대행할 자를 규정해 두고 있음에도 원고 자신이 주동하여 노조자치수습대책위원회를 구성하여 그 위원장으로 피선되어 근무시간중에도 노조활동을 벌여 운수업체인 소속회사의 업무에 지장을 초래하고',\n",
              "    'highlight_indices': '8,9;68,69'},\n",
              "   {'index': 3,\n",
              "    'sentence': '종업원들에게도 나쁜 영향을 끼쳐 소속회사가 취업규칙을 위반하고',\n",
              "    'highlight_indices': ''},\n",
              "   {'index': 4,\n",
              "    'sentence': '고의로 회사업무능률을 저해하였으며 회사업무상의 지휘명령에 위반하였음을 이유로 원고를 징계해고 하였다면,',\n",
              "    'highlight_indices': '0,3'},\n",
              "   {'index': 5,\n",
              "    'sentence': '이는 원고의 노동조합 활동과는 관계없이 회사취업규칙에 의하여 사내질서를 유지하기 위한 사용자 고유의 징계권에 기하여 이루어진 정당한 징계권의 행사로 보아야 한다.',\n",
              "    'highlight_indices': '17,21'}]],\n",
              " 'annotator_id': 3783,\n",
              " 'document_quality_scores': {'readable': 3,\n",
              "  'accurate': 3,\n",
              "  'informative': 3,\n",
              "  'trustworthy': 3},\n",
              " 'extractive': [5, 4, 2],\n",
              " 'abstractive': ['원고가  주동하여 회사업무능률을 저해하고 회사업무상의 지휘명령에 위반하였다면 이에 따른 징계해고는 사내질서를 유지하기 위한 사용자 고유의 정당한 징계권의 행사로 보아야 한다.']}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 원본 데이터에서 필요한 값 추출\n",
        "- text와 extractive 추출\n",
        "  - text : 정규식을 이용해 sentence - highlight_indices 사이의 문장 추출한 후 리스트로 저장\n",
        "  - extracive : 3줄 요약에 해당하는 문장 index 3개가 저장된 리스트\n",
        "- 신문기사, 기고문, 법원 판결분 3개로 나누어져 잇는 파일을 하나로 합침\n",
        "  - train, valid 각각에 대해 수행"
      ],
      "metadata": {
        "id": "fZLg_iNCrBEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train 데이터"
      ],
      "metadata": {
        "id": "Bt6iHAncr59u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAPATH = '/content/drive/MyDrive/인공지능/추출요약/data/raw_data/train'\n",
        "filenames = [x for x in os.listdir (DATAPATH) if x.endswith('json')]\n",
        "filenames.sort()\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoOkgAtAoEGM",
        "outputId": "4577af8a-8c40-41aa-f0b2-bfe52c97e6ad"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_original_law.json',\n",
              " 'train_original_news.json',\n",
              " 'train_original_opinion.json']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list_dic = []\n",
        "\n",
        "# for file in filenames:\n",
        "#   filelocation = os.path.join(DATAPATH, file)\n",
        "\n",
        "#   with open(filelocation, 'r') as json_file:\n",
        "#     data = json.load(json_file)['documents']\n",
        "\n",
        "#     for x in tqdm (range(len(data))):\n",
        "#       text = data[x]['text']\n",
        "#       text = str(text).replace('\"', \"'\")\n",
        "\n",
        "#       extractive = data[x]['extractive']\n",
        "#       for index, value in enumerate(extractive):\n",
        "#         if value == None:\n",
        "#           extractive[index] = 0\n",
        "\n",
        "#       p = re.compile('(?<=sentence\\'\\: \\')(.*?)(?=\\'highlight_indices)')\n",
        "#       texts = p.findall(text)\n",
        "\n",
        "#       sentences = []\n",
        "#       for t in texts:\n",
        "#         sentence = t[:-3]\n",
        "#         sentences.append(sentence)\n",
        "\n",
        "#       mydict = {}\n",
        "#       mydict['text'] = sentences\n",
        "#       mydict['extractive'] = extractive\n",
        "#       list_dic.append(mydict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEFekWp5sLEc",
        "outputId": "5264eeb4-593e-4c90-d7d0-eeef109fa95c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24329/24329 [00:01<00:00, 16558.77it/s]\n",
            "100%|██████████| 243983/243983 [00:20<00:00, 11715.99it/s]\n",
            "100%|██████████| 56760/56760 [00:07<00:00, 8017.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_dic)"
      ],
      "metadata": {
        "id": "mmg7GrQSu971"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/drive/MyDrive/인공지능/추출요약/data/raw_data/train.json\", 'w') as fh:\n",
        "#   json.dump(list_dic, fh)"
      ],
      "metadata": {
        "id": "evjSqF9jukmJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def list_chunk(lst, n):\n",
        "#     return [lst[i:i+n] for i in range(0, len(lst), n)]\n",
        "\n",
        "# data_chunked = list_chunk(data, 32507) ## 전체 데이터를 10개로 분할\n",
        "\n",
        "# for i, d in enumerate(data_chunked):\n",
        "#   with open(\"/content/drive/MyDrive/인공지능/추출요약/data/raw_data/train.{}.json\".format(i), 'w') as fh:\n",
        "#     json.dump(d, fh)"
      ],
      "metadata": {
        "id": "LrxQT1E4ttHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### val "
      ],
      "metadata": {
        "id": "K5ML9zfXv70C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAPATH = '/content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid'\n",
        "filenames = [x for x in os.listdir (DATAPATH) if x.endswith('json')]\n",
        "filenames.sort()\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R22bBuDvuc-X",
        "outputId": "3e659831-15b8-41c4-b0e5-604d0225d206"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['valid_original_law.json',\n",
              " 'valid_original_news.json',\n",
              " 'valid_original_opinion.json']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list_dic = []\n",
        "\n",
        "# for file in filenames:\n",
        "#   filelocation = os.path.join(DATAPATH, file)\n",
        "\n",
        "#   with open(filelocation, 'r') as json_file:\n",
        "#     data = json.load(json_file)['documents']\n",
        "\n",
        "#     for x in tqdm (range(len(data))):\n",
        "#       text = data[x]['text']\n",
        "#       text = str(text).replace('\"', \"'\")\n",
        "\n",
        "#       extractive = data[x]['extractive']\n",
        "#       for index, value in enumerate(extractive):\n",
        "#         if value == None:\n",
        "#           extractive[index] = 0\n",
        "\n",
        "#       p = re.compile('(?<=sentence\\'\\: \\')(.*?)(?=\\'highlight_indices)')\n",
        "#       texts = p.findall(text)\n",
        "\n",
        "#       sentences = []\n",
        "#       for t in texts:\n",
        "#         sentence = t[:-3]\n",
        "#         sentences.append(sentence)\n",
        "\n",
        "#       mydict = {}\n",
        "#       mydict['text'] = sentences\n",
        "#       mydict['extractive'] = extractive\n",
        "#       list_dic.append(mydict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCjZwdn5wSBY",
        "outputId": "d2cf47dd-e125-4e0a-d209-967b97b1b946"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3004/3004 [00:00<00:00, 22788.49it/s]\n",
            "100%|██████████| 30122/30122 [00:02<00:00, 10430.85it/s]\n",
            "100%|██████████| 7008/7008 [00:00<00:00, 10395.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"/content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid.json\", 'w') as fh:\n",
        "#   json.dump(list_dic, fh)"
      ],
      "metadata": {
        "id": "0c6lPYLLwT-z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습가능한 형태로 데이터 변환\n",
        "### Tokenizing 후 .json 파일로 저장\n",
        "- Source 및 Target 설정\n",
        "  - Source; 원본 데이터의 'text'에 해당\n",
        "  - Target; 원본 데이터의 'extractive' 리스트의 index에 해당하는 문장을 source에서 추출\n",
        "- Source 및 Target 을 Tokenizing\n",
        "  - Tokenizer; MeCab + BertTokenizer\n",
        "    - 한국어 데이터이므로 MeCab으로 형태소 분석 후 BertTokenizer 사용\n",
        "  - Tokenizing 후 token 단위로 source 및 target 저장"
      ],
      "metadata": {
        "id": "kHaeFzYESibG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n",
        "\n",
        "mecab = Mecab()"
      ],
      "metadata": {
        "id": "1fvadQTwwe61"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train 데이터"
      ],
      "metadata": {
        "id": "WMgT7eFESyfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAPATH = '/content/drive/MyDrive/인공지능/추출요약/data/raw_data'\n",
        "filenames = [x for x in os.listdir (DATAPATH) if 'train' in x and x.endswith('json')]\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykscWhbtSxvo",
        "outputId": "4d3e4eb8-3d2e-4374-94da-96a25551a4f8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.json']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocess(set_name):\n",
        "  with open(\"/content/drive/MyDrive/인공지능/추출요약/data/raw_data/{}.json\".format(set_name), 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "    list_dic = []\n",
        "    for x in tqdm(range(len(data))):\n",
        "      text = data[x]['text']\n",
        "      extractive = data[x]['extractive']\n",
        "\n",
        "      sentences = []\n",
        "      for sentence in text:\n",
        "        sentence_morph = ' '.join(mecab.morphs(sentence))\n",
        "        sentences.append(sentence_morph)\n",
        "\n",
        "      extractives = []\n",
        "      for e in extractive:\n",
        "        extractives.append(sentences[e])\n",
        "\n",
        "      src = [i.split(' ') for i in sentences]\n",
        "      tgt = [i.split(' ') for i in extractives]\n",
        "\n",
        "      mydict = {}\n",
        "      mydict['src'] = src\n",
        "      mydict['tgt'] = tgt\n",
        "      list_dic.append(mydict)\n",
        "\n",
        "    jsonfilelocation = '/content/drive/MyDrive/인공지능/추출요약/data/json_data/' + set_name\n",
        "    os.makedirs(jsonfilelocation, exist_ok=True)\n",
        "\n",
        "    temp = []\n",
        "    DATA_PER_FILE = 50\n",
        "\n",
        "    for i,a in enumerate(tqdm(list_dic)):\n",
        "      if (i+1)%DATA_PER_FILE!=0:\n",
        "        temp.append(a)\n",
        "      else:\n",
        "        temp.append(a)\n",
        "        filename = 'korean.'+ set_name + '.' + str(i//DATA_PER_FILE)+'.json'\n",
        "        with open(os.path.join(jsonfilelocation, filename), \"w\", encoding='utf-8') as json_file:\n",
        "          json.dump(temp, json_file, ensure_ascii=False)\n",
        "          temp = []\n",
        "\n",
        "      #마지막에 남은 데이터 있으면 추가로 append\n",
        "      if len(temp) != 0:\n",
        "        filename = 'korean.'+ set_name + '.' + str(i//DATA_PER_FILE + 1)+'.json'\n",
        "        with open(os.path.join(jsonfilelocation, filename), \"w\", encoding='utf-8') as json_file:\n",
        "          json.dump(temp, json_file, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "EAVq7h0HfCfL"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_preprocess('train')\n",
        "# data_preprocess('valid')"
      ],
      "metadata": {
        "id": "jmrfu8AkfiN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee03cf4-f492-412c-ee42-31ba547da199"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325072/325072 [14:08<00:00, 383.21it/s]\n",
            "100%|██████████| 325072/325072 [1:12:42<00:00, 74.51it/s]\n",
            "100%|██████████| 40134/40134 [01:42<00:00, 392.35it/s]\n",
            "100%|██████████| 40134/40134 [08:44<00:00, 76.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT feature 생성 후 .pt 파일로 저장\n",
        "- format_to_bert 함수 이용\n",
        "- prepro > data_builder.py\n",
        "  - BertData() 클래스에서 tokenizer 설정\n",
        "  - BertTokenizer.from_pretrained('klue/bert-base', strip_accents=False, do_lower_case=False)"
      ],
      "metadata": {
        "id": "Ck0zkvbjedLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from os.path import join as pjoin\n",
        "import sys\n",
        "\n",
        "from multiprocess import Pool\n",
        "sys.path.append('/content/drive/MyDrive/인공지능/추출요약')\n",
        "from SRC.prepro.data_builder import _format_to_bert\n",
        "\n",
        "def format_to_bert(args):\n",
        "    if (args.dataset != ''):\n",
        "        datasets = args.dataset\n",
        "    else:\n",
        "        datasets = ['train', 'valid', 'test']\n",
        "    for corpus_type in datasets:\n",
        "        a_lst = []\n",
        "        for json_f in glob.glob(pjoin(args.raw_path, corpus_type, '*' + corpus_type + '.*.json')):\n",
        "            real_name = json_f.split('/')[-1]\n",
        "            a_lst.append((json_f, args, pjoin(args.save_path, real_name.replace('json', 'bert.pt'))))\n",
        "        # print(a_lst)\n",
        "        pool = Pool(args.n_cpus)\n",
        "        for d in pool.imap(_format_to_bert, a_lst):\n",
        "            pass\n",
        "\n",
        "        pool.close()\n",
        "        pool.join()"
      ],
      "metadata": {
        "id": "60Aczq2geW08"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_name = 'train'\n",
        "\n",
        "bertfilelocation = '/content/drive/MyDrive/인공지능/추출요약/data/bert_data/' + set_name\n",
        "os.makedirs(bertfilelocation, exist_ok=True)\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "  \"dataset\": [set_name], \n",
        "  \"raw_path\": \"/content/drive/MyDrive/인공지능/추출요약/data/json_data/\",\n",
        "  \"save_path\": bertfilelocation,\n",
        "  \"n_cpus\":4,\n",
        "  \"oracle_mode\": \"greedy\",\n",
        "  \"min_src_ntokens\": 5,\n",
        "  \"max_src_ntokens\": 200,\n",
        "  \"min_nsents\": 3,\n",
        "  \"max_nsents\": 100,\n",
        "})\n",
        "\n",
        "format_to_bert(args)"
      ],
      "metadata": {
        "id": "GuIuOhireIoq"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_name = 'valid'\n",
        "\n",
        "bertfilelocation = '/content/drive/MyDrive/인공지능/추출요약/data/bert_data/' + set_name\n",
        "os.makedirs(bertfilelocation, exist_ok=True)\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "  \"dataset\": [set_name], \n",
        "  \"raw_path\": \"/content/drive/MyDrive/인공지능/추출요약/data/json_data/\",\n",
        "  \"save_path\": bertfilelocation,\n",
        "  \"n_cpus\":4,\n",
        "  \"oracle_mode\": \"greedy\",\n",
        "  \"min_src_ntokens\": 5,\n",
        "  \"max_src_ntokens\": 200,\n",
        "  \"min_nsents\": 3,\n",
        "  \"max_nsents\": 100,\n",
        "})\n",
        "\n",
        "format_to_bert(args)"
      ],
      "metadata": {
        "id": "xwU6Jd8xesUi"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "### 1. 사전 학습 모델 로딩\n",
        "- models > model_builder.py\n",
        "  - `Bert()` 클래스에서 model 설정\n",
        "  - `BertModel.from_pretrained('klue/bert-base')`\n",
        "- train.py\n",
        "  - `validate`와 `test` 함수에서 config 설정\n",
        "  - `BertConfig.from_pretrained('klue/bert-base')`\n",
        "\n",
        "### 2. 인코더 설정\n",
        "- models > encoder.py\n",
        "  - `Classifier()` 클래스\n",
        "  - `TransformerInterEncoder()` 클래스\n",
        "  - `RNNEncoder()` 클래스\n",
        "- models > model_builder.py\n",
        "  - `Summarizer()` 클래스에서 encoder 설정"
      ],
      "metadata": {
        "id": "8kYR6o2vY_Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "### 1. 학습 loss 함수 설정\n",
        "- models > trainer.py\n",
        "  - `__init__` 함수에서 loss 설정\n",
        "  - `torch.nn.BCELoss(reduction='none')`\n",
        "\n",
        "### 2. 하이퍼파라미터 설정\n",
        "- 경로 관련 하이퍼파라미터\n",
        "  - model_path (Path): Fine-tuning한 모델이 저장되는 경로\n",
        "  - result_path (Path): 평가 시 추론 결과(candidate)와 정답(gold)이 저장되는 경로\n",
        "  - temp_dir (Path): 평가 시 ROUGE SCORE 결과가 저장되는 경로\n",
        "  - log_file (Path): 로그 파일이 저장되는 경로\n",
        "  - train_from (Path): 학습하고자 하는 체크포인트(pt)가 있을 경우 설정하는 경로\n",
        "  - bert_data_path (Path): 학습에 사용할 Bert Data(.pt)를 설정하는 경로\n",
        "\n",
        "- 모델 관련 하이퍼파라미터\n",
        "  - batch_size (int): A Batch Size (default=1000)\n",
        "  - use_interval (bool): 학습 시 문서 내에서 여러 문장을 구별하기 위해 Interval Segment Embeddings를 사용하는 지의 여부 (default=True)\n",
        "  - hidden_size (int): Transformer Hidden Size (default=128)\n",
        "  - ff_size (int): Feed-forward Filter Size (default=2048)\n",
        "  - heads (int): Transformer Head 개수 (default=8)\n",
        "  - inter_layers (int): Transformer Inter Layer 개수 (default=2)\n",
        "  - rnn_size (int): Encoder RNN RNN Size(default=512)\n",
        "  - train_steps(int): 학습할 Step 수\n",
        "\n",
        "- Optimizer 관련 하이퍼파라미터\n",
        "  - param_init (float): (default=0)\n",
        "  - param_init_glorot (bool): (default=True)\n",
        "  - dropout (float): Dropout (default=0.1)\n",
        "  - optim (str): Optimizer (default='adam')\n",
        "  - lr (float): Learning Rate (default=2e-3)\n",
        "  - beta1 (float): Adam Optimizer 관련 Hyper Parameter (default= 0.9)\n",
        "  - beta2 (float): Adam Optimizer 관련 Hyper Parameter (default=0.999)\n",
        "  - decay_method (str): Weight Decay (default='noam')\n",
        "  - warmup_steps (int): Warm-up Steps (default=6000)\n",
        "  - max_grad_norm (float): Max Gradient Noam (default=0)\n",
        "\n",
        "- 모델 저장 관련 하이퍼파라미터\n",
        "  - save_checkpoint steps (int): 모델 저장 주기\n",
        "\n",
        "- Multi-GPU 관련 하이퍼파라미터\n",
        "  - world_size (int): GPU 개수\n",
        "  - visible_gpus (str): GPU 번호\n",
        "  - gpu_ranks (str): GPU 번호\n",
        "  - seed (int): (default=666)\n",
        "\n",
        "- 기타 하이퍼파라미터\n",
        "  - accum_count (int): (default=2)\n",
        "  - report_every (int): (default=50)\n",
        "  - recall_eval (bool): (default=false)\n",
        "  - report_rouge (bool): (default=false)\n",
        "  - block_trigram (bool): (default=true)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0Jw9lw6vZKIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Multi-GPU 설정\n",
        "- train.py\n",
        "  - `multi_main` 과 `run` 함수 참조\n",
        "  - multiprocessing 중 spawn 사용\n",
        "\n",
        "```\n",
        "if(args.world_size>1):\n",
        "  multi_main(args)\n",
        "```\n",
        "- distributed.py\n",
        "  - `multi_init` 함수 참조\n",
        "  - 분산학습 사용\n",
        "- Colab 은 Multi-GPU를 지원하지 않으므로, 개인 장비에서 가능하다면 실행 추천\n"
      ],
      "metadata": {
        "id": "GLi08adjZT9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. ROUGE 설정\n",
        "- models > trainer.py\n",
        "  - `Trainer()` 클래스의 `test` 함수 참조\n",
        "- others > utils.py\n",
        "  - `test_rouge`, `rouge_results_to_str` 함수 사용"
      ],
      "metadata": {
        "id": "eIVd5emBZXPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 최종 학습 코드\n",
        "- train.py에서 mode를 train으로 선택"
      ],
      "metadata": {
        "id": "oqEZaXzhYLOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "j0KTYK5MuMK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a45904-48a9-46f3-be30-9fc0d9bd779f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 16 12:49:10 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classifier - 논문에는 5만 step"
      ],
      "metadata": {
        "id": "ihb_9J30Ykqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdirlocation = '/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE'\n",
        "os.makedirs(logdirlocation, exist_ok=True)\n",
        "\n",
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "    -mode train \\\n",
        "    -encoder classifier \\\n",
        "    -dropout 0.1 \\\n",
        "    -bert_data_path /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean \\\n",
        "    -model_path /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier \\\n",
        "    -lr 2e-3 \\\n",
        "    -visible_gpus 0 \\\n",
        "    -gpu_ranks 0 \\\n",
        "    -world_size 1 \\\n",
        "    -report_every 1000\\\n",
        "    -save_checkpoint_steps 100 \\\n",
        "    -batch_size 1000 \\\n",
        "    -decay_method noam \\\n",
        "    -train_steps 1000 \\\n",
        "    -accum_count 2 \\\n",
        "    -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_classifier.txt \\\n",
        "    -use_interval true \\\n",
        "    -warmup_steps 200\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZN5OL58YjnG",
        "outputId": "843abda9-8412-4399-f82d-77c22ab564eb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-16 12:50:45.214298: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-16 12:50:45.214389: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-16 12:50:45.214405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[2023-02-16 12:50:47,241 INFO] Device ID 0\n",
            "[2023-02-16 12:50:47,242 INFO] Device cuda\n",
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-02-16 12:50:51,137 INFO] Summarizer(\n",
            "  (bert): Bert(\n",
            "    (model): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (encoder): Classifier(\n",
            "    (linear1): Linear(in_features=768, out_features=1, bias=True)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2023-02-16 12:50:51,177 INFO] * number of parameters: 110618113\n",
            "[2023-02-16 12:50:51,177 INFO] Start training...\n",
            "[2023-02-16 12:50:51,311 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1061.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:50:56,676 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4507.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:50:58,916 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5969.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:01,499 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3086.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:03,586 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4782.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:05,239 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_100.pt\n",
            "[2023-02-16 12:51:08,358 INFO] Training Loss 3.081251\n",
            "[2023-02-16 12:51:08,956 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1399.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:11,145 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4626.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:13,418 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1833.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:15,659 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1130.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:18,089 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4328.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:19,678 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_200.pt\n",
            "[2023-02-16 12:51:22,945 INFO] Training Loss 2.618893\n",
            "[2023-02-16 12:51:23,847 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5416.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:26,823 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.839.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:29,360 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.752.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:31,461 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4215.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:33,872 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.231.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:35,227 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_300.pt\n",
            "[2023-02-16 12:51:38,318 INFO] Training Loss 2.393829\n",
            "[2023-02-16 12:51:38,913 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.2574.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:41,343 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6223.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:43,992 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5560.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:46,664 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3663.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:49,187 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3623.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:49,904 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_400.pt\n",
            "[2023-02-16 12:51:53,084 INFO] Training Loss 2.205525\n",
            "[2023-02-16 12:51:54,843 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1651.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:57,292 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.2533.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:51:59,704 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4147.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:01,930 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.2617.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:04,120 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_500.pt\n",
            "[2023-02-16 12:52:07,184 INFO] Training Loss 2.117506\n",
            "[2023-02-16 12:52:07,473 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4177.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:09,682 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6313.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:12,138 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5009.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:14,736 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6444.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:17,562 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6045.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:19,032 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_600.pt\n",
            "[2023-02-16 12:52:22,121 INFO] Training Loss 2.030086\n",
            "[2023-02-16 12:52:23,545 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4785.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:25,959 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3520.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:28,218 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5979.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:31,106 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1132.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:33,474 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6491.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:33,945 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_700.pt\n",
            "[2023-02-16 12:52:37,062 INFO] Training Loss 1.958028\n",
            "[2023-02-16 12:52:39,467 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3266.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:41,820 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5664.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:44,547 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1162.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:46,992 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5334.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:48,443 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_800.pt\n",
            "[2023-02-16 12:52:51,679 INFO] Training Loss 1.891127\n",
            "[2023-02-16 12:52:52,550 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.608.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:54,881 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4083.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:57,320 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.187.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:52:59,236 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3867.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:53:01,459 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.743.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:53:03,427 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_900.pt\n",
            "[2023-02-16 12:53:06,686 INFO] Training Loss 1.854558\n",
            "[2023-02-16 12:53:07,494 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1584.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:53:09,753 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1090.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:53:12,114 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1662.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:53:14,523 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4840.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:53:16,810 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6013.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:53:18,094 INFO] Step 1000/ 1000; xent: 1.83; lr: 0.0000632;  32 docs/s;    147 sec\n",
            "[2023-02-16 12:53:18,098 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_1000.pt\n",
            "[2023-02-16 12:53:21,188 INFO] Training Loss 0.000000\n",
            "[2023-02-16 12:53:21,343 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5962.bert.pt, number of examples: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5-2 RNN"
      ],
      "metadata": {
        "id": "lABM2F1-SmlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdirlocation = '/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE'\n",
        "os.makedirs(logdirlocation, exist_ok=True)\n",
        "\n",
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode train \\\n",
        "  -encoder rnn \\\n",
        "  -dropout 0.1 \\\n",
        "  -bert_data_path /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean \\\n",
        "  -model_path /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn \\\n",
        "  -lr 2e-3 \\\n",
        "  -visible_gpus 0 \\\n",
        "  -gpu_ranks 0 \\\n",
        "  -world_size 1 \\\n",
        "  -report_every 1000\\\n",
        "  -save_checkpoint_steps 100 \\\n",
        "  -batch_size 1000 \\\n",
        "  -decay_method noam \\\n",
        "  -train_steps 1000 \\\n",
        "  -accum_count 2 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_rnn.txt \\\n",
        "  -use_interval true \\\n",
        "  -warmup_steps 200 \\\n",
        "  -rnn_size 768 # 신경써서 해야함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNQ-hGwQdHdL",
        "outputId": "ee96e7b0-470d-4191-ccb0-9c3b051e2f04"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-16 12:57:07.334283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-16 12:57:07.334376: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-16 12:57:07.334394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[2023-02-16 12:57:09,203 INFO] Device ID 0\n",
            "[2023-02-16 12:57:09,204 INFO] Device cuda\n",
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-02-16 12:57:13,828 INFO] Summarizer(\n",
            "  (bert): Bert(\n",
            "    (model): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (encoder): RNNEncoder(\n",
            "    (rnn): LayerNormLSTM(\n",
            "      (hidden0): ModuleList(\n",
            "        (0): LayerNormLSTMCell(\n",
            "          768, 384\n",
            "          (ln_ih): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (ln_hh): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (ln_ho): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (hidden1): ModuleList(\n",
            "        (0): LayerNormLSTMCell(\n",
            "          768, 384\n",
            "          (ln_ih): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (ln_hh): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "          (ln_ho): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (wo): Linear(in_features=768, out_features=1, bias=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2023-02-16 12:57:13,897 INFO] * number of parameters: 114177025\n",
            "[2023-02-16 12:57:13,897 INFO] Start training...\n",
            "[2023-02-16 12:57:14,034 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1061.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:57:18,398 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4507.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:57:21,580 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5969.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:57:25,817 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3086.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:57:28,654 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4782.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:57:31,153 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_100.pt\n",
            "[2023-02-16 12:57:34,314 INFO] Training Loss 2.297948\n",
            "[2023-02-16 12:57:35,090 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1399.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:57:38,177 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4626.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:57:41,439 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1833.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:57:44,781 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1130.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:57:48,694 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4328.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:57:51,487 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_200.pt\n",
            "[2023-02-16 12:57:56,182 INFO] Training Loss 2.274303\n",
            "[2023-02-16 12:57:57,282 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5416.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:01,397 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.839.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:04,938 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.752.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:08,321 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4215.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:11,955 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.231.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:13,923 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_300.pt\n",
            "[2023-02-16 12:58:17,620 INFO] Training Loss 2.202384\n",
            "[2023-02-16 12:58:18,345 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.2574.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:21,636 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6223.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:25,871 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5560.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:30,652 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3663.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:34,461 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3623.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:35,655 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_400.pt\n",
            "[2023-02-16 12:58:39,024 INFO] Training Loss 2.070545\n",
            "[2023-02-16 12:58:41,252 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1651.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:44,722 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.2533.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:48,256 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4147.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:51,620 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.2617.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:58:55,146 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_500.pt\n",
            "[2023-02-16 12:58:58,483 INFO] Training Loss 2.012893\n",
            "[2023-02-16 12:58:58,757 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4177.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:01,953 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6313.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:06,103 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5009.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:10,407 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6444.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:14,837 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6045.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:17,281 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_600.pt\n",
            "[2023-02-16 12:59:20,411 INFO] Training Loss 1.941418\n",
            "[2023-02-16 12:59:22,483 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4785.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:25,651 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3520.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:28,984 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5979.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:33,970 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1132.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:37,802 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6491.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:38,480 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_700.pt\n",
            "[2023-02-16 12:59:41,707 INFO] Training Loss 1.890174\n",
            "[2023-02-16 12:59:45,381 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3266.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:48,470 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5664.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:52,829 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1162.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:57,281 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5334.bert.pt, number of examples: 50\n",
            "[2023-02-16 12:59:59,494 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_800.pt\n",
            "[2023-02-16 13:00:03,003 INFO] Training Loss 1.835096\n",
            "[2023-02-16 13:00:04,037 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.608.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:00:07,399 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4083.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:00:11,017 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.187.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:00:13,743 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3867.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:00:16,830 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.743.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:00:19,761 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_900.pt\n",
            "[2023-02-16 13:00:23,216 INFO] Training Loss 1.808650\n",
            "[2023-02-16 13:00:24,165 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1584.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:00:27,075 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1090.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:00:30,489 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1662.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:00:34,712 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4840.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:00:39,033 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6013.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:00:41,336 INFO] Step 1000/ 1000; xent: 1.79; lr: 0.0000632;  23 docs/s;    207 sec\n",
            "[2023-02-16 13:00:41,340 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_1000.pt\n",
            "[2023-02-16 13:00:44,703 INFO] Training Loss 0.000000\n",
            "[2023-02-16 13:00:44,838 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5962.bert.pt, number of examples: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5-3 Transformer"
      ],
      "metadata": {
        "id": "BQP6b3cQTO6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdirlocation = '/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE'\n",
        "os.makedirs(logdirlocation, exist_ok=True)\n",
        "\n",
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode train \\\n",
        "  -encoder transformer \\\n",
        "  -dropout 0.1 \\\n",
        "  -bert_data_path /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean \\\n",
        "  -model_path /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer \\\n",
        "  -lr 2e-3 \\\n",
        "  -visible_gpus 0 \\\n",
        "  -gpu_ranks 0 \\\n",
        "  -world_size 1 \\\n",
        "  -report_every 1000\\\n",
        "  -save_checkpoint_steps 100 \\\n",
        "  -batch_size 1000 \\\n",
        "  -decay_method noam \\\n",
        "  -train_steps 1000 \\\n",
        "  -accum_count 2 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_transformer.txt \\\n",
        "  -use_interval true \\\n",
        "  -warmup_steps 200 \\\n",
        "  -ff_size 2048 \\\n",
        "  -inter_layers 2 \\\n",
        "  -heads 8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz5ylGv4TMAK",
        "outputId": "01218369-1d8a-4679-99b2-30031d1498de"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-16 13:00:49.733720: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-16 13:00:49.733830: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-16 13:00:49.733851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[2023-02-16 13:00:51,656 INFO] Device ID 0\n",
            "[2023-02-16 13:00:51,657 INFO] Device cuda\n",
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[2023-02-16 13:00:56,701 INFO] Summarizer(\n",
            "  (bert): Bert(\n",
            "    (model): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (encoder): TransformerInterEncoder(\n",
            "    (pos_emb): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer_inter): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_values): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (linear_query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=768, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (layer_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "    (wo): Linear(in_features=768, out_features=1, bias=True)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2023-02-16 13:00:56,754 INFO] * number of parameters: 121647617\n",
            "[2023-02-16 13:00:56,754 INFO] Start training...\n",
            "[2023-02-16 13:00:56,901 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1061.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:00,759 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4507.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:03,452 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5969.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:06,477 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3086.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:08,791 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4782.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:10,710 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_100.pt\n",
            "[2023-02-16 13:01:14,301 INFO] Training Loss 2.187751\n",
            "[2023-02-16 13:01:16,455 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1399.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:18,905 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4626.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:21,513 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1833.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:24,290 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1130.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:27,306 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4328.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:29,426 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_200.pt\n",
            "[2023-02-16 13:01:33,046 INFO] Training Loss 2.247146\n",
            "[2023-02-16 13:01:33,974 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5416.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:36,907 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.839.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:39,479 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.752.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:41,826 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4215.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:44,362 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.231.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:45,832 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_300.pt\n",
            "[2023-02-16 13:01:49,480 INFO] Training Loss 2.229245\n",
            "[2023-02-16 13:01:50,218 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.2574.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:52,936 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6223.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:56,158 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5560.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:01:59,197 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3663.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:02,095 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3623.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:02,914 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_400.pt\n",
            "[2023-02-16 13:02:06,605 INFO] Training Loss 2.168181\n",
            "[2023-02-16 13:02:08,512 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1651.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:11,387 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.2533.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:14,207 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4147.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:16,759 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.2617.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:19,452 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_500.pt\n",
            "[2023-02-16 13:02:22,985 INFO] Training Loss 2.188936\n",
            "[2023-02-16 13:02:23,247 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4177.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:25,712 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6313.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:28,594 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5009.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:31,412 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6444.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:34,546 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6045.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:36,132 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_600.pt\n",
            "[2023-02-16 13:02:39,831 INFO] Training Loss 2.126147\n",
            "[2023-02-16 13:02:41,542 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4785.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:44,201 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3520.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:46,686 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5979.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:50,004 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1132.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:52,879 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6491.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:02:53,327 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_700.pt\n",
            "[2023-02-16 13:02:56,889 INFO] Training Loss 2.137321\n",
            "[2023-02-16 13:02:59,567 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3266.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:02,158 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5664.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:05,380 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1162.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:08,278 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5334.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:09,976 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_800.pt\n",
            "[2023-02-16 13:03:13,731 INFO] Training Loss 2.114011\n",
            "[2023-02-16 13:03:14,631 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.608.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:17,608 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4083.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:20,276 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.187.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:22,415 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.3867.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:25,109 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.743.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:27,350 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_900.pt\n",
            "[2023-02-16 13:03:31,043 INFO] Training Loss 2.102231\n",
            "[2023-02-16 13:03:31,864 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1584.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:34,365 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1090.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:37,260 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.1662.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:40,126 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.4840.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:42,748 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.6013.bert.pt, number of examples: 50\n",
            "[2023-02-16 13:03:44,221 INFO] Step 1000/ 1000; xent: 2.09; lr: 0.0000632;  28 docs/s;    167 sec\n",
            "[2023-02-16 13:03:44,226 INFO] Saving checkpoint /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_1000.pt\n",
            "[2023-02-16 13:03:48,939 INFO] Training Loss 0.000000\n",
            "[2023-02-16 13:03:49,092 INFO] Loading train dataset from /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean.train.5962.bert.pt, number of examples: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 최종 평가 코드\n",
        "- train.py에서 mode를 test로 선택\n",
        "- valid 데이터 사용\n",
        "  - 이때 데이터 이름을 test로 변경해야 진행되므로 `korean.valid.{}.bert.pt` 데이터를 `korean.test.{}.bert.pt` 데이터로 이름 변경\n",
        "- **주의**\n",
        "  - test 시 pyrouge 라이브러리를 통해 rouge score를 계산하게 되는데, Colab 상에서는 dependency 문제로 인해 에러 발생\n",
        "  - 위의 Install libraries 가이드대로 pyrouge 설치 후 SRC, 데이터, 모델을 로컬 디스크에 다운로드 후 로컬에서 실행해야 함\n"
      ],
      "metadata": {
        "id": "dnyw814OTpGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-1. Classifier"
      ],
      "metadata": {
        "id": "XF8C57FEVDRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdirlocation = '/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE'\n",
        "os.makedirs(logdirlocation, exist_ok=True)\n",
        "\n",
        "resdirlocation = '/content/drive/MyDrive/인공지능/추출요약/RESULT'\n",
        "os.makedirs(resdirlocation, exist_ok=True)\n",
        "\n",
        "tmpdirlocation = '/content/drive/MyDrive/인공지능/추출요약/TEMP'\n",
        "os.makedirs(tmpdirlocation, exist_ok=True)\n",
        "\n",
        "\n",
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode test \\\n",
        "  -bert_data_path /content/drive/MyDrive/인공지능/추출요약/data/bert_data/valid/korean \\\n",
        "  -model_path /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier \\\n",
        "  -result_path /content/drive/MyDrive/인공지능/추출요약/RESULT/ \\\n",
        "  -temp_dir /content/drive/MyDrive/인공지능/추출요약/TEMP/ \\\n",
        "  -visible_gpus 0 \\\n",
        "  -gpu_ranks 0 \\\n",
        "  -world_size 1 \\\n",
        "  -batch_size 30000 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_classifier.txt \\\n",
        "  -test_from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_1000.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxDeUlEYTkGu",
        "outputId": "84fb1dcd-0ff4-4b28-f454-f0c37c5cecec"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-16 13:09:42.578223: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-16 13:09:42.578315: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-16 13:09:42.578335: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[2023-02-16 13:09:44,522 INFO] Loading checkpoint from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_1000.pt\n",
            "Namespace(accum_count=1, batch_size=30000, bert_data_path='/content/drive/MyDrive/인공지능/추출요약/data/bert_data/valid/korean', beta1=0.9, beta2=0.999, block_trigram=True, dataset='', decay_method='', dropout=0.1, encoder='classifier', ff_size=512, gpu_ranks=[0], heads=4, hidden_size=128, input_text='', inter_layers=2, log_file='/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_classifier.txt', lr=1, max_grad_norm=0, mode='test', model_path='/content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier', optim='adam', param_init=0, param_init_glorot=True, recall_eval=False, report_every=1, report_rouge=True, result_path='/content/drive/MyDrive/인공지능/추출요약/RESULT/', rnn_size=512, save_checkpoint_steps=5, seed=666, temp_dir='/content/drive/MyDrive/인공지능/추출요약/TEMP/', test_all=False, test_from='/content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_1000.pt', train_from='', train_steps=1000, use_interval=True, visible_gpus='0', warmup_steps=8000, world_size=1)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/인공지능/추출요약/SRC/train.py\", line 402, in <module>\n",
            "    test(args, device_id, cp, step)\n",
            "  File \"/content/drive/MyDrive/인공지능/추출요약/SRC/train.py\", line 213, in test\n",
            "    test_iter =data_loader.Dataloader(args, load_dataset(args, 'test', shuffle=False),\n",
            "  File \"/content/drive/MyDrive/인공지능/추출요약/SRC/models/data_loader.py\", line 124, in __init__\n",
            "    self.cur_iter = self._next_dataset_iterator(datasets)\n",
            "  File \"/content/drive/MyDrive/인공지능/추출요약/SRC/models/data_loader.py\", line 145, in _next_dataset_iterator\n",
            "    self.cur_dataset = next(dataset_iter)\n",
            "  File \"/content/drive/MyDrive/인공지능/추출요약/SRC/models/data_loader.py\", line 99, in load_dataset\n",
            "    yield _lazy_dataset_loader(pt, corpus_type)\n",
            "  File \"/content/drive/MyDrive/인공지능/추출요약/SRC/models/data_loader.py\", line 83, in _lazy_dataset_loader\n",
            "    dataset = torch.load(pt_file)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 771, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 270, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/serialization.py\", line 251, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/인공지능/추출요약/data/bert_data/valid/korean.test.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-2. RNN"
      ],
      "metadata": {
        "id": "cCvfLymeWKWp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FyMltN5mWEOW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}