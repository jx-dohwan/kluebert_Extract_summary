{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 설치"
      ],
      "metadata": {
        "id": "5ZBy3xvviYUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mecab"
      ],
      "metadata": {
        "id": "lUMFREYyiu_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install konlpy"
      ],
      "metadata": {
        "id": "Qvf7JdfuiaGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update"
      ],
      "metadata": {
        "id": "vc1bORuWinRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install curl git"
      ],
      "metadata": {
        "id": "HEDJMAoriq3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "metadata": {
        "id": "pvxxuf7PisWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiprocess"
      ],
      "metadata": {
        "id": "Kp1lfZ5GizTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install multiprocess"
      ],
      "metadata": {
        "id": "6Cxum78FitjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers"
      ],
      "metadata": {
        "id": "4J2FY9u7i2tM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install transformers"
      ],
      "metadata": {
        "id": "MXwPCEKRi1Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyRouge"
      ],
      "metadata": {
        "id": "LTMKtjw8i62a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyrouge --upgrade\n",
        "!pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
        "!pip install pyrouge\n",
        "!pip show pyrouge\n",
        "!git clone https://github.com/andersjo/pyrouge.git\n",
        "from pyrouge import Rouge155\n",
        "!pyrouge_set_rouge_path '/content/pyrouge/tools/ROUGE-1.5.5'"
      ],
      "metadata": {
        "id": "sey_OHqFi5JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install libxml-parser-perl"
      ],
      "metadata": {
        "id": "pdXNcunLi9G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "cd pyrouge/tools/ROUGE-1.5.5/data\n",
        "rm WordNet-2.0.exc.db # only if exist\n",
        "cd WordNet-2.0-Exceptions\n",
        "rm WordNet-2.0.exc.db # only if exist\n",
        "\n",
        "./buildExeptionDB.pl . exc WordNet-2.0.exc.db\n",
        "cd ../\n",
        "ln -s WordNet-2.0-Exceptions/WordNet-2.0.exc.db WordNet-2.0.exc.db"
      ],
      "metadata": {
        "id": "9r7wRNCEi-QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorboardX"
      ],
      "metadata": {
        "id": "asPxLctOjCi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install tensorboardX"
      ],
      "metadata": {
        "id": "uo3fZ0CUjAIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 라이브러리 Import "
      ],
      "metadata": {
        "id": "GkXfgw6JjPjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import easydict"
      ],
      "metadata": {
        "id": "pRAwyKjJjN3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터\n",
        "\n",
        "\n",
        "###원본 데이터 탐색\n",
        "- 데이터 구성\n",
        "  - 원문 데이터 40만 건 (신문기사 30만 건, 기고문 6만 건, 잡지기사 1만 건, 법원 판결문 3만 건)을 활용하여 각각 추출요약 40만 건, 생성요약 40만 건, 총 80만 건의 요약문 도출\n",
        "  - 원문으로부터 변형 없이 그대로 선택된 3개 문장으로 추출요약문 생성\n",
        "  - 원문의 내용을 바탕으로 재작성된 생성요약문 생성"
      ],
      "metadata": {
        "id": "oyUWdh3CnSei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAPATH = '/content/drive/MyDrive/인공지능/추출요약/data/raw_data/train'\n",
        "filenames = [x for x in os.listdir (DATAPATH) if x.endswith('json')]\n",
        "filenames.sort()\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4aLJuzSjShA",
        "outputId": "a7b5a2cf-3e1c-4f22-cd0d-b670470da042"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_original_law.json',\n",
              " 'train_original_news.json',\n",
              " 'train_original_opinion.json']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = filenames[0]\n",
        "filelocation = os.path.join(DATAPATH, file)\n",
        "\n",
        "with open(filelocation, 'r') as json_file:\n",
        "  data = json.load(json_file) ['documents']"
      ],
      "metadata": {
        "id": "Gq0TJ2fFnzdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmQwzvMgoBS8",
        "outputId": "5e977be2-765d-4e5c-db6e-e0008d372eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '100004',\n",
              " 'category': '일반행정',\n",
              " 'size': 'small',\n",
              " 'char_count': 377,\n",
              " 'publish_date': '19841226',\n",
              " 'title': '부당노동행위구제재심판정취소',\n",
              " 'text': [[{'index': 0,\n",
              "    'sentence': '원고가 소속회사의 노동조합에서 분규가 발생하자 노조활동을 구실로 정상적인 근무를 해태하고,',\n",
              "    'highlight_indices': ''},\n",
              "   {'index': 1, 'sentence': '노조조합장이 사임한 경우,', 'highlight_indices': ''},\n",
              "   {'index': 2,\n",
              "    'sentence': '노동조합규약에 동 조합장의 직무를 대행할 자를 규정해 두고 있음에도 원고 자신이 주동하여 노조자치수습대책위원회를 구성하여 그 위원장으로 피선되어 근무시간중에도 노조활동을 벌여 운수업체인 소속회사의 업무에 지장을 초래하고',\n",
              "    'highlight_indices': '8,9;68,69'},\n",
              "   {'index': 3,\n",
              "    'sentence': '종업원들에게도 나쁜 영향을 끼쳐 소속회사가 취업규칙을 위반하고',\n",
              "    'highlight_indices': ''},\n",
              "   {'index': 4,\n",
              "    'sentence': '고의로 회사업무능률을 저해하였으며 회사업무상의 지휘명령에 위반하였음을 이유로 원고를 징계해고 하였다면,',\n",
              "    'highlight_indices': '0,3'},\n",
              "   {'index': 5,\n",
              "    'sentence': '이는 원고의 노동조합 활동과는 관계없이 회사취업규칙에 의하여 사내질서를 유지하기 위한 사용자 고유의 징계권에 기하여 이루어진 정당한 징계권의 행사로 보아야 한다.',\n",
              "    'highlight_indices': '17,21'}]],\n",
              " 'annotator_id': 3783,\n",
              " 'document_quality_scores': {'readable': 3,\n",
              "  'accurate': 3,\n",
              "  'informative': 3,\n",
              "  'trustworthy': 3},\n",
              " 'extractive': [5, 4, 2],\n",
              " 'abstractive': ['원고가  주동하여 회사업무능률을 저해하고 회사업무상의 지휘명령에 위반하였다면 이에 따른 징계해고는 사내질서를 유지하기 위한 사용자 고유의 정당한 징계권의 행사로 보아야 한다.']}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 원본 데이터에서 필요한 값 추출\n",
        "- text와 extractive 추출\n",
        "  - text : 정규식을 이용해 sentence - highlight_indices 사이의 문장 추출한 후 리스트로 저장\n",
        "  - extracive : 3줄 요약에 해당하는 문장 index 3개가 저장된 리스트\n",
        "- 신문기사, 기고문, 법원 판결분 3개로 나누어져 잇는 파일을 하나로 합침\n",
        "  - train, valid 각각에 대해 수행"
      ],
      "metadata": {
        "id": "fZLg_iNCrBEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train 데이터"
      ],
      "metadata": {
        "id": "Bt6iHAncr59u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAPATH = '/content/drive/MyDrive/인공지능/추출요약/data/raw_data/train'\n",
        "filenames = [x for x in os.listdir (DATAPATH) if x.endswith('json')]\n",
        "filenames.sort()\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoOkgAtAoEGM",
        "outputId": "4577af8a-8c40-41aa-f0b2-bfe52c97e6ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train_original_law.json',\n",
              " 'train_original_news.json',\n",
              " 'train_original_opinion.json']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_dic = []\n",
        "\n",
        "for file in filenames:\n",
        "  filelocation = os.path.join(DATAPATH, file)\n",
        "\n",
        "  with open(filelocation, 'r') as json_file:\n",
        "    data = json.load(json_file)['documents']\n",
        "\n",
        "    for x in tqdm (range(len(data))):\n",
        "      text = data[x]['text']\n",
        "      text = str(text).replace('\"', \"'\")\n",
        "\n",
        "      extractive = data[x]['extractive']\n",
        "      for index, value in enumerate(extractive):\n",
        "        if value == None:\n",
        "          extractive[index] = 0\n",
        "\n",
        "      p = re.compile('(?<=sentence\\'\\: \\')(.*?)(?=\\'highlight_indices)')\n",
        "      texts = p.findall(text)\n",
        "\n",
        "      sentences = []\n",
        "      for t in texts:\n",
        "        sentence = t[:-3]\n",
        "        sentences.append(sentence)\n",
        "\n",
        "      mydict = {}\n",
        "      mydict['text'] = sentences\n",
        "      mydict['extractive'] = extractive\n",
        "      list_dic.append(mydict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEFekWp5sLEc",
        "outputId": "5264eeb4-593e-4c90-d7d0-eeef109fa95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 24329/24329 [00:01<00:00, 16558.77it/s]\n",
            "100%|██████████| 243983/243983 [00:20<00:00, 11715.99it/s]\n",
            "100%|██████████| 56760/56760 [00:07<00:00, 8017.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_dic)"
      ],
      "metadata": {
        "id": "mmg7GrQSu971"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/인공지능/추출요약/data/raw_data/train.json\", 'w') as fh:\n",
        "  json.dump(list_dic, fh)"
      ],
      "metadata": {
        "id": "evjSqF9jukmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def list_chunk(lst, n):\n",
        "#     return [lst[i:i+n] for i in range(0, len(lst), n)]\n",
        "\n",
        "# data_chunked = list_chunk(data, 32507) ## 전체 데이터를 10개로 분할\n",
        "\n",
        "# for i, d in enumerate(data_chunked):\n",
        "#   with open(\"/content/drive/MyDrive/인공지능/추출요약/data/raw_data/train.{}.json\".format(i), 'w') as fh:\n",
        "#     json.dump(d, fh)"
      ],
      "metadata": {
        "id": "LrxQT1E4ttHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### val "
      ],
      "metadata": {
        "id": "K5ML9zfXv70C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAPATH = '/content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid'\n",
        "filenames = [x for x in os.listdir (DATAPATH) if x.endswith('json')]\n",
        "filenames.sort()\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R22bBuDvuc-X",
        "outputId": "3e659831-15b8-41c4-b0e5-604d0225d206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['valid_original_law.json',\n",
              " 'valid_original_news.json',\n",
              " 'valid_original_opinion.json']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_dic = []\n",
        "\n",
        "for file in filenames:\n",
        "  filelocation = os.path.join(DATAPATH, file)\n",
        "\n",
        "  with open(filelocation, 'r') as json_file:\n",
        "    data = json.load(json_file)['documents']\n",
        "\n",
        "    for x in tqdm (range(len(data))):\n",
        "      text = data[x]['text']\n",
        "      text = str(text).replace('\"', \"'\")\n",
        "\n",
        "      extractive = data[x]['extractive']\n",
        "      for index, value in enumerate(extractive):\n",
        "        if value == None:\n",
        "          extractive[index] = 0\n",
        "\n",
        "      p = re.compile('(?<=sentence\\'\\: \\')(.*?)(?=\\'highlight_indices)')\n",
        "      texts = p.findall(text)\n",
        "\n",
        "      sentences = []\n",
        "      for t in texts:\n",
        "        sentence = t[:-3]\n",
        "        sentences.append(sentence)\n",
        "\n",
        "      mydict = {}\n",
        "      mydict['text'] = sentences\n",
        "      mydict['extractive'] = extractive\n",
        "      list_dic.append(mydict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCjZwdn5wSBY",
        "outputId": "d2cf47dd-e125-4e0a-d209-967b97b1b946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3004/3004 [00:00<00:00, 22788.49it/s]\n",
            "100%|██████████| 30122/30122 [00:02<00:00, 10430.85it/s]\n",
            "100%|██████████| 7008/7008 [00:00<00:00, 10395.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid.json\", 'w') as fh:\n",
        "  json.dump(list_dic, fh)"
      ],
      "metadata": {
        "id": "0c6lPYLLwT-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습가능한 형태로 데이터 변환\n",
        "### Tokenizing 후 .json 파일로 저장\n",
        "- Source 및 Target 설정\n",
        "  - Source; 원본 데이터의 'text'에 해당\n",
        "  - Target; 원본 데이터의 'extractive' 리스트의 index에 해당하는 문장을 source에서 추출\n",
        "- Source 및 Target 을 Tokenizing\n",
        "  - Tokenizer; MeCab + BertTokenizer\n",
        "    - 한국어 데이터이므로 MeCab으로 형태소 분석 후 BertTokenizer 사용\n",
        "  - Tokenizing 후 token 단위로 source 및 target 저장"
      ],
      "metadata": {
        "id": "kHaeFzYESibG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n",
        "\n",
        "mecab = Mecab()"
      ],
      "metadata": {
        "id": "1fvadQTwwe61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train 데이터"
      ],
      "metadata": {
        "id": "WMgT7eFESyfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATAPATH = '/content/drive/MyDrive/인공지능/추출요약/data/raw_data'\n",
        "filenames = [x for x in os.listdir (DATAPATH) if 'train' in x and x.endswith('json')]\n",
        "filenames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykscWhbtSxvo",
        "outputId": "4d3e4eb8-3d2e-4374-94da-96a25551a4f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.json']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_preprocess(set_name):\n",
        "  with open(\"/content/drive/MyDrive/인공지능/추출요약/data/raw_data/{}.json\".format(set_name), 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "    list_dic = []\n",
        "    for x in tqdm(range(len(data))):\n",
        "      text = data[x]['text']\n",
        "      extractive = data[x]['extractive']\n",
        "\n",
        "      sentences = []\n",
        "      for sentence in text:\n",
        "        sentence_morph = ' '.join(mecab.morphs(sentence))\n",
        "        sentences.append(sentence_morph)\n",
        "\n",
        "      extractives = []\n",
        "      for e in extractive:\n",
        "        extractives.append(sentences[e])\n",
        "\n",
        "      src = [i.split(' ') for i in sentences]\n",
        "      tgt = [i.split(' ') for i in extractives]\n",
        "\n",
        "      mydict = {}\n",
        "      mydict['src'] = src\n",
        "      mydict['tgt'] = tgt\n",
        "      list_dic.append(mydict)\n",
        "\n",
        "    jsonfilelocation = '/content/drive/MyDrive/인공지능/추출요약/data/json_data/' + set_name\n",
        "    os.makedirs(jsonfilelocation, exist_ok=True)\n",
        "\n",
        "    temp = []\n",
        "    DATA_PER_FILE = 50\n",
        "\n",
        "    for i,a in enumerate(tqdm(list_dic)):\n",
        "      if (i+1)%DATA_PER_FILE!=0:\n",
        "        temp.append(a)\n",
        "      else:\n",
        "        temp.append(a)\n",
        "        filename = 'korean.'+ set_name + '.' + str(i//DATA_PER_FILE)+'.json'\n",
        "        with open(os.path.join(jsonfilelocation, filename), \"w\", encoding='utf-8') as json_file:\n",
        "          json.dump(temp, json_file, ensure_ascii=False)\n",
        "          temp = []\n",
        "\n",
        "      #마지막에 남은 데이터 있으면 추가로 append\n",
        "      if len(temp) != 0:\n",
        "        filename = 'korean.'+ set_name + '.' + str(i//DATA_PER_FILE + 1)+'.json'\n",
        "        with open(os.path.join(jsonfilelocation, filename), \"w\", encoding='utf-8') as json_file:\n",
        "          json.dump(temp, json_file, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "EAVq7h0HfCfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_preprocess('train')\n",
        "# data_preprocess('valid')"
      ],
      "metadata": {
        "id": "jmrfu8AkfiN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee03cf4-f492-412c-ee42-31ba547da199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 325072/325072 [14:08<00:00, 383.21it/s]\n",
            "100%|██████████| 325072/325072 [1:12:42<00:00, 74.51it/s]\n",
            "100%|██████████| 40134/40134 [01:42<00:00, 392.35it/s]\n",
            "100%|██████████| 40134/40134 [08:44<00:00, 76.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT feature 생성 후 .pt 파일로 저장\n",
        "- format_to_bert 함수 이용\n",
        "- prepro > data_builder.py\n",
        "  - BertData() 클래스에서 tokenizer 설정\n",
        "  - BertTokenizer.from_pretrained('klue/bert-base', strip_accents=False, do_lower_case=False)"
      ],
      "metadata": {
        "id": "Ck0zkvbjedLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from os.path import join as pjoin\n",
        "import sys\n",
        "\n",
        "from multiprocess import Pool\n",
        "sys.path.append('/content/drive/MyDrive/인공지능/추출요약')\n",
        "from SRC.prepro.data_builder import _format_to_bert\n",
        "\n",
        "def format_to_bert(args):\n",
        "    if (args.dataset != ''):\n",
        "        datasets = args.dataset\n",
        "    else:\n",
        "        datasets = ['train', 'valid', 'test']\n",
        "    for corpus_type in datasets:\n",
        "        a_lst = []\n",
        "        for json_f in glob.glob(pjoin(args.raw_path, corpus_type, '*' + corpus_type + '.*.json')):\n",
        "            real_name = json_f.split('/')[-1]\n",
        "            a_lst.append((json_f, args, pjoin(args.save_path, real_name.replace('json', 'bert.pt'))))\n",
        "        # print(a_lst)\n",
        "        pool = Pool(args.n_cpus)\n",
        "        for d in pool.imap(_format_to_bert, a_lst):\n",
        "            pass\n",
        "\n",
        "        pool.close()\n",
        "        pool.join()"
      ],
      "metadata": {
        "id": "60Aczq2geW08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_name = 'train'\n",
        "\n",
        "bertfilelocation = '/content/drive/MyDrive/인공지능/추출요약/data/bert_data/' + set_name\n",
        "os.makedirs(bertfilelocation, exist_ok=True)\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "  \"dataset\": [set_name], \n",
        "  \"raw_path\": \"/content/drive/MyDrive/인공지능/추출요약/data/json_data/\",\n",
        "  \"save_path\": bertfilelocation,\n",
        "  \"n_cpus\":4,\n",
        "  \"oracle_mode\": \"greedy\",\n",
        "  \"min_src_ntokens\": 5,\n",
        "  \"max_src_ntokens\": 200,\n",
        "  \"min_nsents\": 3,\n",
        "  \"max_nsents\": 100,\n",
        "}) \n",
        "\n",
        "format_to_bert(args)"
      ],
      "metadata": {
        "id": "GuIuOhireIoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_name = 'valid'\n",
        "\n",
        "bertfilelocation = '/content/drive/MyDrive/인공지능/추출요약/data/bert_data/' + set_name\n",
        "os.makedirs(bertfilelocation, exist_ok=True)\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "  \"dataset\": [set_name], \n",
        "  \"raw_path\": \"/content/drive/MyDrive/인공지능/추출요약/data/json_data/\",\n",
        "  \"save_path\": bertfilelocation,\n",
        "  \"n_cpus\":4,\n",
        "  \"oracle_mode\": \"greedy\",\n",
        "  \"min_src_ntokens\": 5,\n",
        "  \"max_src_ntokens\": 200,\n",
        "  \"min_nsents\": 3,\n",
        "  \"max_nsents\": 100,\n",
        "})\n",
        "\n",
        "format_to_bert(args)"
      ],
      "metadata": {
        "id": "xwU6Jd8xesUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "### 1. 사전 학습 모델 로딩\n",
        "- models > model_builder.py\n",
        "  - `Bert()` 클래스에서 model 설정\n",
        "  - `BertModel.from_pretrained('klue/bert-base')`\n",
        "- train.py\n",
        "  - `validate`와 `test` 함수에서 config 설정\n",
        "  - `BertConfig.from_pretrained('klue/bert-base')`\n",
        "\n",
        "### 2. 인코더 설정\n",
        "- models > encoder.py\n",
        "  - `Classifier()` 클래스\n",
        "  - `TransformerInterEncoder()` 클래스\n",
        "  - `RNNEncoder()` 클래스\n",
        "- models > model_builder.py\n",
        "  - `Summarizer()` 클래스에서 encoder 설정"
      ],
      "metadata": {
        "id": "8kYR6o2vY_Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "### 1. 학습 loss 함수 설정\n",
        "- models > trainer.py\n",
        "  - `__init__` 함수에서 loss 설정\n",
        "  - `torch.nn.BCELoss(reduction='none')`\n",
        "\n",
        "### 2. 하이퍼파라미터 설정\n",
        "- 경로 관련 하이퍼파라미터\n",
        "  - model_path (Path): Fine-tuning한 모델이 저장되는 경로\n",
        "  - result_path (Path): 평가 시 추론 결과(candidate)와 정답(gold)이 저장되는 경로\n",
        "  - temp_dir (Path): 평가 시 ROUGE SCORE 결과가 저장되는 경로\n",
        "  - log_file (Path): 로그 파일이 저장되는 경로\n",
        "  - train_from (Path): 학습하고자 하는 체크포인트(pt)가 있을 경우 설정하는 경로\n",
        "  - bert_data_path (Path): 학습에 사용할 Bert Data(.pt)를 설정하는 경로\n",
        "\n",
        "- 모델 관련 하이퍼파라미터\n",
        "  - batch_size (int): A Batch Size (default=1000)\n",
        "  - use_interval (bool): 학습 시 문서 내에서 여러 문장을 구별하기 위해 Interval Segment Embeddings를 사용하는 지의 여부 (default=True)\n",
        "  - hidden_size (int): Transformer Hidden Size (default=128)\n",
        "  - ff_size (int): Feed-forward Filter Size (default=2048)\n",
        "  - heads (int): Transformer Head 개수 (default=8)\n",
        "  - inter_layers (int): Transformer Inter Layer 개수 (default=2)\n",
        "  - rnn_size (int): Encoder RNN RNN Size(default=512)\n",
        "  - train_steps(int): 학습할 Step 수\n",
        "\n",
        "- Optimizer 관련 하이퍼파라미터\n",
        "  - param_init (float): (default=0)\n",
        "  - param_init_glorot (bool): (default=True)\n",
        "  - dropout (float): Dropout (default=0.1)\n",
        "  - optim (str): Optimizer (default='adam')\n",
        "  - lr (float): Learning Rate (default=2e-3)\n",
        "  - beta1 (float): Adam Optimizer 관련 Hyper Parameter (default= 0.9)\n",
        "  - beta2 (float): Adam Optimizer 관련 Hyper Parameter (default=0.999)\n",
        "  - decay_method (str): Weight Decay (default='noam')\n",
        "  - warmup_steps (int): Warm-up Steps (default=6000)\n",
        "  - max_grad_norm (float): Max Gradient Noam (default=0)\n",
        "\n",
        "- 모델 저장 관련 하이퍼파라미터\n",
        "  - save_checkpoint steps (int): 모델 저장 주기\n",
        "\n",
        "- Multi-GPU 관련 하이퍼파라미터\n",
        "  - world_size (int): GPU 개수\n",
        "  - visible_gpus (str): GPU 번호\n",
        "  - gpu_ranks (str): GPU 번호\n",
        "  - seed (int): (default=666)\n",
        "\n",
        "- 기타 하이퍼파라미터\n",
        "  - accum_count (int): (default=2)\n",
        "  - report_every (int): (default=50)\n",
        "  - recall_eval (bool): (default=false)\n",
        "  - report_rouge (bool): (default=false)\n",
        "  - block_trigram (bool): (default=true)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0Jw9lw6vZKIK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Multi-GPU 설정\n",
        "- train.py\n",
        "  - `multi_main` 과 `run` 함수 참조\n",
        "  - multiprocessing 중 spawn 사용\n",
        "\n",
        "```\n",
        "if(args.world_size>1):\n",
        "  multi_main(args)\n",
        "```\n",
        "- distributed.py\n",
        "  - `multi_init` 함수 참조\n",
        "  - 분산학습 사용\n",
        "- Colab 은 Multi-GPU를 지원하지 않으므로, 개인 장비에서 가능하다면 실행 추천\n"
      ],
      "metadata": {
        "id": "GLi08adjZT9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. ROUGE 설정\n",
        "- models > trainer.py\n",
        "  - `Trainer()` 클래스의 `test` 함수 참조\n",
        "- others > utils.py\n",
        "  - `test_rouge`, `rouge_results_to_str` 함수 사용"
      ],
      "metadata": {
        "id": "eIVd5emBZXPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 최종 학습 코드\n",
        "- train.py에서 mode를 train으로 선택"
      ],
      "metadata": {
        "id": "oqEZaXzhYLOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "j0KTYK5MuMK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a45904-48a9-46f3-be30-9fc0d9bd779f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 16 12:49:10 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    51W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classifier - 논문에는 5만 step"
      ],
      "metadata": {
        "id": "ihb_9J30Ykqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdirlocation = '/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE'\n",
        "os.makedirs(logdirlocation, exist_ok=True)\n",
        "\n",
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "    -mode train \\\n",
        "    -encoder classifier \\\n",
        "    -dropout 0.1 \\\n",
        "    -bert_data_path /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean \\\n",
        "    -model_path /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier \\\n",
        "    -lr 2e-3 \\\n",
        "    -visible_gpus 0 \\\n",
        "    -gpu_ranks 0 \\\n",
        "    -world_size 1 \\\n",
        "    -report_every 1000\\\n",
        "    -save_checkpoint_steps 100 \\\n",
        "    -batch_size 1000 \\\n",
        "    -decay_method noam \\\n",
        "    -train_steps 1000 \\\n",
        "    -accum_count 2 \\\n",
        "    -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_classifier.txt \\\n",
        "    -use_interval true \\\n",
        "    -warmup_steps 200\n"
      ],
      "metadata": {
        "id": "9ZN5OL58YjnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5-2 RNN"
      ],
      "metadata": {
        "id": "lABM2F1-SmlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdirlocation = '/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE'\n",
        "os.makedirs(logdirlocation, exist_ok=True)\n",
        "\n",
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode train \\\n",
        "  -encoder rnn \\\n",
        "  -dropout 0.1 \\\n",
        "  -bert_data_path /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean \\\n",
        "  -model_path /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn \\\n",
        "  -lr 2e-3 \\\n",
        "  -visible_gpus 0 \\\n",
        "  -gpu_ranks 0 \\\n",
        "  -world_size 1 \\\n",
        "  -report_every 1000\\\n",
        "  -save_checkpoint_steps 100 \\\n",
        "  -batch_size 1000 \\\n",
        "  -decay_method noam \\\n",
        "  -train_steps 1000 \\\n",
        "  -accum_count 2 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_rnn.txt \\\n",
        "  -use_interval true \\\n",
        "  -warmup_steps 200 \\\n",
        "  -rnn_size 768 # 신경써서 해야함"
      ],
      "metadata": {
        "id": "lNQ-hGwQdHdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5-3 Transformer"
      ],
      "metadata": {
        "id": "BQP6b3cQTO6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdirlocation = '/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE'\n",
        "os.makedirs(logdirlocation, exist_ok=True)\n",
        "\n",
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode train \\\n",
        "  -encoder transformer \\\n",
        "  -dropout 0.1 \\\n",
        "  -bert_data_path /content/drive/MyDrive/인공지능/추출요약/data/bert_data/train/korean \\\n",
        "  -model_path /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer \\\n",
        "  -lr 2e-3 \\\n",
        "  -visible_gpus 0 \\\n",
        "  -gpu_ranks 0 \\\n",
        "  -world_size 1 \\\n",
        "  -report_every 1000\\\n",
        "  -save_checkpoint_steps 100 \\\n",
        "  -batch_size 1000 \\\n",
        "  -decay_method noam \\\n",
        "  -train_steps 1000 \\\n",
        "  -accum_count 2 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_transformer.txt \\\n",
        "  -use_interval true \\\n",
        "  -warmup_steps 200 \\\n",
        "  -ff_size 2048 \\\n",
        "  -inter_layers 2 \\\n",
        "  -heads 8"
      ],
      "metadata": {
        "id": "zz5ylGv4TMAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 최종 평가 코드\n",
        "- train.py에서 mode를 test로 선택\n",
        "- valid 데이터 사용\n",
        "  - 이때 데이터 이름을 test로 변경해야 진행되므로 `korean.valid.{}.bert.pt` 데이터를 `korean.test.{}.bert.pt` 데이터로 이름 변경\n",
        "- **주의**\n",
        "  - test 시 pyrouge 라이브러리를 통해 rouge score를 계산하게 되는데, Colab 상에서는 dependency 문제로 인해 에러 발생\n",
        "  - 위의 Install libraries 가이드대로 pyrouge 설치 후 SRC, 데이터, 모델을 로컬 디스크에 다운로드 후 로컬에서 실행해야 함\n"
      ],
      "metadata": {
        "id": "dnyw814OTpGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-1. Classifier"
      ],
      "metadata": {
        "id": "XF8C57FEVDRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdirlocation = '/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE'\n",
        "os.makedirs(logdirlocation, exist_ok=True)\n",
        "\n",
        "resdirlocation = '/content/drive/MyDrive/인공지능/추출요약/RESULT'\n",
        "os.makedirs(resdirlocation, exist_ok=True)\n",
        "\n",
        "tmpdirlocation = '/content/drive/MyDrive/인공지능/추출요약/TEMP'\n",
        "os.makedirs(tmpdirlocation, exist_ok=True)\n",
        "\n",
        "\n",
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode test \\\n",
        "  -bert_data_path /content/drive/MyDrive/인공지능/추출요약/data/bert_data/valid/korean \\\n",
        "  -model_path /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier \\\n",
        "  -result_path /content/drive/MyDrive/인공지능/추출요약/RESULT/ \\\n",
        "  -temp_dir /content/drive/MyDrive/인공지능/추출요약/TEMP/ \\\n",
        "  -visible_gpus 0 \\\n",
        "  -gpu_ranks 0 \\\n",
        "  -world_size 1 \\\n",
        "  -batch_size 30000 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_classifier.txt \\\n",
        "  -test_from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_classifier/model_step_1000.pt"
      ],
      "metadata": {
        "id": "AxDeUlEYTkGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-2. RNN"
      ],
      "metadata": {
        "id": "cCvfLymeWKWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdirlocation = '/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE'\n",
        "os.makedirs(logdirlocation, exist_ok=True)\n",
        "\n",
        "resdirlocation = '/content/drive/MyDrive/인공지능/추출요약/RESULT'\n",
        "os.makedirs(resdirlocation, exist_ok=True)\n",
        "\n",
        "tmpdirlocation = '/content/drive/MyDrive/인공지능/추출요약/TEMP'\n",
        "os.makedirs(tmpdirlocation, exist_ok=True)\n",
        "\n",
        "\n",
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode test \\\n",
        "  -bert_data_path /content/drive/MyDrive/인공지능/추출요약/data/bert_data/valid/korean \\\n",
        "  -model_path /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn \\\n",
        "  -result_path /content/drive/MyDrive/인공지능/추출요약/RESULT/ \\\n",
        "  -temp_dir /content/drive/MyDrive/인공지능/추출요약/TEMP/ \\\n",
        "  -visible_gpus 0 \\\n",
        "  -gpu_ranks 0 \\\n",
        "  -world_size 1 \\\n",
        "  -batch_size 30000 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_rnn.txt \\\n",
        "  -test_from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_rnn/model_step_1000.pt"
      ],
      "metadata": {
        "id": "FyMltN5mWEOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6-3. Transformer"
      ],
      "metadata": {
        "id": "xxHOr5mq7yys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logdirlocation = '/content/drive/MyDrive/인공지능/추출요약/LOG/KLUE'\n",
        "os.makedirs(logdirlocation, exist_ok=True)\n",
        "\n",
        "resdirlocation = '/content/drive/MyDrive/인공지능/추출요약/RESULT'\n",
        "os.makedirs(resdirlocation, exist_ok=True)\n",
        "\n",
        "tmpdirlocation = '/content/drive/MyDrive/인공지능/추출요약/TEMP'\n",
        "os.makedirs(tmpdirlocation, exist_ok=True)\n",
        "\n",
        "\n",
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode test \\\n",
        "  -bert_data_path /content/drive/MyDrive/인공지능/추출요약/data/bert_data/valid/korean \\\n",
        "  -model_path /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer \\\n",
        "  -result_path /content/drive/MyDrive/인공지능/추출요약/RESULT/ \\\n",
        "  -temp_dir /content/drive/MyDrive/인공지능/추출요약/TEMP/ \\\n",
        "  -visible_gpus 0 \\\n",
        "  -gpu_ranks 0 \\\n",
        "  -world_size 1 \\\n",
        "  -batch_size 30000 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_transformer.txt \\\n",
        "  -test_from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_1000.pt"
      ],
      "metadata": {
        "id": "hxvdIFGl7xZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 세줄 요약 추론 코드 구현\n",
        "- train.py에서 mode를 inference로 선택\n",
        "- models > trainer.py의 `summ` 함수 참조\n"
      ],
      "metadata": {
        "id": "MPsS9SV37_oI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. 추론 결과에 대한 정량적/정성적 평가 \n",
        "- 가장 성능이 좋았던 transformer encoder로 추론\n",
        "- valid.json 중 5개 검증"
      ],
      "metadata": {
        "id": "cDrIsPCj7_lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid/sample.zip -d /content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVtaFRILIQJ6",
        "outputId": "5f488c27-cc5a-40e2-974f-d49508f148e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid/sample.zip\n",
            "  inflating: /content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid/valid_0.txt  \n",
            "  inflating: /content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid/valid_1.txt  \n",
            "  inflating: /content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid/valid_2.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode inference \\\n",
        "  -visible_gpus -1 \\\n",
        "  -gpu_ranks -1 \\\n",
        "  -world_size 0 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_transformer.txt \\\n",
        "  -test_from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_1000.pt \\\n",
        "  -input_text /content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid/valid_0.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVpUYih78B-L",
        "outputId": "1dc0909b-48f5-4fff-fa22-d803554f2864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-20 04:58:35.353171: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-20 04:58:36.213574: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 04:58:36.213693: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 04:58:36.213705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[1] 취소소송은 처분 등이 있음을 안 날부터 90일 이내에 제기하여야 하고,\n",
            "\n",
            "처분 등이 있은 날부터 1년을 경과하면\n",
            "\n",
            "제기하지 못하며( 행정소송법 제20조 제1항, 제2항), 청구취지를 변경하여 구 소가 취하되고 새로운 소가 제기된 것으로 변경되었을 때에 새로운 소에 대한 제소기간의 준수 등은 원칙적으로 소의 변경이 있은 때를 기준으로 하여야 한다.\n",
            "\n",
            "[2] 일반적으로 행정처분에 효력기간이 정하여져 있는 경우에는 그 기간의 경과로 그 행정처분의 효력은 상실되며,\n",
            "\n",
            "다만 허가에 붙은 기한이 그 허가된 사업의 성질상 부당하게 짧은 경우에는 이를 그 허가 자체의 존속기간이 아니라 그 허가조건의 존속기간으로 보아 그 기한이 도래함으로써 그 조건의 개정을 고려한다는 뜻으로 해석할 수 있다.\n",
            "\n",
            "[3] 사도개설허가에서 정해진 공사기간 내에 사도로 준공검사를 받지 못한 경우,\n",
            "\n",
            "이 공사기간을 사도개설허가 자체의 존속기간(유효기간)으로 볼 수 없다는 이유로 사도개설허가가 당연히 실효되는 것은 아니라고 한 사례.\n",
            "[2023-02-20 04:58:38,893 INFO] Loading checkpoint from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_1000.pt\n",
            "gpu_rank 0\n",
            "[2023-02-20 04:58:42,957 INFO] * number of parameters: 121647617\n",
            "['[1] 취소소송은 처분 등이 있음을 안 날부터 90일 이내에 제기하여야 하고,', '제기하지 못하며( 행정소송법 제20조 제1항, 제2항), 청구취지를 변경하여 구 소가 취하되고 새로운 소가 제기된 것으로 변경되었을 때에 새로운 소에 대한 제소기간의 준수 등은 원칙적으로 소의 변경이 있은 때를 기준으로 하여야 한다.', '[2] 일반적으로 행정처분에 효력기간이 정하여져 있는 경우에는 그 기간의 경과로 그 행정처분의 효력은 상실되며,']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode inference \\\n",
        "  -visible_gpus -1 \\\n",
        "  -gpu_ranks -1 \\\n",
        "  -world_size 0 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_transformer.txt \\\n",
        "  -test_from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_1000.pt \\\n",
        "  -input_text /content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid/valid_1.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvmfyFy8--Kx",
        "outputId": "bbbf6d78-efd0-4431-a3b5-f2306c0e1aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-20 04:58:45.446155: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-20 04:58:46.300719: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 04:58:46.300815: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 04:58:46.300851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\"더불어민주당 이해찬 대표가 30 일 오후 국회에서 기자간담회를 열고 조국 전 법무부 장관 사태와 관련해 \\\"국민 여러분께 매우 송구하다\\\"고 밝혔다.\n",
            "\n",
            "더불어민주당 이해찬 대표가 30 일 기자간담회를 열고 '조국 사태'와 관련, \\\"국민 여러분께 매우 송구하다\\\"는 입장을 밝혔다.\n",
            "\n",
            "이 대표는 \\\"검찰 개혁이란 대의에 집중하다 보니, 국민 특히 청년이 느꼈을 불공정에 대한 상대적 박탈감, 좌절감을 깊이 있게 헤아리지 못했다\\\"며 \\\"여당 대표로서 무거운 책임감을 느낀다\\\"고 머리를 숙였다.\n",
            "\n",
            "조국 전 법무부 장관이 14 일 사퇴한 이후 이 대표가 당 안팎의 쇄신 요구에 대해 입장을 표명한 것은 이번이 처음이다.\n",
            "\n",
            "청와대와 여당은 '조국 정국'을 거치며 분출된 '공정'과 '정의'의 민심을 받들어 검찰 개혁에 매진하겠다면서도 두 달간 극심한 분열과 갈등을 초래한데 대해선 진지하게 성찰하는 모습을 보이지 않았다.\n",
            "\n",
            "그나마 초선인 이철희 의원이 \\\"당이 대통령 뒤에 비겁하게 숨어 있었다\\\"고 비판했고, 표창원 의원은 \\\"책임을 느끼는 분들이 각자 형태로 그 책임감을 행동으로 옮겨야 할 때\\\"라고 지적했다.\n",
            "\n",
            "뒤늦게나마 이 대표가 자성의 목소리를 내긴 했으나 당 안팎의 쇄신 요구에 어떻게 응할지 구체적 플랜을 제시하지 못해 여전히 안이하다는 지적도 나온다.\n",
            "\n",
            "이 대표는 28 일 윤호중 사무총장을 단장으로 하는 총선기획단을 발족했고 조만간 인재영입위원회도 출범시킬 계획이라고 밝혔다.\n",
            "\n",
            "이 대표는 \\\"민주당의 가치를 공유하는 참신한 인물을 영입해 준비된 정책과 인물로 승부하겠다\\\"고 다짐했다.\n",
            "\n",
            "하지만 당 일각에선 \\\"총선기획단장을 비롯한 당직 인선부터 쇄신 의지를 보여야 한다\\\"는 비판의 목소리가 나온다.\n",
            "\n",
            "무조건 물러나는 게 능사는 아니지만 국정 혼선을 초래한 데 대해 당 지도부가 겸허하게 책임지는 모습을 보이는 게 쇄신의 출발점이 돼야 한다는 지적도 있다.\n",
            "\n",
            "선거는 대중의 이해와 요구를 잘 대표하는 정치인을 뽑는 행위다.\n",
            "\n",
            "민생을 외면하며 낡은 이념과 진영 싸움에 매몰된 구시대 인물들을 과감히 물갈이하라는 게 국민의 요구다.\n",
            "\n",
            "대신 4 차 산업혁명의 거센 파고를 헤쳐나갈 전문성을 갖춘 젊고 유능한 인재들을 널리 구해야 하다.\n",
            "\n",
            "이해찬 대표의 이날 유감 표명이 여권 전반의 대대적인 인적 쇄신으로 이어지길 기대한다.\n",
            "[2023-02-20 04:58:48,941 INFO] Loading checkpoint from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_1000.pt\n",
            "gpu_rank 0\n",
            "[2023-02-20 04:58:53,054 INFO] * number of parameters: 121647617\n",
            "['\"더불어민주당 이해찬 대표가 30 일 오후 국회에서 기자간담회를 열고 조국 전 법무부 장관 사태와 관련해 \\\\\"국민 여러분께 매우 송구하다\\\\\"고 밝혔다.', '더불어민주당 이해찬 대표가 30 일 기자간담회를 열고 \\'조국 사태\\'와 관련, \\\\\"국민 여러분께 매우 송구하다\\\\\"는 입장을 밝혔다.', '이 대표는 \\\\\"검찰 개혁이란 대의에 집중하다 보니, 국민 특히 청년이 느꼈을 불공정에 대한 상대적 박탈감, 좌절감을 깊이 있게 헤아리지 못했다\\\\\"며 \\\\\"여당 대표로서 무거운 책임감을 느낀다\\\\\"고 머리를 숙였다.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/인공지능/추출요약/SRC/train.py \\\n",
        "  -mode inference \\\n",
        "  -visible_gpus -1 \\\n",
        "  -gpu_ranks -1 \\\n",
        "  -world_size 0 \\\n",
        "  -log_file /content/drive/MyDrive/인공지능/추출요약/LOG/KLUE/bert_transformer.txt \\\n",
        "  -test_from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_1000.pt \\\n",
        "  -input_text /content/drive/MyDrive/인공지능/추출요약/data/raw_data/valid/valid_2.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOOxBIrb_TgI",
        "outputId": "3a559257-7054-4c96-fe78-23b8424828b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-02-20 04:58:55.573854: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-20 04:58:56.433972: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 04:58:56.434056: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-02-20 04:58:56.434069: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[ 박재원 기자 ] '대한민국 5G 홍보대사'를 자처한 문재인 대통령은 \\\"넓고, 체증 없는 '통신 고속도로'가 5G\\\"라며 \\\"대한민국의 대전환이 이제 막 시작됐다\\\"고 기대감을 높였다.\n",
            "\n",
            "문 대통령은 8일 서울 올림픽공원에서 열린 5G플러스 전략발표에 참석해 \\\"5G 시대는 우리가 생각하고, 만들면 그것이 세계 표준이 되는 시대\\\"라며 \\\"5G는 대한민국 혁신성장의 인프라\\\"라고 강조했다.\n",
            "\n",
            "산업화 시대에 고속도로가 우리 경제의 '대동맥' 역할을 했듯, 5G가 4차 산업혁명 시대의 고속도로가 돼 새로운 기회를 열어 줄 것이란 설명이다.\n",
            "\n",
            "문 대통령은 \\\"5G가 각 분야에 융합되면, 정보통신산업을 넘어 자동차, 드론(무인항공기), 로봇, 지능형 폐쇄회로TV(CCTV)를 비롯한 제조업과 벤처에 이르기까지 우리 산업 전체의 혁신을 통한 동반성장이 가능하다\\\"고 밝혔다.\n",
            "\n",
            "세계 최초 상용화에 성공한 5G가 반도체를 이을 우리 경제의 새 먹거리가 될 것이란 관측이다.\n",
            "\n",
            "정부는 2026년 세계 5G 시장 규모가 1161조원에 달할 것으로 보고 있다.\n",
            "\n",
            "작년 반도체 시장 규모가 529조원인 점을 고려하면 2배 이상 큰 미래 시장이 창출되는 셈이다.\n",
            "\n",
            "문 대통령은 아직은 국민에게 다소 낯선 5G 시대의 미래상을 친절히 설명해 눈길을 끌기도 했다.\n",
            "\n",
            "문 대통령은 \\\"'지금 스마트폰으로 충분한데, 5G가 왜 필요하지?'라고 생각할 수 있다\\\"며 \\\"4세대 이동통신은 '아직은' 빠르지만 가까운 미래에는 결코 빠르지 않다\\\"고 했다.\n",
            "\n",
            "그러면서 \\\"자동차가 많아질수록 더 넓은 길이 필요한 것처럼 사물과 사물을 연결하고, 데이터를 주고받는 이동통신망도 더 넓고 빠른 길이 필요하다\\\"고 덧붙였다.\n",
            "\n",
            "문 대통령은 세계 최초 상용화에 성공한 우리 5G 기술을 널리 알리는 홍보대사를 자처하기도 했다.\n",
            "\n",
            "5G 시장을 선점하기 위한 각국의 경쟁이 뜨겁게 달아오른 만큼 정부 차원에서 적극 지원하겠다는 방침이다.\n",
            "\n",
            "문 대통령은 \\\"평창동계올림픽 360도 중계, 작년 4·27 남북한 정상회담 때 프레스센터에서 사용된 스마트월처럼 기회가 생기면 대통령부터 나서서 우리의 앞선 기술을 홍보하겠다\\\"고 말했다.\n",
            "[2023-02-20 04:58:59,174 INFO] Loading checkpoint from /content/drive/MyDrive/인공지능/추출요약/MODEL/KLUE/bert_transformer/model_step_1000.pt\n",
            "gpu_rank 0\n",
            "[2023-02-20 04:59:03,427 INFO] * number of parameters: 121647617\n",
            "['[ 박재원 기자 ] \\'대한민국 5G 홍보대사\\'를 자처한 문재인 대통령은 \\\\\"넓고, 체증 없는 \\'통신 고속도로\\'가 5G\\\\\"라며 \\\\\"대한민국의 대전환이 이제 막 시작됐다\\\\\"고 기대감을 높였다.', '문 대통령은 8일 서울 올림픽공원에서 열린 5G플러스 전략발표에 참석해 \\\\\"5G 시대는 우리가 생각하고, 만들면 그것이 세계 표준이 되는 시대\\\\\"라며 \\\\\"5G는 대한민국 혁신성장의 인프라\\\\\"라고 강조했다.', \"산업화 시대에 고속도로가 우리 경제의 '대동맥' 역할을 했듯, 5G가 4차 산업혁명 시대의 고속도로가 돼 새로운 기회를 열어 줄 것이란 설명이다.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 성능개선방항\n",
        "|개선사항|이유|진행률(%)|\n",
        "|:-----:|:-----:|:-----:|\n",
        "|Data Augmentation|법률문서 낮은 score||\n",
        "|5만 step로 학습|테스트로 1000만 학습||\n",
        "|Transformer로 서비스 구현|Transformer가 가장 성능이 좋음||\n",
        "|RoBERTa, ELECTRA등 고려|BERT보다 좋은 성능 모델 존재||\n",
        "\n"
      ],
      "metadata": {
        "id": "_iao0Ny4MmCf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0IZvEmwHMBlB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}