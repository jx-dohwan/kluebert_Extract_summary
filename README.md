
## ğŸ’¡í”„ë¡œì íŠ¸ ì†Œê°œ
```
1ï¸âƒ£ ì£¼ì œ : í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš”ì•½
2ï¸âƒ£ ì„¤ëª… : [Fine-tune BERT for Extractive Summarization](https://arxiv.org/pdf/1903.10318v2.pdf)ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œ ìš”ì•½ ëª¨ë¸ êµ¬í˜„ 
3ï¸âƒ£ ëª¨ë¸ : Hugging Face [klue/bert-base](https://huggingface.co/klue/bert-base) ëª¨ë¸ ì‚¬ìš©í•˜ì—¬ ì§„í–‰
```
## ë…¼ë¬¸ ì†Œê°œ
- pre-training ëª¨ë¸ì„ post-trainingë¥¼ í†µí•´ì„œ ë„ë©”ì¸ ì ì‘ì„ í•˜ê³  fine-tuningë¥¼ ì§„í–‰í•´ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•œë‹¤.
- fine-grained : ì†ŒëŸ‰ì˜ í›„ë³´ì—ì„œ ìµœì ì˜ í›„ë³´ë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì£¼ë¡œ One-tower êµ¬ì¡°ì˜ ëª¨ë¸ì„ êµ¬í˜„í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.
![](img/mrs.png)
<Br><br>
### ë¶€ì—°ì„¤ëª…
- post-training
  - ì „ì²´ ëŒ€í™”ë¥¼ ì—¬ëŸ¬ ê°œì˜ short context-response pairsë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ì„ í•™ìŠµ
    - candidateì„ positive, random negative, context negativeë¡œ 3ê°œ classë¡œ êµ¬ì„±í•´ì„œ í•™ìŠµ
    - ì´ë¥¼ í†µí•´ ë°œí™” ê´€ë ¨ ë¶„ë¥˜(URC)ë¡œ ë°œí™”ê°„ì˜ ê´€ê³„ ë° ë°œí™” ë‚´ì  ê´€ê³„ë¥¼ ë°°ì›Œ ë°ì´í„° ì¦ê°•ê³¼ ì„±ëŠ¥ í–¥ìƒì˜ íš¨ê³¼ë¥¼ ì–»ëŠ”ë‹¤. 
  - MLM ì‚¬ìš©

---
## 1. post-training & fine-tuning

```
!pip install transformers==4.25.1


!python3 post_pretrain/train.py
!python3 fine_tuning/train.py
```

## 2. Test
```
import torch
from model import FineModel
fine_model = FineModel().cuda()
fine_model.load_state_dict(torch.load('/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼/á„†á…¥á†¯á„á…µá„á…¥á†«á„‹á…³á†¼á„ƒá…¡á†¸á„‰á…¥á†«á„á…¢á†¨/fine_model.bin'))


context = ["ì–´ë–¤ ë¬¸ì œê°€ ìˆìœ¼ì‹ ê°€ìš”?", "ì–´ë–¤ ì°¨ë¥¼ ì‚¬ì•¼ í• ì§€ ì˜ ëª¨ë¥´ê² ì–´ìš”.", "ì°¨ëŠ” í•œ ë²ˆ ì‚¬ë©´ 10ë…„ë„ ë„˜ê²Œ ì¨ì„œ, ì‹ ì¤‘í•˜ê²Œ ê³¨ë¼ì•¼ í•´ìš”."]
candidates = ["ì € ì¢€ ë„ì™€ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?", "ì €ëŠ” ë†êµ¬ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤.", "ìë™ì°¨ëŠ” ì‹ ì¤‘íˆ ê³¨ë¼ì•¼í•©ë‹ˆë‹¤.", "ì €ëŠ” ì°¨ ë§ˆì‹œëŠ”ê±°ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤.", "ë‚ ì”¨ê°€ í™”ì°½í•©ë‹ˆë‹¤.", "ì €ëŠ” ì°¨ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤."]
     

context_token = [fine_model.tokenizer.cls_token_id]
for utt in context:
    context_token += fine_model.tokenizer.encode(utt, add_special_tokens=False)
    context_token += [fine_model.tokenizer.sep_token_id]

session_tokens = []    
for response in candidates:
    response_token = [fine_model.tokenizer.eos_token_id]
    response_token += fine_model.tokenizer.encode(response, add_special_tokens=False)
    candidate_tokens = context_token + response_token        
    session_tokens.append(candidate_tokens)
    
# ìµœëŒ€ ê¸¸ì´ ì°¾ê¸° for padding
max_input_len = 0
input_tokens_len = [len(x) for x in session_tokens]
max_input_len = max(max_input_len, max(input_tokens_len))    
    
batch_input_tokens = []
batch_input_attentions = []
for session_token in session_tokens:
    input_token = session_token + [fine_model.tokenizer.pad_token_id for _ in range(max_input_len-len(session_token))]
    input_attention = [1 for _ in range(len(session_token))] + [0 for _ in range(max_input_len-len(session_token))]
    batch_input_tokens.append(input_token)
    batch_input_attentions.append(input_attention)
    
batch_input_tokens = torch.tensor(batch_input_tokens).cuda()
batch_input_attentions = torch.tensor(batch_input_attentions).cuda()


softmax = torch.nn.Softmax(dim=1)
results = fine_model(batch_input_tokens, batch_input_attentions)
prob = softmax(results)
true_prob = prob[:,1].tolist()

print(context)
for utt, prob in zip(candidates, true_prob):
    print(utt, '##', round(prob,3))
```

---
## ğŸ—“ï¸ í”„ë¡œì íŠ¸ ê°œì„  ì§„í–‰

|ê°œì„  ì„œë¹„ìŠ¤|ì§„í–‰ì‚¬í•­(%)|
|:----------:|:------:|
|ë°ì´í„°ì…‹ ì¶”ê°€ í™•ë³´||
|í‚¤ì›Œë“œì— ë”°ë¼ biasë˜ëŠ” ë¬¸ì œ ì—°êµ¬||
|ë‹¤í™”ìì˜ ê²½ìš° ê³ ë ¤||
|ë°œí™”ì˜ ì¶”ê°€ íŠ¹ì§• ê³ ë ¤||
|context ê¸¸ì´ ì„ ì •||
|ê°™ì€ ì„¸ì…˜ì„ íŒë‹¨í•  ëª¨ë“ˆìˆëŠ”ì§€ ê³ ë ¤||
|teacher ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ì„ í™œìš©í•˜ì—¬ í•™ìŠµ|


---
