
## ğŸ’¡í”„ë¡œì íŠ¸ ì†Œê°œ
```
1ï¸âƒ£ ì£¼ì œ : í…ìŠ¤íŠ¸ ì¶”ì¶œ ìš”ì•½
2ï¸âƒ£ ì„¤ëª… : [Fine-tune BERT for Extractive Summarization](https://arxiv.org/pdf/1903.10318v2.pdf)ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œ ìš”ì•½ ëª¨ë¸ êµ¬í˜„ 
3ï¸âƒ£ ëª¨ë¸ : Hugging Face [klue/bert-base](https://huggingface.co/klue/bert-base) ëª¨ë¸ ì‚¬ìš©í•˜ì—¬ ì§„í–‰
```
## ë…¼ë¬¸ ì†Œê°œ
- BERTë¥¼ ê¸°ë°˜ìœ¼ë¡œ Simple Classifier, Inter-sentence Transformer, Recurrent Neural Network ì„¸ê°€ì§€ ì¢…ë¥˜ì˜ summarization-specific layersë¥¼ ì¶”ê°€í•˜ì—¬ ì¶”ì¶œ ìš”ì•½ ì‹¤í—˜ ì§„í–‰
<br>

![](img/bertsum.png)
<Br>
### ë¶€ì—°ì„¤ëª…
- Embedding Multiple Sentences
  - ë¬¸ì¥ì˜ ì‹œì‘ : [CLS], ë¬¸ì¥ì˜ ë : [SEP] ì„ ì‚½ì…í•˜ì—¬ ê¸°ì¡´ [SEP]ë§Œ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ë“¤ì„ êµ¬ë¶„í•˜ë˜ BERTëª¨ë¸ì„ ê°œì„ í–ˆë‹¤.
  - ì—¬ëŸ¬ê°œì˜ [CLS] í† í°ì„ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì¥ë“¤ì˜ featureë¥¼ [CLS] í† í°ì— ì €ì¥í•œë‹¤.
- Interval segment Embedding
  - ì—¬ëŸ¬ ë¬¸ì¥ì´ í¬í•¨ëœ ë¬¸ì¥ì—ì„œ ë¬¸ì„œë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©ë¨
  - ìš”ì•½ ë¬¸ì„œì˜ íŠ¹ì„± ìƒ ë‘ê°œ ì´ìƒì˜ ë¬¸ì¥ì´ í¬í•¨ë˜ë¯€ë¡œ ê¸°ë³¸ì˜ ë°©ì‹ê³¼ëŠ” ë‹¤ë¥´ê²Œ ë¬¸ì¥1~4ë¥¼ Aì™€ Bë¥¼ ë²ˆê°ˆì•„ê°€ë©° êµ¬ë¶„
- Summarization Layers
  - BERTë¡œë¶€í„° ë¬¸ì¥ vectorì— ëŒ€í•œ ì •ë³´ë¥¼ ì–»ì€ ë‹¤ìŒì— ì¶”ì¶œ ìš”ì•½ì„ ìœ„í•˜ì—¬ ë¬¸ì„œ ë‹¨ìœ„ì˜ featureë¥¼ ì¡ê¸° ìœ„í•´ ê·¸ ê²°ê´ê°’ ìœ„ì— summarization-specific layersë¥¼ ìŒ“ëŠ”ë‹¤. 
  - Simple Classifier
    - ê¸°ì¡´ BERTì™€ ê°™ì´ Linear layer ë° Sigmoid function
  - Inter-sentence Transformer
    - ë¬¸ì¥ representationsì„ ìœ„í•˜ì—¬ Transformer layerì„ ì‚¬ìš©í•˜ë©° ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œëŠ” Transformer layerë¡œë¶€í„° ë‚˜ì˜¨ ë¬¸ì¥ vectorë¥¼ sigmoid classifier ë„£ëŠ”ë‹¤. ê·¸ë¦¬ê³  Layerê°€ 2ê°œì¼ ë•Œì˜ ì„±ëŠ¥ì´ ì œì¼ ì¢‹ì•˜ë‹¤.
  - Recurrent Neural Network
    - Transformerì™€ RNNê²°í•©ì‹œ ì„±ëŠ¥ì´ ì¢‹ì•˜ìœ¼ë©° BERT outputì„ LSTM layerë¡œ ë„˜ê²¨ì¤€ë‹¤. ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œëŠ” sigmoid classifierë¥¼ ì‚¬ìš©í•œë‹¤.


---
## 1. post-training & fine-tuning

```
!pip install transformers==4.25.1


!python3 post_pretrain/train.py
!python3 fine_tuning/train.py
```

## 2. Test
```
import torch
from model import FineModel
fine_model = FineModel().cuda()
fine_model.load_state_dict(torch.load('/content/drive/MyDrive/á„‹á…µá†«á„€á…©á†¼á„Œá…µá„‚á…³á†¼/á„†á…¥á†¯á„á…µá„á…¥á†«á„‹á…³á†¼á„ƒá…¡á†¸á„‰á…¥á†«á„á…¢á†¨/fine_model.bin'))


context = ["ì–´ë–¤ ë¬¸ì œê°€ ìˆìœ¼ì‹ ê°€ìš”?", "ì–´ë–¤ ì°¨ë¥¼ ì‚¬ì•¼ í• ì§€ ì˜ ëª¨ë¥´ê² ì–´ìš”.", "ì°¨ëŠ” í•œ ë²ˆ ì‚¬ë©´ 10ë…„ë„ ë„˜ê²Œ ì¨ì„œ, ì‹ ì¤‘í•˜ê²Œ ê³¨ë¼ì•¼ í•´ìš”."]
candidates = ["ì € ì¢€ ë„ì™€ì£¼ì‹¤ ìˆ˜ ìˆë‚˜ìš”?", "ì €ëŠ” ë†êµ¬ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤.", "ìë™ì°¨ëŠ” ì‹ ì¤‘íˆ ê³¨ë¼ì•¼í•©ë‹ˆë‹¤.", "ì €ëŠ” ì°¨ ë§ˆì‹œëŠ”ê±°ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤.", "ë‚ ì”¨ê°€ í™”ì°½í•©ë‹ˆë‹¤.", "ì €ëŠ” ì°¨ë¥¼ ì¢‹ì•„í•©ë‹ˆë‹¤."]
     

context_token = [fine_model.tokenizer.cls_token_id]
for utt in context:
    context_token += fine_model.tokenizer.encode(utt, add_special_tokens=False)
    context_token += [fine_model.tokenizer.sep_token_id]

session_tokens = []    
for response in candidates:
    response_token = [fine_model.tokenizer.eos_token_id]
    response_token += fine_model.tokenizer.encode(response, add_special_tokens=False)
    candidate_tokens = context_token + response_token        
    session_tokens.append(candidate_tokens)
    
# ìµœëŒ€ ê¸¸ì´ ì°¾ê¸° for padding
max_input_len = 0
input_tokens_len = [len(x) for x in session_tokens]
max_input_len = max(max_input_len, max(input_tokens_len))    
    
batch_input_tokens = []
batch_input_attentions = []
for session_token in session_tokens:
    input_token = session_token + [fine_model.tokenizer.pad_token_id for _ in range(max_input_len-len(session_token))]
    input_attention = [1 for _ in range(len(session_token))] + [0 for _ in range(max_input_len-len(session_token))]
    batch_input_tokens.append(input_token)
    batch_input_attentions.append(input_attention)
    
batch_input_tokens = torch.tensor(batch_input_tokens).cuda()
batch_input_attentions = torch.tensor(batch_input_attentions).cuda()


softmax = torch.nn.Softmax(dim=1)
results = fine_model(batch_input_tokens, batch_input_attentions)
prob = softmax(results)
true_prob = prob[:,1].tolist()

print(context)
for utt, prob in zip(candidates, true_prob):
    print(utt, '##', round(prob,3))
```

---
## ğŸ—“ï¸ í”„ë¡œì íŠ¸ ê°œì„  ì§„í–‰

|ê°œì„ ì‚¬í•­|ì´ìœ |ì§„í–‰ë¥ (%)|
|:-----:|:-----:|:-----:|
|Data Augmentation|ë²•ë¥ ë¬¸ì„œ ë‚®ì€ score||
|5ë§Œ stepë¡œ í•™ìŠµ|í…ŒìŠ¤íŠ¸ë¡œ 1000ë§Œ í•™ìŠµ||
|Transformerë¡œ ì„œë¹„ìŠ¤ êµ¬í˜„|Transformerê°€ ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ìŒ||
|RoBERTa, ELECTRAë“± ê³ ë ¤|BERTë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ ëª¨ë¸ ì¡´ì¬||


---
